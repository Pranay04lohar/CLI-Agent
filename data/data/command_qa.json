[
  {
    "question": "The gitpod GitHub page says\n\nGitpod is an open-source Kubernetes application providing prebuilt,\ncollaborative development environments in your browser - powered by VS\nCode.\n\nHowever, I can not comprehend what it actually does. Can anyone please explain.",
    "answer": "Gitpod co-founder here.\nGitpod = server-side-dev-envs + dev-env-as-code + prebuilds + IDE + collaboration.\n\nFrom a Git Repository on GitHub, Gitlab or Bitbucket, Gitpod can spin up a server-side-dev-environment for you in seconds. That's a docker container that you can fully customize and that includes your source code, git-Terminal, VS Code extensions, your IDE (Theia IDE), etc. The dev environment is enough powerful to run your app and even side-services like databases.\nStep (1) is easily repeatable and reproducible because it's automated and version-controlled and shared across the team. We call this dev-environment-as-code. Think of infrastructure-as-code for your dev environment.\nAfter (1), you're immediately ready-to-code, because your workplace is already compiled and all dependencies of your code have been downloaded. Gitpod does that by running your build tools on git-push (like CI/CD would do) and \"prebuilds\" and store your workspace until you need it. This really shines when reviewing PRs in Gitpod.\nCollaboration becomes much easier once your dev environments live server-side and your IDE runs in the browser. Sending a snapshot of your dev environment to a colleague is as easy as sending a URL. The same goes for live shared coding in the same IDE and dev-environments.\n\nAt the end of the day, you start treating your dev environments as something ephemeral: You start them, you code, your push your code, and you forget your dev environment. For your next thing, you'll use a fresh dev environment.\nThe ease of mind that you get from not messing, massaging, and maintaining dev environments on your local machine is incredibly liberating.\nGitpod can be used on gitpod.io, or self-hosted on Kubernetes, GCP, or AWS."
  },
  {
    "question": "Once upon a time, there was a file in my project that I would now like to be able to get.\nThe problem is: I have no idea of when have I deleted it and on which path it was.\nHow can I locate the commits of this file when it existed?",
    "answer": "If you do not know the exact path you may use\ngit log --all --full-history -- \"**/thefile.*\"\n\nIf you know the path the file was at, you can do this:\ngit log --all --full-history -- <path-to-file>\n\nThis should show a list of commits in all branches which touched that file. Then, you can find the version of the file you want, and display it with...\ngit show <SHA> -- <path-to-file>\n\nOr restore it into your working copy with:\ngit checkout <SHA>^ -- <path-to-file>\nNote the caret symbol (^), which gets the checkout prior to the one identified, because at the moment of <SHA> commit the file is deleted, we need to look at the previous commit to get the deleted file's contents"
  },
  {
    "question": "What is the difference between a Pull request and a Merge request?\nIn GitHub, it's a Pull Request while in GitLab, for example, it's a Merge Request. So, is there a difference between both of these?",
    "answer": "GitLab's \"merge request\" feature is equivalent to GitHub's \"pull request\" feature.  Both are means of pulling changes from another branch or fork into your branch and merging the changes with your existing code. They are useful tools for code review and change management.\nAn article from GitLab discusses the differences in naming the feature:\n\nMerge or pull requests are created in a git management application and ask an assigned person to merge two branches. Tools such as GitHub and Bitbucket choose the name pull request since the first manual action would be to pull the feature branch. Tools such as GitLab and Gitorious choose the name merge request since that is the final action that is requested of the assignee. In this article we'll refer to them as merge requests.\n\nA \"merge request\" should not be confused with the git merge command. Neither should a \"pull request\" be confused with the git pull command.  Both git commands are used behind the scenes in both pull requests and merge requests, but a merge/pull request refers to a much broader topic than just these two commands."
  },
  {
    "question": "I have a problem when I push my code to git while I have developer access in my project, but everything is okay when I have master access. Where is the problem come from? And how to fix it?\nError message:\n\nerror: You are not allowed to push code to protected branches on this project.\n...\nerror: failed to push some refs to ...",
    "answer": "there's no problem - everything works as expected.\nIn GitLab some branches can be protected. By default only Maintainer/Owner users can commit to protected branches (see permissions docs). master branch is protected by default - it forces developers to issue merge requests to be validated by project maintainers before integrating them into main code.\nYou can turn on and off protection on selected branches in Project Settings (where exactly depends on GitLab version - see instructions below). \nOn the same settings page you can also allow developers to push into the protected branches. With this setting on, protection will be limited to rejecting operations requiring git push --force (rebase etc.)\nSince GitLab 9.3\nGo to project: \"Settings\" → \"Repository\" → \"Expand\" on \"Protected branches\"\n\nI'm not really sure when this change was introduced, screenshots are from 10.3 version.\nNow you can select who is allowed to merge or push into selected branches (for example: you can turn off pushes to master at all, forcing all changes to branch to be made via Merge Requests). Or you can click \"Unprotect\" to completely remove protection from branch.\nSince GitLab 9.0\nSimilarly to GitLab 9.3, but no need to click \"Expand\" - everything is already expanded:\nGo to project:  \"Settings\" → \"Repository\" → scroll down to \"Protected branches\".\n\nPre GitLab 9.0\nProject: \"Settings\" → \"Protected branches\" (if you are at least 'Master' of given project).\n\nThen click on \"Unprotect\" or \"Developers can push\":"
  },
  {
    "question": "How to check which version of GitLab is installed on the server?   \nI am about version specified in GitLab changelog:\nhttps://gitlab.com/gitlab-org/gitlab-foss/blob/master/CHANGELOG.md\nFor example: \"6.5.0\", \"6.4.3\", etc.\nСan this be done only through the terminal?\nIs there a way to do that remotely (with browser instead of terminal)?",
    "answer": "I have updated my server to GitLab 6.6.4 and finally found the way to get version of GitLab remotely without SSH access to server.\nYou should be logged in to access the following page:\nhttps://your.domain.name/help\nIt shows something similar to:\n\nGitLab 6.6.4 42e34ae\nGitLab is open source software to collaborate on code.\n...\netc."
  },
  {
    "question": "I have created several repositories in GitLab. One of those was for testing purposes and has some commits and branches. I want to delete or remove this repository. How can I do this?",
    "answer": "Go to the project page\nSelect \"Settings\"\nSelect the \"General\" section (you must be in the repository you want to delete it)\nIf you have enough rights, then at the bottom of the page will be a button for \"Advanced settings\" (i.e. project settings that may result in data loss) or \"Remove project\" (in newer GitLab versions)\nPush this button and follow the instructions\n\nThis is only available for admins/owner. As a mere project maintainer, you do not see the \"Remove project\" button."
  },
  {
    "question": "Can one transfer repositories from GitLab to GitHub if the need be. If so, how exactly can I go about doing the same?\nAlso, are there any pitfalls in doing so or precautionary measures that I need to keep in mind  before doing so given that I may decide to eventually move them to GitHub (as it has more features at the moment that I might find handy for my project).",
    "answer": "You can transfer those (simply by adding a remote to a GitHub repo and pushing them)\n\ncreate an empty repo on GitHub\ngit remote add github https://yourLogin@github.com/yourLogin/yourRepoName.git\ngit push --mirror github\n\nThe history will be the same.\nBut you will lose the access control (teams defined in GitLab with specific access rights on your repo)\nIf you face any issue with the https URL of the GitHub repo:\nThe requested URL returned an error: 403\n\nAll you need to do is to enter your GitHub password, but the OP suggests:\n\nThen you might need to push it the ssh way. You can read more on how to do it here.\n\nSee \"Pushing to Git returning Error Code 403 fatal: HTTP request failed\".\n\nNote that mike also adds in the comments:\n\nGitLab can also be set to push mirror to downstream repositories, and there are specific instructions for push mirroring to GitHub.\nThis can use a GitHub Personal Access Token and also be set to periodically push.\nYou might use this option to share on GitHub, but keep your main development activity in your GitLab instance.\n\n\ntswaehn suggests in the comments the tool piceaTech/node-gitlab-2-github\n\nIt is possible to migrate issues, labels, ... with this tool github.com/piceaTech/node-gitlab-2-github: I tested it, not bad.\nBut had issues when transferring attachments of the issues itself.\nStill worth a try, maybe.\n\nFrotz notes in the comments that:\n\nI've since discovered that there's a wait option available to use in the user-editable settings.ts file that's not documented anywhere.\nI discovered it while implementing my own quick-and-dirty delay, which did work in stopping the \"Abuse detected\" slowdowns\n\n\n\nDoes this mean that when I want to push new changes I would have to do git push github [branch_name] instead of using origin?\n\nNo, you can:\n\ndelete origin, (git remote remove origin),\nrename github remote as origin (git remote rename github origin), and\ngo on git push (to origin, which is now GitHub): the transfer from GitLab to GitHub is complete."
  },
  {
    "question": "I have run gitlabhq rails server on virtual machine, following 1-6 steps from this tutorial https://github.com/gitlabhq/gitlab-recipes/blob/master/install/centos/README.md and starts rails server executing command sudo -u git -H bundle exec rails s -e production. After that I created user, using admin tools and created new project under this user. Then I'm trying to push the existing project to this repo as always. But in the last step, git push origin master fails with the error \n\n[remote rejected] master -> master (pre-receive hook declined)\n\nAdditional info:\n1) I haven't activated user (project owner) via email activation link, because I haven't configured post service on server-side and I didn't find instructions how to do that in this manual.\n2) Gitlab server generates tips how to push project to repo and there is not repositories/ in path. I mean it generates git@mygitlabhost:user/repo.git instead of git@mygitlabhost:repositories/user/repo.git which is correct.\n3) When i tried to debug it, I opened pre-receive script inside repo on server and tried to output variables (there is 3 of them): refs = ARGF.read, key_id  = ENV['GL_ID'] and repo_path = Dir.pwd and found, that key_id is always empty. Maybe the problem is here... If so, please give me suggestions on how to fix that. Thanks",
    "answer": "GitLab by default marks master branch as protected (See part Protecting your code in https://about.gitlab.com/2014/11/26/keeping-your-code-protected/ why). If so in your case, then this can help:\n\nOpen your project > Settings > Repository and go to \"Protected branches\", find \"master\" branch into the list and click \"Unprotect\" and try again.\n\nvia https://gitlab.com/gitlab-com/support-forum/issues/40\nFor version 8.11 and above how-to here: https://docs.gitlab.com/ee/user/project/protected_branches.html#restricting-push-and-merge-access-to-certain-users"
  },
  {
    "question": "I accidentally pushed my local master to a branch called origin on gitlab and now it is the default. Is there a way to rename this branch or set a new master branch to master?",
    "answer": "Updated:\nPrerequisites:\n\nYou have the Owner or Maintainer role in the project.\n\nTo update the default branch for an individual project:\n\nOn the left sidebar, select Search or go to find your project.\nSelect Settings > Repository.\nExpand Branch defaults. For Default branch, select a new default branch.\nOptional. Select the Auto-close referenced issues on default branch checkbox to close issues when a merge request uses a closing pattern.\nSelect Save changes.\n\n\nTo change a default branch name for an instance or group:\n\nOn the left sidebar, at the bottom, select Admin Area.\nSelect Settings > Repository.\nExpand Default branch.\nFor Initial default branch name, select a new default branch.\nSelect Save changes."
  },
  {
    "question": "My problem is that I can't push or fetch from GitLab. However, I can clone (via HTTP or via SSH). I get this error when I try to push :\n\nPermission denied (publickey) fatal : Could not read from remote repository\n\nFrom all the threads I've looked, here is what I have done :\n\nSet up an SSH key on my computer and added the public key to GitLab\nDone the config --global for username and email\nCloned via SSH and via HTTP to check if it would resolve the issue\nDone the ssh -T git@gitlab.com command\n\nIf you have any insight about how to resolve my issue, it would be greatly appreciated.",
    "answer": "I found this after searching a lot. It will work perfectly fine for me.\n\nGo to \"Git Bash\" just like cmd. Right click and \"Run as Administrator\".\nType ssh-keygen\nPress enter.\nIt will ask you to save the key to the specific directory.\nPress enter. It will prompt you to type password or enter without password.\nThe public key will be created to the specific directory.\nNow go to the directory and open .ssh folder.\nYou'll see a file id_rsa.pub. Open it on notepad. Copy all text from it.\nGo to https://gitlab.com/-/profile/keys or\nPaste here in the \"key\" textfield.\nNow click on the \"Title\" below. It will automatically get filled.\nThen click \"Add key\".\n\nNow give it a shot and it will work for sure."
  },
  {
    "question": "I'm facing this error when I try to clone a repository from GitLab (GitLab 6.6.2 4ef8369):\nremote: Counting objects: 66352, done.\nremote: Compressing objects: 100% (10417/10417), done.\nerror: RPC failed; curl 18 transfer closed with outstanding read data remaining\nfatal: The remote end hung up unexpectedly\nfatal: early EOF\nfatal: index-pack failed\n\nThe clone is then aborted. How can I avoid this?",
    "answer": "It happens more often than not, I am on a slow internet connection and I have to clone a decently huge git repository. The most common issue is that the connection closes and the whole clone is cancelled.\nCloning into 'large-repository'...\nremote: Counting objects: 20248, done.\nremote: Compressing objects: 100% (10204/10204), done.\nerror: RPC failed; curl 18 transfer closed with outstanding read data remaining \nfatal: The remote end hung up unexpectedly\nfatal: early EOF\nfatal: index-pack failed\n\nAfter a lot of trial and errors and a lot of “remote end hung up unexpectedly” I have a way that works for me. The idea is to do a shallow clone first and then update the repository with its history.\n$ git clone http://github.com/large-repository --depth 1\n$ cd large-repository\n$ git fetch --unshallow"
  },
  {
    "question": "I am learning GitLab CI/CD.\nI installed GitLab and GitLab Runner from Officials. Whenever I run the pipeline during Maven build, the job gets stuck. I have a registered runner and it is available to my project, but jobs get stuck.\n.gitlab-ci.yml\nimage: docker:latest\nservices:\n- docker:dind\n\nvariables:\n DOCKER_DRIVER: overlay\n SPRING_PROFILES_ACTIVE: gitlab-ci\n\nstages:\n- build\n- package\n- deploy\n\nmaven-build:\n image: maven:3-jdk-8\n stage: build\n script: \"mvn package -B\"\n artifacts:\n paths:\n  - target/*.jar\n\ndocker-build:\nstage: package\nscript:\n - docker build -t registry.com/ci-cd-demo .\n - docker push registry.com/ci-cd-demo\n\nk8s-deploy:\n image: google/cloud-sdk\n stage: deploy\n script:\n  - echo \"$GOOGLE_KEY\" > key.json\n  - gcloud container clusters get-credentials standard-cluster-demo -- \n  zone us-east1-c --project ascendant-study-222206\n   - kubectl apply -f deployment.yml\n\nMy runner settings\n\n\nError message when runner is already associated with project:\n\nCan you help me?",
    "answer": "The job is stuck because your runners have tags and your jobs don't. Follow these 4 steps to enable your runner to run without tags:\n\n\nOr set tags to your jobs. For more info: Configuration of your jobs with .gitlab-ci.yml - Tags"
  },
  {
    "question": "Suppose that I would like to implement a fix to a project of someone else. That project resides on GitHub.\nI could create a fork on GitHub and implement the fix.\nHowever, I would like to create my fork on GitLab rather than on GitHub.\nIs that possible? How?\nI have read this article:\nhttps://about.gitlab.com/2016/12/01/how-to-keep-your-fork-up-to-date-with-its-origin/\nAnyway, I am not sure what should I do in my case.\n\nShould I just create a fork on GitLab of the project from GitHub somehow?\nOr should I create a mirror on GitLab of the project from GitHub?\nOr should I create a mirror on GitLab and then fork the mirror?\nOr should I do  something completely different?\n\nWhat is the correct approach.\nThanks.\nUPDATE\nRepository mirroring on GitLab does not make sense probably. I can create a mirror of MY GitHub repository on GitLab but I cannot create a mirror of a repository of someone else.\nhttps://docs.gitlab.com/ee/user/project/repository/mirror/\nThis is what I have done so far:\nI have cloned the original GitHub project to my local machine. I have commited the fix to a new branch in my local repository. I have created an empty project on GitLab. I have set origin in my local repository to that empty project on GitLab and pushed both branches to GitLab. I have set upstream in my local repository to the GitHub repository.\nWhen I want to get new commits from the original GitHub repository to the repository on GitLab (i.e. sync the repositories), I can do this using my local repo as an intermediate step. However, there is no direct connection between the repo on GitHub and the repo on GitLab. Is my setup correct? Is there any difference if I make a fork on GitHub?",
    "answer": "If you just want to track changes, first make an empty repository in GitLab (or whatever else you may be using) and clone it to your computer.\nThen add the GitHub project as the \"upstream\" remote with:\ngit remote add upstream https://github.com/user/repo\n\nNow you can fetch and pull from the upstream should there be any changes. (You can also push or merge to it if you have access rights.)\ngit pull upstream master\n\nFinally, push back to your own GitLab repository:\ngit push origin master\n\nIf you don't want to manually pull upstream/push origin, GitLab offers a mirroring ability in Settings => Repository => Mirroring repositories."
  },
  {
    "question": "In my GitLab repository, I have a group with 20 projects. I want to clone all projects at once. Is that possible?",
    "answer": "Update Dec. 2022, use glab repo clone\nglab repo clone -g <group> -a=false -p --paginate\n\nWith:\n\n-p, --preserve-namespace: Clone the repo in a subdirectory based on namespace\n--paginate: Make additional HTTP requests to fetch all pages of projects before cloning. Respects --per-page\n-a, --archived: Limit by archived status. Use with -a=false to exclude archived repositories. Used with --group flag\n\nThat does support cloning more than 100 repositories (since MR 1030, and glab v1.24.0, Dec. 2022)\nThis is for gitlab.com or for a self-managed GitLab instance, provided you set the environment variable GITLAB_URI or GITLAB_HOST: it specifies the URL of the GitLab server if self-managed (eg: https://gitlab.example.com).\n\nOriginal answer and updates (starting March 2015):\nNot really, unless:\n\nyou have a 21st project which references the other 20 as submodules.\n(in which case a clone followed by a git submodule update --init would be enough to get all 20 projects cloned and checked out)\n\nor you somehow list the projects you have access (GitLab API for projects), and loop on that result to clone each one (meaning that can be scripted, and then executed as \"one\" command)\n\n\n\nSince 2015, Jay Gabez mentions in the comments (August 2019) the tool gabrie30/ghorg\n\nghorg allows you to quickly clone all of an org's or user's repos into a single directory.\n\nUsage:\n$ ghorg clone someorg\n$ ghorg clone someuser --clone-type=user --protocol=ssh --branch=develop\n$ ghorg clone gitlab-org --scm=gitlab --namespace=gitlab-org/security-products\n$ ghorg clone --help\n\n\nAlso (2020): https://github.com/ezbz/gitlabber\nusage: gitlabber [-h] [-t token] [-u url] [--debug] [-p]\n                [--print-format {json,yaml,tree}] [-i csv] [-x csv]\n                [--version]\n                [dest]\n\nGitlabber - clones or pulls entire groups/projects tree from gitlab"
  },
  {
    "question": "I have an account of a Gitlab installation where I created the repository \"ffki-startseite\"\nNow I want to clone the repository git://freifunk.in-kiel.de/ffki-startseite.git into that repository with all commits and branches, so I can start working on it in my own scope.\nHow can I import it?",
    "answer": "I was able to fully export my project along with all commits, branches and tags to gitlab via following commands run locally on my computer:\n\nTo illustrate my example, I will be using https://github.com/raveren/kint as the source repository that I want to import into gitlab. I created an empty project named Kint (under namespace raveren) in gitlab beforehand and it told me the http git url of the newly created project there is http://gitlab.example.com/raveren/kint.git\nThe commands are OS agnostic.\n\nIn a new directory:\ngit clone --mirror https://github.com/raveren/kint\ncd kint.git\ngit remote add gitlab http://gitlab.example.com/raveren/kint.git\ngit push gitlab --mirror\n\nNow if you have a locally cloned repository that you want to keep using with the new remote, just run the following commands* there:\ngit remote remove origin\ngit remote add origin http://gitlab.example.com/raveren/kint.git\ngit fetch --all\n\n*This assumes that you did not rename your remote master from origin, otherwise, change the first two lines to reflect it."
  },
  {
    "question": "We are using GitLab for our private project. There are some forked libraries from github, that we want to install as npm module. Installing that module directly from npm is ok and for example this:\nnpm install git://github.com/FredyC/grunt-stylus-sprite.git\n\n...works correctly too, but doing the same for GitLab, just changing domain gets me this error.\nnpm WARN `git config --get remote.origin.url` returned wrong result (git://git.domain.com/library/grunt-stylus-sprite.git)\nnpm ERR! git clone git://git.domain.com/library/grunt-stylus-sprite.git Cloning into bare repository 'D:\\users\\Fredy\\AppData\\Roaming\\npm-cache\\_git-remotes\\git-git-domain-com-library-grunt-stylus-sprite-git-6f33bc59'...\nnpm ERR! git clone git://git.domain.com/library/grunt-stylus-sprite.git fatal:unable to connect to git.domain.com:\nnpm ERR! git clone git://git.domain.com/library/grunt-stylus-sprite.git git.domain.com[0: 77.93.195.214]: errno=No error\nnpm ERR! Error: Command failed: Cloning into bare repository 'D:\\users\\Fredy\\App\nData\\Roaming\\npm-cache\\_git-remotes\\git-git-domain-com-library-grunt-stylus-spr\nite-git-6f33bc59'...\nnpm ERR! fatal: unable to connect to git.domain.com:\nnpm ERR! git.domain.com[0: xx.xx.xx.xx]: errno=No error\n\nFrom the web interface of GitLab, I have this URL\ngit@git.domain.com:library/grunt-stylus-sprite.git. Running this against npm install it tries to install git module from npm registry.\nHowever using URL: git+ssh@git.domain.com:library/grunt-stylus-sprite.git is suddenly asking me for the password. My SSH key doesn't include passphrase, so I assume it wasn't able to load that key. Maybe there is some configuration for that I have missed ? Key is located at standard location in my home directory with the name \"id_rsa\".\nI am on Windows 7 x64.\nUPDATE\nSince NPM v3 there is built-in support for GitLab and other sources (BitBucket, Gist), from where you can install packages. It works for public and private ones so it's not exactly related to this, but some might find it useful.\nnpm install gitlab:<gitlabname>/<gitlabrepo>[#<commit-ish>]\n\nCheck out documentation: https://docs.npmjs.com/cli/install\nI you want to work with private repos in Gitlab you are required to manage your credentials/auth-token in your .npmrc. See here: https://docs.gitlab.com/ee/user/packages/npm_registry/#authenticate-to-the-package-registry",
    "answer": "You have the following methods for connecting to a private gitlab repository\nWith SSH\ngit+ssh://git@git.mydomain.com:Username/Repository#{branch|tag}\ngit+ssh://git@git.mydomain.com/Username/Repository#{branch|tag}\n\nWith HTTPS\ngit+https://git@git.mydomain.com/Username/Repository#{branch|tag}\n\nWith HTTPS and deploy token\ngit+https://<token-name>:<token>@gitlab.com/Username/Repository#{branch|tag}"
  },
  {
    "question": "I'm trying to add a ruby rails file to my repository in gitlab, but it doesn't allow me to add the file saying that my file does not have a commit checked out.\nI've tried git pull, and making the the file again and git adding it, but it still doesn't work. This is the error message I get:\nerror: '172069/08_lab_routes_controllers_views_172069_172188-Copy/adventure_game/' does not have a commit checked out\nfatal: adding files failed",
    "answer": "If you have a subdirectory with a .git directory and try to git add . you will see this message.\nThis can happen if you have a git repo and then create/clone another repo in a subdirectory under that repo."
  },
  {
    "question": "We have recently started to use GitLab.\nCurrently using a \"centralized\" workflow.\nWe are considering moving to the github-flow but I want to make sure.\nWhat are the pros and cons of git-flow vs github-flow?",
    "answer": "As discussed in GitMinutes episode 17, by Nicholas Zakas in his article on  \"GitHub workflows inside of a company\":\n\nGit-flow is a process for managing changes in Git that was created by Vincent Driessen and accompanied by some Git extensions for managing that flow.\n  The general idea behind git-flow is to have several separate branches that always exist, each for a different purpose: master, develop, feature, release, and hotfix.\n  The process of feature or bug development flows from one branch into another before it’s finally released.\nSome of the respondents indicated that they use git-flow in general.\n  Some started out with git-flow and moved away from it. \nThe primary reason for moving away is that the git-flow process is hard to deal with in a continuous (or near-continuous) deployment model.\nThe general feeling is that git-flow works well for products in a more traditional release model, where releases are done once every few weeks, but that this process breaks down considerably when you’re releasing once a day or more.\n\nIn short:\nStart with a model as simple as possible (like GitHub flow tends to be), and move towards a more complex model if you need to.\n\nYou can see an interesting illustration of a simple workflow, based on GitHub-Flow at:\n\"A simple git branching model\", with the main elements being:\n\n\nmaster must always be deployable.\nall changes made through feature branches (pull-request + merge)\nrebase to avoid/resolve conflicts; merge in to master\n\n\n\n\nFor an actual more complete and robust workflow, see gitworkflow (one word)."
  },
  {
    "question": "I'm using GitLab to write a read.me file.  \nI tried to create a link to a header. According to the wiki an id should be automatically created:\nsee here\nI created a header using:\n### 1. This is my Header\n\nand tried to create a link to it:\n[link](#1--this-is-my-header)\n\nbut it is not working. \nWhat am I doing wrong?",
    "answer": "In the Documentation you link to we learn that...\n\nThe IDs are generated from the content of the header according to the\n  following rules:\n\nAll text is converted to lowercase.\nAll non-word text (e.g., punctuation, HTML) is removed.\nAll spaces are converted to hyphens.\nTwo or more hyphens in a row are converted to one.\nIf a header with the same ID has already been generated, a unique incrementing number is appended, starting at 1.\n\n\nNote rule 4: \"Two or more hyphens in a row are converted to one.\" However, the example you tried has two hyphens in a row (after the 1). Remove one of them and you should have it.\n[link](#1-this-is-my-header)\n\nFrom time to time I have encountered a unique header which is converted into an ID in some non-obvious way. A quick way to work out the ID is to use your browser's view source and/or inspect tools to view the HTML source code. For example, you might find the following HTML for your example:\n<h3 id=\"1-this-is-my-header\">1. This is my Header</h3>\n\nThen just use the contents of the id attribute with a hash to link to that header: #1-this-is-my-header."
  },
  {
    "question": "I created a custom docker image and push it to docker hub but when I run it in CI/CD it gives me this error.\nexec /usr/bin/sh: exec format error\nWhere :\nDockerfile\nFROM ubuntu:20.04\nRUN apt-get update\nRUN apt-get install -y software-properties-common\nRUN apt-get install -y python3-pip\nRUN pip3 install robotframework\n\n.gitlab-ci.yml\nrobot-framework:\n  image: rethkevin/rf:v1\n  allow_failure: true\n  script:\n    - ls\n    - pip3 --version\n\nOutput\nRunning with gitlab-runner 15.1.0 (76984217)\n  on runner zgjy8gPC\nPreparing the \"docker\" executor\nUsing Docker executor with image rethkevin/rf:v1 ...\nPulling docker image rethkevin/rf:v1 ...\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nPreparing environment\n00:01\nRunning on runner-zgjy8gpc-project-1049-concurrent-0 via 1c8189df1d47...\nGetting source from Git repository\n00:01\nFetching changes with git depth set to 20...\nReinitialized existing Git repository in /builds/reth.bagares/test-rf/.git/\nChecking out 339458a3 as main...\nSkipping Git submodules setup\nExecuting \"step_script\" stage of the job script\n00:00\nUsing docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...\nexec /usr/bin/sh: exec format error\nCleaning up project directory and file based variables\n00:01\nERROR: Job failed: exit code 1\n\nany thoughts on this to resolve the error?",
    "answer": "The problem is that you built this image for arm64/v8 -- but your runner is using a different architecture.\nIf you run:\ndocker image inspect rethkevin/rf:v1\n\nYou will see this in the output:\n...\n        \"Architecture\": \"arm64\",\n        \"Variant\": \"v8\",\n        \"Os\": \"linux\",\n...\n\nTry building and pushing your image from your GitLab CI runner so the architecture of the image will match your runner's architecture.\nAlternatively, you can build for multiple architectures using docker buildx . Alternatively still, you could also run a GitLab runner on ARM architecture so that it can run the image for the architecture you built it on.\n\nWith modern versions of docker, you may also explicitly control the platform docker uses. Docker will use platform emulation if the specified platform is different from your native platform.\nFor example:\nUsing the DOCKER_DEFAULT_PLATFORM environment variable:\nDOCKER_DEFAULT_PLATFORM=\"linux/amd64\" docker build -t test .\n\nUsing the --platform argument, either in the CLI or in your dockerfile:\ndocker build --platform=\"linux/amd64\" -t test .\n\nFROM --platform=linux/amd64 ubuntu:jammy\n\nSystems with docker desktop installed should already be able to do this. If your system is using docker without docker desktop, you may need to install the docker-buildx plugins explicitly."
  },
  {
    "question": "What is the difference between Jenkins and other CI like GitLab CI, drone.io coming with the Git distribution. On some research I could only come up that GitLab community edition doesn't allow Jenkins to be added, but GitLab enterprise edition does. Are there any other significant differences?",
    "answer": "This is my experience:\nAt my work we manage our repositories with GitLab EE and we have a Jenkins server (1.6) running.\nIn the basis they do pretty much the same. They will run some scripts on a server/Docker image.\nTL;DR;\n\nJenkins is easier to use/learn, but it has the risk to become a plugin hell\nJenkins has a GUI (this can be preferred if it has to be accessible/maintainable by other people)\nIntegration with GitLab is less than with GitLab CI\nJenkins can be split off your repository\n\nMost CI servers are pretty straight forward (concourse.ci, gitlab-ci, circle-ci, travis-ci, drone.io, gocd and what else have you). They allow you to execute shell/bat scripts from a YAML file definition. Jenkins is much more pluggable, and comes with a UI. This can be either an advantage or disadvantage, depending on your needs.\nJenkins is very configurable because of all the plugins that are available. The downside of this is that your CI server can become a spaghetti of plugins.\nIn my opinion chaining and orchestrating of jobs in Jenkins is much simpler (because of the UI) than via YAML (calling curl commands). Besides that Jenkins supports plugins that will install certain binaries when they are not available on your server (don't know about that for the others).\nNowadays (Jenkins 2 also supports more \"proper ci\" with the Jenkinsfile and the pipline plugin which comes default as from Jenkins 2), but used to be less coupled to the repository than i.e. GitLab CI.\nUsing YAML files to define your build pipeline (and in the end running pure shell/bat) is cleaner.\nThe plug-ins available for Jenkins allow you to visualize all kinds of reporting, such as test results, coverage and other static analyzers. Of course, you can always write or use a tool to do this for you, but it is definitely a plus for Jenkins (especially for managers who tend to value these reports too much).\nLately I have been working more and more with GitLab CI. At GitLab they are doing a really great job making the whole experience fun. I understand that people use Jenkins, but when you have GitLab running and available it is really easy to get started with GitLab CI. There won't be anything that will integrate as seamlessly as GitLab CI, even though they put quite some effort in third-party integrations.\n\nTheir documentation should get you started in no time.\nThe threshold to get started is very low.\nMaintenance is easy (no plugins).\nScaling runners is simple.\nCI fully part of your repository.\nJenkins jobs/views can get messy.\n\nSome perks at the time of writing:\n\nOnly support for a single file in the community edition. Multiples files in the enterprise edition."
  },
  {
    "question": "Question:\nIs there a way to automatically checkout git submodules via the same method (ssh or https) as the main repository?\nBackground:\nWe have a non-public gitlab repository (main) that has a submodule (utils) which is also hosted as a non-public gitlab repository on the same server. Those repositories can be accessed either via ssh or https:\n\nuser@gitlabserver.com:my/path/repo.git\nhttps://gitlabserver.com/my/path/repo.git\n\nBoth variants obviously require different forms of authentication and depending on the client computer and the user, one or the other is preferred. \nFor the top level repository (main) that is not an issue, as anyone can choose the method he or she prefers, but for the sub module this depends on the .gitmodules file and hence is (initially) the same for all.\nNow instead of everyone having to adapt the .gitmodules file to whatever they prefer and make sure they don't accidentally commit those changes, it would be nice, if there was a way to just specify the server and repo path and git chooses either the same method that is used for the main repo, or something that can be set in gitconfig.",
    "answer": "I finally solved this problem by specifying the submodules url as a relative path:\nSo lets say your main git repository can be reached  \n\neither via https://gitlabserver.com/my/path/main.git\nor via user@gitlabserver.com:my/path/main.git\n\nAnd the .gitmodules file looks like this:\n[submodule \"utils\"]     \n    path = libs/utils   \n    url = https://gitlabserver.com/my/path/utils.git\n\nThat would mean that even when you check out the main application via ssh, the submodule utils would still be accessed via https.\nHowever, you can replace the absolute path with a relative one like this: \n[submodule \"utils\"]     \n    path = libs/utils   \n    url = ../utils.git\n\nand from now on use \n\neither git clone --recursive https://gitlabserver.com/my/path/main.git\nor git clone --recursive user@gitlabserver.com:my/path/main.git\n\nto get the whole repository structure which ever way you want. Obviously that doesn't work for cases where the relative ssh and the https paths are not the same, but at least for gitlab hosted repositories this is the case.\nThis is also handy if you (for whatever reason) mirror your repository structure at two different remote sites."
  },
  {
    "question": "I use GitLab on their servers. I would like to download my latest built artifacts (build via GitLab CI) via the API like this:\ncurl --header \"PRIVATE-TOKEN: 9koXpg98eAheJpvBs5tK\" \"https://gitlab.com/api/v3/projects/1/builds/8/artifacts\"\n\nWhere do I find this project ID? Or is this way of using the API not intended for hosted GitLab projects?",
    "answer": "I just found out an even easier way to get the project id: just see the HTML content of the gitlab page hosting your project. There is an input with a field called project_id, e.g:\n<input type=\"hidden\" name=\"project_id\" id=\"project_id\" value=\"335\" />"
  },
  {
    "question": "I have one project on Gitlab and I worked with it for the last few days!\nNow i want pull project on my home PC but show me below error : \nInvocation failed Unexpected Response from Server:  Unauthorized\n        java.lang.RuntimeException: Invocation failed Unexpected Response from Server:  Unauthorized\n        at org.jetbrains.git4idea.nativessh.GitNativeSshAskPassXmlRpcClient.handleInput(GitNativeSshAskPassXmlRpcClient.java:34)\n        at org.jetbrains.git4idea.nativessh.GitNativeSshAskPassApp.main(GitNativeSshAskPassApp.java:30)\n        Caused by: java.io.IOException: Unexpected Response from Server:  Unauthorized\n        at org.apache.xmlrpc.LiteXmlRpcTransport.sendRequest(LiteXmlRpcTransport.java:231)\n        at org.apache.xmlrpc.LiteXmlRpcTransport.sendXmlRpc(LiteXmlRpcTransport.java:90)\n        at org.apache.xmlrpc.XmlRpcClientWorker.execute(XmlRpcClientWorker.java:72)\n        at org.apache.xmlrpc.XmlRpcClient.execute(XmlRpcClient.java:194)\n        at org.apache.xmlrpc.XmlRpcClient.execute(XmlRpcClient.java:185)\n        at org.apache.xmlrpc.XmlRpcClient.execute(XmlRpcClient.java:178)\n\nMy android studio version is 3.4 !",
    "answer": "Enabling credentials helper worked for me, using Android Studio 3.6.2 on Windows 10\nAndroidStudio -> File -> Settings -> Git ->  Use credential helper"
  },
  {
    "question": "I had created a private repository which I then changed to public repository. However, I could not find any way to release. Is it possible to create releases in GitLab? If so, how are they done?",
    "answer": "To create a release on the GitLab website:\n\nGo to your repository\nIn the menu choose Repository > Tags\nAdd a tag for the version of your app. For example, v1.3.1.\nAdd a message (title) about the release. For example, Release 1.3.1.\nAdd a note that describes the details of the release. (Not optional. Adding a note to a tag is what makes it a release.)\nClick Create tag.\n\n\nThe release will now show up under Project > Releases. Read more at the GitLab documentation. GitLab recommends that you use the Release API now, but their documentation is hard to follow. It would be the preferred method for automating everything with CI/CD, though."
  },
  {
    "question": "I want to create a public repo to put some sample files from my main repo (private). Is there any way to soft link few folders from a git repo to another git repo?",
    "answer": "Then you should use submodules for this task.\nSubmodule are different git repositories under the same root.\nThis way you can manage 2 different project at folder level inside the root repository\n\nSubmodules allow foreign repositories to be embedded within a dedicated subdirectory of the source tree, always pointed at a particular commit.\n\n\ngit submodule\nBreak your big project to sub projects as you did so far.\nNow add each sub project to you main project using :\ngit submodule add <url>\n\nOnce the project is added to your repo, you have to init and update it.\ngit submodule init\ngit submodule update\n\nAs of Git 1.8.2 new option --remote was added\ngit submodule update --remote --merge\n\nwill fetch the latest changes from upstream in each submodule, merge them in, and check out the latest revision of the submodule.\nAs the docs describe it:\n\n--remote\nThis option is only valid for the update command. Instead of using the superproject’s recorded SHA-1 to update the submodule, use the status of the submodule’s remote-tracking branch.\n\nThis is equivalent to running git pull in each submodule.\n\n\nHowever, how would I push a commit in the scenario of bug fix in C which affects the code shared with the parent layers?\n\nAgain: using submodule will place your code inside your main project as part of its content. The difference between having it locally inside the folder or having it as part of a submodule is that in submodule the content is managed (commited) to a different standalone repository.\n\nThis is an illustration of submodule - project inside another project in which each project is a standalone project.\n\n\ngit subtree\nGit subtree allows you to insert any repository as a sub-directory of another one\nVery similar to submodule but the main difference is where your code is managed. In submodules the content is placed inside a separate repo and is managed there which allow you to clone it to many other repos as well.\nsubtree is managing the content as part of the root project and not in a separate project.\nInstead of writing down how to set it up and to understand how to use it you can simply read this excellent post which will explain it all.\nhttps://developer.atlassian.com/blog/2015/05/the-power-of-git-subtree/"
  },
  {
    "question": "Today I've enabled Gitlab's 2nd-factor authentication. After that, since I logged in the Gitlab website, I need to use my cell phone to pass a 6-digits plus my password, that's good, it makes me feel safe.\nHowever, when I use the general operations, for example git clone some-repo.git, I got the error:\nCloning into 'some-repo'...\nremote: HTTP Basic: Access denied\nremote: You must use a personal access token with 'api' scope for Git over HTTP.\nremote: You can generate one at https://gitlab.com/profile/personal_access_tokens\nfatal: Authentication failed for 'some-repo.git'\n\nThen I try existing cloned local repo, using git pull, the same error occurs. Before I enabled the 2nd-factor authentication, all the above operation worked fine.\nFlowing the above error's instructions, I went to the mentioned address: https://gitlab.com/profile/personal_access_tokens. I created the following token, and save the token's key.\n\nHowever, I don't know what to do with this key. Can someone tell me how to use this key to enable the basic operations like git pull, git clone, git push etc...\nEdit\nI had many repos on local before I enabled the 2nd-factor authentication. I want these to work too.",
    "answer": "As explained in using gitlab token to clone without authentication, you can clone a GitLab repo using your Personal Access Token like this:\ngit clone https://oauth2:ACCESS_TOKEN@gitlab.com/yourself/yourproject.git\n\nAs for how to update your existing clones to use the GitLab Personal Access Token, you should edit your .git/config file in each local git directory, which will have an entry something like this:\n[remote \"origin\"]\n    url = https://yourself@gitlab.com/yourself/yourproject.git\n\nChange the url:\n[remote \"origin\"]\n    url = https://oauth2:ACCESS_TOKEN@gitlab.com/yourself/yourproject.git\n\nNow you can continue using this existing git clone as you did before you enabled 2FA."
  },
  {
    "question": "We have a project that is composed of multiple (non-public) repositories. \nTo build the whole project, the build system needs to have the files of all repositories (master branches).\nIs there a way I can configure GitLab CI to provide the repositories I need?\nI guess I could do a git fetch or similar during the CI build, but how to deal with authentication then?",
    "answer": "If you are running GitLab version 8.12 or later, the permissions model was reworked. Along with this new permission model comes the CI environment variable CI_JOB_TOKEN. The premium version of GitLab uses this environment variable for triggers, but you can use it to clone repos.\ndummy_stage:\n  script:\n    - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.instance/group/project.git"
  },
  {
    "question": "I have stored a Markdown file and an image file in a Git repo as follows:\n\nreadme.markdown\nimages/\n\nimage.png\n\n\nI reference the image from readme.markdown like this:\n![](./images/image.png)\n\nThis renders as expected in ReText, but does not render when I push the repo to GitLab.\nHow can I reference the image from the Markdown file so that it renders when viewed in GitLab?",
    "answer": "![](images/image.png) without the ./ works for me: https://gitlab.com/cirosantilli/test/blob/bffbcc928282ede14dcb42768f10a7ef21a665f1/markdown.md#image\nI have opened a request for this to be allowed at: http://feedback.gitlab.com/forums/176466-general/suggestions/6746307-support-markdown-image-path-in-current-directory-s , but it entered into Internet void when GitLab dumped UserVoice."
  },
  {
    "question": "I want to publish some programming documentation I have in a public available repository. This documentation has formatted text, some UML diagrams, and a lot of code examples. I think that GitHub or GitLab are good places to publish this.\nTo publish the UML diagrams, I would like to have some easy way to keep them updated into the repository and visible as images in the wiki. I don't want to keep the diagrams in my computer (or on the cloud), edit them, generate an image, and then publish it every time.\nIs there a way to put the diagrams in the repository (in PlantUML syntax would be ideal), link them in the markdown text, and make the images auto-update every time the diagram is updated?",
    "answer": "Edit: Alternative with Proxy service\nThis way is significantly different and simpler than the answer below; it uses the PlantUML proxy service:\nhttp://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/plantuml/plantuml-server/master/src/main/webapp/resource/test2diagrams.txt\n\nThe GitHub markdown for this would be:\n![alternative text](http://www.plantuml.com/plantuml/proxy?cache=no&src=https://raw.github.com/plantuml/plantuml-server/master/src/main/webapp/resource/test2diagrams.txt)\n\nThis method suffers from not being able to specify the SVG format (it defaults to PNG), and it is perhaps not possible to work-around the caching bug mentioned in the comments.\n\nAfter trying the other answer, I discovered the service to be slow and seemingly not up to the latest version of PlantUML.\nI've found a different way that's not quite as straightforward, but it works via  PlantUML.com's server (in the cloud). As such, it should work anywhere you can hotlink to an image.\nIt exploits the !includeurl function and is essentially an indirection. The markdown file links to a PlantUML source that includes the diagram's source. This method allows modifying the source in GitHub, and any images in the GitHub markdown files will automatically update. But it requires a tricky step to create the URL to the indirection.\n\n\nGet the URL to the raw PlantUML source, e.g., https://raw.githubusercontent.com/linux-china/plantuml-gist/master/src/main/uml/plantuml_gist.puml (using the example in the joanq's answer)\n\nGo to http://plantuml.com/plantuml/form (or PlantText.com) and create a one-line PlantUML source that uses the !includeurl URL-TO-RAW-PLANTUML-SOURCE-ON-GITHUB operation. Continuing with the example URL, the PlantUML (meta)source is:\n!includeurl https://raw.githubusercontent.com/linux-china/plantuml-gist/master/src/main/uml/plantuml_gist.puml\n\n\nCopy the image URL from PlantUML.com's image, e.g., http://plantuml.com:80/plantuml/png/FSfB2e0m303Hg-W1RFPUHceiDf36aWzwVEl6tOEPcGGvZXBAKtNljW9eljD9NcCFAugNU15FU3LWadWMh2GPEcVnQBoSP0ujcnS5KnmaWH7-O_kEr8TU and paste it into your GitHub markdown file. This URL won't change.\n![PlantUML model](http://plantuml.com:80/plantuml/png/3SNB4K8n2030LhI0XBlTy0YQpF394D2nUztBtfUHrE0AkStCVHu0WP_-MZdhgiD1RicMdLpXMJCK3TC3o2iEDwHSxvNVjWNDE43nv3zt731SSLbJ7onzbyeF)\n\n\n\nBonus: You can even get access to the SVG format by modifying the plantuml/png/ part of the URL to be plantuml/svg/ as follows\n![PlantUML model](http://plantuml.com:80/plantuml/svg/3SNB4K8n2030LhI0XBlTy0YQpF394D2nUztBtfUHrE0AkStCVHu0WP_-MZdhgiD1RicMdLpXMJCK3TC3o2iEDwHSxvNVjWNDE43nv3zt731SSLbJ7onzbyeF)\n\nExample on GitHub\nhttps://github.com/fuhrmanator/course-activity-planner/blob/master/ooad/overview.md\nCaveat with private repos\nAs davidbak pointed out in a comment, the raw file in a private repo will have a URL with token=<LONGSTRINGHERE> in it, and this token changes as the source file updates. Unfortunately, the markdown breaks when this happens, so you have to update the Readme file after you commit the file to GitHub, which is not a great solution."
  },
  {
    "question": "I formatted my Windows 7 laptop and in an attempt to have git setup working again, I installed git and source tree application.\nI deleted the SSH Key from gitlab and regenerated the key using ssh-keygen. But when I try to add the SSH Key at gitlab, it throws the following exception :\nKey is invalid\nFingerprint has already been taken\nFingerprint cannot be generated\n\nBecause of this I am unable to clone the git repository from the source tree application since gitlab is unable to authenticate the SSH key.I followed queries at google groups of gitlab but none of them seem to resolve my issue. Is there any workaround or steps to get the SSH key accepted by gitlab?",
    "answer": "In my case; the public key i was trying to add was already used with 'work' Gitlab account and i received the said error upon trying to use the same key with 'personal' Gitlab account.\nSolution - Add another public key on the same machine and use that with 'personal' gitlab account (both on same machine).\nnavigate to .ssh folder in your profile (even works on windows) and run command\nssh-keygen -t rsa\n\nwhen asked for file name give another filename id_rsa_2 (or any other).\nenter for no passphrase (or otherwise).\nYou will end up making id_rsa_2 and id_rsa_2.pub\nuse the command\ncat id_rsa_2.pub\n\ncopy and save key in 'personal' Gitlab account.\ncreate a file with no extension in .ssh folder named 'config'\nput this block of configuration in your config file\nHost           gitlab.com\nHostName       gitlab.com\nIdentityFile   C:\\Users\\<user name>\\.ssh\\id_rsa\nUser           <user name>\n\n\nHost           gitlab_2\nHostName       gitlab.com\nIdentityFile   C:\\Users\\<user name>\\.ssh\\id_rsa_2\nUser           <user name>\n\nnow whenever you want to use 'personal' gitlab account simply change alias in git URLs for action to remote servers.\nfor example\ninstead of using\ngit clone git@gitlab.com:..............\nsimply use\ngit clone git@gitlab_2:...............\ndoing that would use the second configuration with gitlab.com (from 'config' file) and will use the new id_rsa_2 key pair for authentication.\nFind more about above commands on this link\nhttps://clubmate.fi/how-to-setup-and-manage-multiple-ssh-keys/"
  },
  {
    "question": "According to the official gitlab documentation, one way to enable docker build within ci pipelines, is to make use of the dind service (in terms of gitlab-ci services).\nHowever, as it is always the case with ci jobs running on docker executors, the docker:latest image is also needed.\nCould someone explain:\n\nwhat is the difference between the docker:dind and the docker:latest images?\n(most importantly): why are both the service and the docker image needed (e.g. as indicated in this example, linked to from the github documentation) to perform e.g. a docker build whithin a ci job? doesn't the docker:latest image (within which the job will be executed!) incorporate the docker daemon (and I think the docker-compose also), which are the tools necessary for the commands we need (e.g. docker build, docker push etc)?\n\nUnless I am wrong, the question more or less becomes:\n\nWhy a docker client and a docker daemon cannot reside in the same docker (enabled) container",
    "answer": "what is the difference between the docker:dind and the docker:latest images?\n\n\ndocker:latest contains everything necessary to connect to a docker daemon, i.e., to run docker build, docker run and such. It also contains the docker daemon but it's not started as its entrypoint.\ndocker:dind builds on docker:latest and starts a docker daemon as its entrypoint.\n\nSo, their content is almost the same but through their entrypoints one is configured to connect to tcp://docker:2375 as a client while the other is meant to be used for a daemon.\n\nwhy are both the service and the docker image needed […]?\n\nYou don't need both. You can just use either of the two, start dockerd as a first step, and then run your docker build and docker run commands as usual like I did here; apparently this was the original approach in gitlab at some point. But I find it cleaner to just write services: docker:dind instead of having a before_script to setup dockerd. Also you don't have to figure out how to start & install dockerd properly in your base image (if you are not using docker:latest.)\nDeclaring the service in your .gitlab-ci.yml also lets you swap out the docker-in-docker easily if you know that your runner is mounting its /var/run/docker.sock into your image. You can set the protected variable DOCKER_HOST to unix:///var/run/docker.sock to get faster builds. Others who don't have access to such a runner can still fork your repository and fallback to the dind service without modifying your .gitlab-ci.yml."
  },
  {
    "question": "I have set up and we are running a default install of GitLab v6.0.1 (we're about to upgrade as well). It was a \"Production\" setup, following this guide precisely to the letter:\nhttps://github.com/gitlabhq/gitlabhq/blob/master/doc/install/installation.md\nNow, how do we safely change the URL of a working install?\nApparently our URL is very long and we've come up with a new URL. I've edited a number of configuration files and the \"Application Status Checks\" report everything is OK. I've rebooted the server to ensure things are still working.\nI can access Nginx just fine, over our original SSL. I can browse the GitLab site, create a repository, etc. I can fork and commit just fine.\nIt all seems to be OK; but, since this is not a native environment for me, I wanted to double check that I have done everything to rename a GitLab site.\nThe files I've edited are:\n/etc/hosts\n  127.0.0.1  localhost\n  10.0.0.10  wake.domain.com    wake\n  10.0.0.10  git.domain.com     git\n\n/home/git/gitlab/config/gitlab.yml\n  production: &base\n    gitlab:\n      host: git.domain.com\n\n/home/git/gitlab-shell/config.yml\n  gitlab_url: \"https://git.domain.com\"\n  ^- yes, we are on SSL and that is working, even on a new URL\n\n/etc/nginx/sites-available/gitlab\n  server {\n    server_name git.domain.com",
    "answer": "GitLab Omnibus\nFor an Omnibus install, it is a little different.\nThe correct place in an Omnibus install is:\n/etc/gitlab/gitlab.rb\n    external_url 'http://gitlab.example.com'\n\nFinally, you'll need to execute sudo gitlab-ctl reconfigure and sudo gitlab-ctl restart so the changes apply.\n\nI was making changes in the wrong places and they were getting blown away.\nThe incorrect paths are:\n/opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml\n/var/opt/gitlab/.gitconfig\n/var/opt/gitlab/nginx/conf/gitlab-http.conf\n\nPay attention to those warnings that read:\n# This file is managed by gitlab-ctl. Manual changes will be\n# erased! To change the contents below, edit /etc/gitlab/gitlab.rb\n# and run `sudo gitlab-ctl reconfigure`."
  },
  {
    "question": "Is it possible to move a GitLab repository from one group to another in GitLab?\nFor example, I have https://gitlab.com/my-user/my-repo. I'd like to move it to https://gitlab.com/my-group/another-group/my-repo.\nIdeally, I'd like to keep all the issues associated with it.",
    "answer": "Yes, you can move your GitLab project from one namespace to another.\nYour project -> Settings -> General -> Advanced\n\nThen, almost at the end of the list.\nTransfer project"
  },
  {
    "question": "I implemented the oauth2 web flow in order to get access_token from users of my app. With the access_token, I would like to do the following actions:  \n\nGet user informations  \nCreate a repo for this user  \nPush code to this repo (using git push  )\n\nI already successfully get the user information(1) and create a repo(2)\nThe problem is I can't push code (3), I got \"Unauthorized\" error.  \nThe command I run:\ngit remote add origin https://gitlab-ci-token<mytoken>@gitlab.com/myuser/myrepo.git  \ngit push origin master",
    "answer": "You should do\ngit remote add origin https://<access-token-name>:<access-token>@gitlab.com/myuser/myrepo.git\n\nNote that this stores the access token as plain text in the .git\\config file.  To avoid this you can use the git credential system, providing the access token name for \"username\" and the access token for \"password\".  This should store the credentials in the git credential system in a more secure way."
  },
  {
    "question": "In my CI pipeline I am generating an artifact public/graph.png that visualises some aspect of my code. In a later step I want to commit that to the repo from within the CI pipeline. Here's the pertinent part of .gitlab-ci.yml:\ncommit-graph:\n  stage: pages\n  script:\n    - git config user.email \"cipipeline@example.com\"\n    - git config user.name \"CI Pipeline\"\n    - cd /group/project\n    - mv public/graph.png .\n    - git add graph.png\n    - git commit -m \"committing graph.png [ci skip]\"\n    - echo $CI_COMMIT_REF_NAME\n    - git push origin HEAD:$CI_COMMIT_REF_NAME\n\nWhen the pipeline runs within gitlab it fails with:\n\n$ git config user.email \"cipipeline@dhgitlab.dunnhumby.co.uk\"\n  $ git config user.name \"CI Pipeline\"\n  $ cd /group/project\n  $ mv public/graph.png .\n  $ git add graph.png\n  $ git commit -m \"committing graph.png [ci skip]\"\n  [detached HEAD 22a50d1] committing graph.png [ci skip]\n   1 file changed, 0 insertions(+), 0 deletions(-)\n   create mode 100644 graph.png\n  $ echo $CI_COMMIT_REF_NAME\n  jamiet/my-branch\n  $ git push origin HEAD:$CI_COMMIT_REF_NAME\n  fatal: unable to access 'https://gitlab-ci-token:xxxxxxxxxxxxxxxxxxxx@example.com/group/project/project.git/': server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none\n\nNot sure what I'm doing wrong and don't know enough about SSL to understand that error. Can anyone advise?\nWe are hosting gitlab ourselves by the way.",
    "answer": "Nowadays there is a much cleaner way to solve this without using SSH but using a project scoped access token, also see this answer.\nIn the GitLab project create an project scoped access token so it is linked to the project, not to an individual. Next store this token as an GitLab CI/CD variable. You can now connect using the following:\npush-back-to-remote:\n  script:\n    - git config user.email \"my-email@email.com\"\n    - git config user.name \"ci-bot\"\n    - git remote add gitlab_origin https://oauth2:$ACCESS_TOKEN@gitlab.com/path-to-project.git\n    - git add .\n    - git commit -m \"push back from pipeline\"\n    - git push gitlab_origin HEAD:main -o ci.skip # prevent triggering pipeline again"
  },
  {
    "question": "Following this tutorial [link] to install gitlab on a dedicated server. I need to :\nsudo -u git -H bundle install --deployment --without development test postgres aws\n\nBut an error occurred while installing rugged :\nGem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension.\n\n    /usr/local/bin/ruby extconf.rb\nchecking for cmake... no\nERROR: CMake is required to build Rugged.\n*** extconf.rb failed ***\nCould not create Makefile due to some reason, probably lack of necessary\nlibraries and/or headers.  Check the mkmf.log file for more details.  You may\nneed configuration options.\n\nProvided configuration options:\n    --with-opt-dir\n    --without-opt-dir\n    --with-opt-include\n    --without-opt-include=${opt-dir}/include\n    --with-opt-lib\n    --without-opt-lib=${opt-dir}/lib\n    --with-make-prog\n    --without-make-prog\n    --srcdir=.\n    --curdir\n    --ruby=/usr/local/bin/ruby\n\n\n    Gem files will remain installed in /home/git/gitlab/vendor/bundle/ruby/2.0.0/gems/rugged-0.21.2 for inspection.\n    Results logged to /home/git/gitlab/vendor/bundle/ruby/2.0.0/gems/rugged-0.21.2/ext/rugged/gem_make.out\n    An error occurred while installing rugged (0.21.2), and Bundler cannot continue.\n    Make sure that `gem install rugged -v '0.21.2'` succeeds before bundling.\n\nSo I installed rugged -> I installed CMake & config-pkg :\n/home/git/gitlab$ sudo gem install rugged\nBuilding native extensions.  This could take a while...\nSuccessfully installed rugged-0.21.2\nParsing documentation for rugged-0.21.2\nunable to convert \"\\xC0\" from ASCII-8BIT to UTF-8 for lib/rugged/rugged.so, skipping\n1 gem installed\n\nBut it doesnt change anything : \nErrno::EACCES: Permission denied - /home/git/gitlab/vendor/bundle/ruby/2.0.0/gems/rugged-0.21.2/LICENSE\nAn error occurred while installing rugged (0.21.2), and Bundler cannot continue.\nMake sure that `gem install rugged -v '0.21.2'` succeeds before bundling.\n\nAny idea ?",
    "answer": "For OSX if you're using homebrew:  \nbrew install cmake \nbundle install"
  },
  {
    "question": "I've been following this guide on configuring GitLab continuous integration with Jenkins.\nAs part of the process, it is necessary to set the refspec as follows: +refs/heads/*:refs/remotes/origin/* +refs/merge-requests/*/head:refs/remotes/origin/merge-requests/*\nWhy this is necessary is not explained in the post, so I began looking online for an explanation and looked at the official documentation as well as some related StackOverflow questions like this one.\nIn spite of this, I'm still confused:\nWhat exactly is refspec? And why is the above refspec necessary – what does it do?",
    "answer": "A refspec tells git how to map references from a remote to the local repo.\nThe value you listed was +refs/heads/*:refs/remotes/origin/* +refs/merge-requests/*/head:refs/remotes/origin/merge-requests/*; so let's break that down.\nYou have two patterns with a space between them; this just means you're giving multiple rules.  (The pro git book refers to this as two refspecs; which is probably technically more correct. However, you just about always have the ability to list multiple refspecs if you need to, so in day to day life it likely makes little difference.)\nThe first pattern, then, is +refs/heads/*:refs/remotes/origin/* which has three parts:\n\nThe + means to apply the rule without failure even if doing so would move a target ref in a non-fast-forward manner.  I'll come back to that.\nThe part before the : (but after the + if there is one) is the \"source\" pattern.  That's refs/heads/*, meaning this rule applies to any remote reference under refs/heads (meaning, branches).\nThe part after the : is the \"destination\" pattern. That's refs/remotes/origin/*.\n\nSo if the origin has a branch master, represented as refs/heads/master, this will create a remote branch reference origin/master represented as refs/remotes/origin/master.  And so on for any branch name (*).\nSo back to that +... suppose the origin has\nA --- B <--(master)\n\nYou fetch and, applying that refspec you get\nA --- B <--(origin/master)\n\n(If you applied typical tracking rules and did a pull you also have master pointed at B.)  \nA --- B <--(origin/master)(master)\n\nNow some things happen on the remote.  Someone maybe did a reset that erased B, then committed C, then forced a push.  So the remote says\nA --- C <--(master)\n\nWhen you fetch, you get\nA --- B\n \\\n  C\n\nand git must decide whether to allow the move of origin/master from B to C.  By default it wouldn't allow this because it's not a fast-forward (it would tell you it rejected the pull for that ref), but because the rule starts with + it will accept it.  \nA --- B <--(master)\n \\\n  C <--(origin/master)\n\n(A pull will in this case result in a merge commit.)\nThe second pattern is similar, but for merge-requests refs (which I assume is related to your server's implementation of PR's; I'm not familiar with it).\nMore about refspecs:  https://git-scm.com/book/en/v2/Git-Internals-The-Refspec"
  },
  {
    "question": "I have a private repository in GitLab. I have to give its access to members of my team. How can I do that using GitLab web-interface?\nI know, how to do this in GitHub, but in GitLab it's somehow different.",
    "answer": "Update 2021: This answer is out of date, scroll down for the 2021 info.\nUPDATE: Gitlab has changed a bit in 2 years, so here is the updated flow.\n\nClick on the project that you want to share.\nClick on the Settings tab (the gear icon on the left).\nClick on the Members subtab.\nAdd member, and find the user if it exists on GitLab, or insert\nemail to send an invitation.\nSelect access level for the user, the possible levels are:\n\n\nGuest, can see wiki pages, view and create issues.\nReporter, can do what the guest can plus view the code.\nDeveloper, normal developer access, can develop, but cannot push or merge to procted branches by default.\nMaintainer, can do everything except managing the project.\n\n\nFor more info about the user access level refer to gitlab official help.\n(Optional) Set expiration date of the user access.\n\n\nOld instructions:\n\nClick on the project that you want to share.\nClick on members.\nAdd member, and find the user if it exists on GitLab, or insert\nemail to send an invitation."
  },
  {
    "question": "How can I change the project owner in GitLab?\nThere are options in project settings, but in the \"transfer\" field, it does not recognize any username or anything. Is it possible to change the owner-permissions and root-privileges?",
    "answer": "TL;DR\nMove your project to a new group where both you and the other user are owners, then the other user must transfer it to his own namespace.\nBackground\nThe other answers obviously do not work to transfer a project to a different user, although the comments section of one is enough for someone to figure it out. Also there is this issue on GitLab itself that provides some insights.\nMy Situation\nI installed and now administer a few instances of GitLab for a few small developer teams as well as one for my personal projects. Resultingly, I have run into numerous questions about this. I keep coming back to this question only to realize that it was never actually answered correctly.\nThe Namespace Problem\nThe issue that you face when doing this is that there can only be one owner of a project, but to transfer a project you have to own the namespace that you are transferring it to. To my knowledge there is no other way to move a project. For completeness, I'll add that the namespace here is, e.g., \"gitlab.com/my-user-name/...\" or \"gitlab.com/my-group-name/...\".\n\nSolution\nBecause one user cannot \"own\" another namespace (not even admins), the only option to set up a scenario where two users own the same namespace is with a group. Perform the following steps to accomplish this.\n\nCreate a new group.\nAdd the user that you want to transfer your project to as an owner member of that group.\nTransfer your project to that group (a namespace you manage because you are an owner).\nLogin as the other user, then transfer the group project to the \"other user\" namespace.\n\nAt this point you will be left as a master in the project. You can now remove yourself from the project entirely if desired."
  },
  {
    "question": "I have been attempting to complete this tutorial, but have run into a problem with the foreman start line.  I am using a windows 7, 64 bit machine and am attempting to do this in the git bash terminal provided by the Heroku Toolbelt.\nWhen I enter foreman start I receive:\nsh.exe\": /c/Program Files (x86)/Heroku/ruby-1.9.2/bin/foreman: \"c:/Program: bad\ninterpreter: No such file or directory\n\nSo I tried entering the cmd in git bash by typing cmd and then using foreman start (similar to a comment on one of the answers to this question suggests).  This is what that produced:\nBad file descriptor\nc:/Program Files (x86)/Heroku/ruby-1.9.2/lib/ruby/gems/1.9.1/gems/foreman-0.62.0\n/lib/foreman/engine.rb:377:in `read_nonblock'\nc:/Program Files (x86)/Heroku/ruby-1.9.2/lib/ruby/gems/1.9.1/gems/foreman-0.62.0\n/lib/foreman/engine.rb:377:in `block (2 levels) in watch_for_output'\nc:/Program Files (x86)/Heroku/ruby-1.9.2/lib/ruby/gems/1.9.1/gems/foreman-0.62.0\n/lib/foreman/engine.rb:373:in `loop'\nc:/Program Files (x86)/Heroku/ruby-1.9.2/lib/ruby/gems/1.9.1/gems/foreman-0.62.0\n/lib/foreman/engine.rb:373:in `block in watch_for_output'\n21:06:08 web.1  | exited with code 1\n21:06:08 system | sending SIGKILL to all processes\n\nI have no clue what the second set of errors is trying to tell me, since the file location it seems to claim engine.rb is running from does not even exist on my computer.\nI have looked at other answers to similar problems, however I am not receiving similar errors and so do not believe a solution to my problem currently exists.",
    "answer": "I had this problem. I fixed it by uninstalling version 0.62 of the foreman gem and installing 0.61.\ngem uninstall foreman\ngem install foreman -v 0.61"
  },
  {
    "question": "I am trying to export my application to another process management format/system (specifically, upstart). In doing so, I have come across a number of roadblocks, mostly due to lacking documentation.\nAs a non-root user, I ran the following command (as shown here):\n-bash> foreman export upstart /etc/init\nERROR: Could not create: /etc/init\n\nI \"could not create\" the directory due to inadequate permissions, so I used sudo:\n-bash> sudo foreman export upstart /etc/init\nPassword:\nERROR: Could not chown /var/log/app to app\n\nI \"could not chown... to app\" because there is no user named app.\nWhere is app coming from?\nHow should I use forman to export to upstart?",
    "answer": "app is default for both the name of the app and the name of the user the application should be run as when the corresponding options (--app and --user) are not used. See the foreman man page for the available options, but note that at the time of this writing the official synopsis did not include [options]:\nforeman export [options] <format> [location]\n\nExample:\n-bash> sudo foreman export --app foo --user bar upstart /etc/init\nPassword:\n[foreman export] writing: foo.conf\n[foreman export] writing: foo-web.conf\n[foreman export] writing: foo-web-1.conf\n[foreman export] writing: foo-worker.conf\n[foreman export] writing: foo-worker-1.conf\n\nResult:\n-bash> l /etc/init/\ntotal 80\ndrwxr-xr-x  12 root  wheel   408 20 Oct 09:31 .\ndrwxr-xr-x  94 root  wheel  3196 20 Oct 08:05 ..\n-rw-r--r--   1 root  wheel   236 20 Oct 09:31 foo-web-1.conf\n-rw-r--r--   1 root  wheel    41 20 Oct 09:31 foo-web.conf\n-rw-r--r--   1 root  wheel   220 20 Oct 09:31 foo-worker-1.conf\n-rw-r--r--   1 root  wheel    41 20 Oct 09:31 foo-worker.conf\n-rw-r--r--   1 root  wheel   315 20 Oct 09:31 foo.conf\n\n-bash> l /var/log/foo/\ntotal 0\ndrwxr-xr-x   2 bar     wheel    68 20 Oct 09:31 .\ndrwxr-xr-x  45 root    wheel  1530 20 Oct 09:31 .."
  },
  {
    "question": "I want the foreman gem to use the PORT value provided in the my development env file instead of using its own values. My files setup is shown below:\n\nA bash script to start foreman:\nforeman start -e development.env\nThe development.env file content:\nPORT=3000\nThe Procfile content\nweb: bundle exec rails server thin -p $PORT -e $RAILS_ENV $1\n\nThe dev server ends up starting on port 5000.  I know I can start foreman with --p 3000 to force it to use that port.  But that defeats the purpose of the env file.\nAny suggestions?",
    "answer": "I know this is an old post but it took me a while to figure out so might as well add a note here.\nForeman increments the PORT based on where your define the service in the Procfile.\nSay our PORT environment variable is set to 3000.\nIn our first Procfile example Puma will run on PORT 3000:\nweb: bundle exec puma -q -p $PORT\nworker: bundle exec rake jobs:work\n\nBut in our second Procfile it will run on PORT 3100 as the PORT variable is used on the second line.\nworker: bundle exec rake jobs:work\nweb: bundle exec puma -q -p $PORT\n\nNot sure why, I guess to prevent different processes from trying to take the same PORT."
  },
  {
    "question": "Say I execute the following.\n$ cat test.sh\n#!/bin/bash\necho Hello World\nexit 0\n\n$ cat Hello.yml\n---\n\n- hosts: MyTestHost\n  tasks:\n  - name: Hello yourself\n    script: test.sh\n\n\n$ ansible-playbook  Hello.yml\n\nPLAY [MyTestHost] ****************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [MyTestHost]\n\nTASK: [Hello yourself] ********************************************************\nok: [MyTestHost]\n\nPLAY RECAP ********************************************************************\nMyTestHost                    : ok=2    changed=0    unreachable=0    failed=0\n\n$\n\nI know for sure that it was successful.\nWhere/how do I see the \"Hello World\" echo'ed/printed by my script on the remote host (MyTestHost)? Or the return/exit code of script?\nMy research shows me it would be possible to write a plugin to intercept module execution callbacks or something on those lines and write a log file. I would prefer to not waste my time with that.\nE.g. something like the stdout in below (note that I'm running ansible and not ansible-playbook):\n$ ansible plabb54 -i /project/plab/svn/plab-maintenance/ansible/plab_hosts.txt -m script -a ./test.sh\nplabb54 | success >> {\n    \"rc\": 0,\n    \"stderr\": \"\",\n    \"stdout\": \"Hello World\\n\"\n}\n\n$",
    "answer": "If you pass the -v flag to ansible-playbook on the command line, you'll see the stdout and stderr for each task executed:\n$ ansible-playbook -v playbook.yaml\n\nAnsible also has built-in support for logging. Add the following lines to your ansible configuration file:\n[defaults] \nlog_path=/path/to/logfile\n\nAnsible will look in several places for the config file:\n\nansible.cfg in the current directory where you ran ansible-playbook\n~/.ansible.cfg\n/etc/ansible/ansible.cfg"
  },
  {
    "question": "Recently I created new roles called spd in my existing project. While other script works fine in the setup. This newly created fails. Please point me to what is going wrong here\nansible/roles\n      spd\n        tasks\n        templates\n        defaults\n\ndeploy-spd.yml\n - hosts:\n   roles:\n     - spd\n\ninventory file\n[kube-master]\nkubernetes-master-1 ansible_host=10.20.0.225 ansible_user=centos ansible_become=true\nkubernetes-master-2 ansible_host=10.20.0.226 ansible_user=centos ansible_become=true\nkubernetes-master-3 ansible_host=10.20.0.227 ansible_user=centos ansible_become=true\n\nFailure\nbash-4.3# ansible-playbook -i inventory/inventory deploy-test-ms.yml --ask-vault-pass\nVault password:\n\nPLAY [kube-master] *************************************************************\n\nTASK [setup] *******************************************************************\nThursday 16 March 2017  13:32:05 +0000 (0:00:00.026)       0:00:00.026 ********\nfatal: [kubernetes-master-1]: FAILED! => {\"failed\": true, \"msg\": \"to use the 'ssh' connection type with passwords, you must install the sshpass program\"}\nfatal: [kubernetes-master-2]: FAILED! => {\"failed\": true, \"msg\": \"to use the 'ssh' connection type with passwords, you must install the sshpass program\"}\nfatal: [kubernetes-master-3]: FAILED! => {\"failed\": true, \"msg\": \"to use the 'ssh' connection type with passwords, you must install the sshpass program\"}\n\nPLAY RECAP *********************************************************************\nkubernetes-master-1 : ok=0    changed=0    unreachable=0    failed=1\nkubernetes-master-2 : ok=0    changed=0    unreachable=0    failed=1\nkubernetes-master-3 : ok=0    changed=0    unreachable=0    failed=1\n\nUPDATE:\n**With failed script**    \n\n        Using module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/setup.py\n<10.20.0.227> ESTABLISH SSH CONNECTION FOR USER: centos\nUsing module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/setup.py\nUsing module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/setup.py\nUsing module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/setup.py\n<172.23.169.137> ESTABLISH SSH CONNECTION FOR USER: centos\n<10.20.0.225> ESTABLISH SSH CONNECTION FOR USER: centos\n<10.20.0.226> ESTABLISH SSH CONNECTION FOR USER: centos\n   \n**With successfull script**    \n\nThursday 16 March 2017  14:03:49 +0000 (0:00:00.066)       0:00:00.066 ********\nUsing module file /usr/lib/python2.7/site-packages/ansible/modules/core/system/setup.py\n<10.20.0.237> ESTABLISH SSH CONNECTION FOR USER: centos\n<10.20.0.237> SSH: EXEC ssh -F ./ssh.cfg -o ControlMaster=auto -o ControlPersist=30m -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey\n-o PasswordAuthentication=no -o User=centos -o ConnectTimeout=30 -o 'ControlPath=~/.ssh/ansible-%r@%h:%p' 10.20.0.237 '/bin/sh -c '\"'\"'( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1489673029.48-15997231643297\n4 `\" && echo ansible-tmp-1489673029.48-159972316432974=\"` echo $HOME/.ansible/tmp/ansible-tmp-1489673029.48-159972316432974 `\" ) && sleep 0'\"'\"''\n<10.20.0.237> PUT /tmp/tmpnHJPbc TO /home/centos/.ansible/tmp/ansible-tmp-1489673029.48-159972316432974/setup.py\n<10.20.0.237> SSH: EXEC scp -F ./ssh.cfg -o ControlMaster=auto -o ControlPersist=30m -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey\n-o PasswordAuthentication=no -o User=centos -o ConnectTimeout=30 -o 'ControlPath=~/.ssh/ansible-%r@%h:%p' /tmp/tmpnHJPbc '[10.20.0.237]:/home/centos/.ansible/tmp/ansible-tmp-1489673029.48-159972316432974/setup.py'\n<10.20.0.237> ESTABLISH SSH CONNECTION FOR USER: centos\n<10.20.0.237> SSH: EXEC ssh -F ./ssh.cfg -o ControlMaster=auto -o ControlPersist=30m -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey\n-o PasswordAuthentication=no -o User=centos -o ConnectTimeout=30 -o 'ControlPath=~/.ssh/ansible-%r@%h:%p' 10.20.0.237 '/bin/sh -c '\"'\"'chmod u+x /home/centos/.ansible/tmp/ansible-tmp-1489673029.48-159972316432974/ /home/cento\ns/.ansible/tmp/ansible-tmp-1489673029.48-159972316432974/setup.py && sleep 0'\"'\"''",
    "answer": "It is the host machine which needs the sshpass program installed. Again, this error message of:\n\nERROR! to use the 'ssh' connection type with passwords, you must install the sshpass program\n\nApplies to the HOST (provisioner) not the GUEST (machine(s) being provisioned). Thus install sshpass on the provisioner.\nInstall sshpass on Ubuntu 16.04 or similar\napt-get install sshpass\n\nInstall sshpass on Mac OS\nbrew install hudochenkov/sshpass/sshpass\n\nSource: How to install sshpass on Mac?"
  },
  {
    "question": "I have the following setup:\n\nMany different projects which are separate git repositories, but all have mostly the same server configuration\nEach project in turn depends on many other projects and we use the composer dependency manager to get them together (PHP language here).\n\nI want to use Vagrant and include a Vagrant file in each repository, so my team members can clone a repository, run vagrant up and be ready to go.\nMy question is now directed towards the provisioning. I need to install several tools and packages like apache, git, mysql and several php packages, then download some files (like a recent development db dump), set everything up in /var/www and run the composer install command.\nSo one option to do this is using a manager using recipes like chef or puppet.\nThe alternative would be to write a bash file and use shell provisioning.\nI have not much experience with chef / puppet, so naturally, it seems easier to use the shell option, but I want to understand if this is not a good / viable option in the long run.\nWhy to me it seems a bad approach to go with puppet / chef:\nI understand that I will have to use several different recipes and will almost always use the same recipes for my different repositories, so I would have to include all of them in all the repositories. Consider having 20 repos and needing 10 recipes, that means that I will need to add 200 recipes as a git-submodule or alike (also each team member needs to clone the repository, then clone 10 recipe repositories and only then run vagrant up for each project). In contrast, I would just need to have a small repo with my shell script and clone it 20 times.\nI am probably missing something, so please advice whether I should opt for chef / puppet and why it makes sense even if my repositories all have a very similar server setup.",
    "answer": "The following article concerns yet another CM tool (ansible), but I think the author does an excellent job of explaining the benefits of transitioning away from shell scripts.\nhttp://devopsu.com/blog/ansible-vs-shell-scripts/\nquote 1:\n\nWhat really surprised me was the response from some of these more famous devs. They basically said, \"This is really cool, but I probably won't read it since my manual-install/shell-script workflow is fine for now.\"\nI was a little shocked, but once I thought about it for a few minutes, I realized that their choice was perfectly sane and rational given what they knew about CM tools.\n\nquote 2:\n\nFor them, using a CM tool meant weeks of effort learning complex concepts, struggling with a complex installation process, and maintaining that complex system over time. They were somewhat aware of the benefits, but the costs of using a CM tool just seemed too high to make it worth the effort.\n\nThe benefits over shell scripts are summarized at the end and I think they apply to all CM tools, puppet, chef, salt, ansible...\n\nWhich method is most likely to end up in source control?\nWhich method can be run multiple times safely with confidence?\nWhich method can easily be run against multiple servers?\nWhich method actually verifies (tests) your server for correctness?\nWhich method can target certain servers easily (web, db, etc)?\nWhich method supports easily templating your configuration files?\nWhich method will grow to easily support your whole stack?\n\nHope this helps."
  },
  {
    "question": "When I define(?) a resource e.g. to ensure dir structure, are there any loops available?\nLike that:\n  for X in [app1,app2] do:\n    file { '/opt/app/' + X:\n      ensure => directory,\n      owner  => 'root',\n      group  => 'root',\n      mode   => '0644',\n    }\n\nI have tens of directories and I am really tired with declaring it in puppet.. It would take 15 LOC of bash.\nAny ideas?",
    "answer": "Older versions of the puppet language have no support for loops. \nBut you can use an array instead of a simple string for the title and declare several resources at the same time with the same params:\n$b = '/opt/app'\nfile { [ \"$b/app1\", \"$b/app2\" ]:\n  ensure => directory,\n  owner  => 'root',\n  group  => 'root',\n  mode   => 0644,\n}\n\nYou can also declare many resources of the same type with different params by ending each resource with a ;, which is a bit more compact than repeating the file and the {s and }s:\nfile {\n  [ \"$b/app1\", \"$b/app2\" ]:\n    ensure => directory,\n    owner  => 'root',\n    group  => 'root',\n    mode   => 0755;\n  [ \"$b/app1/secret\", \"$b/app2/secret\" ]:\n    ensure => directory,\n    owner  => 'root',\n    group  => 'root',\n    mode   => 0700;\n}\n\nIn the specific case of files, you can set up a source and use recursion:\nfile { \"/opt/app\":\n  source => \"puppet:///appsmodule/appsdir\",\n  recurse => true;\n}\n\n(that would require having a source of that directory structure for puppet to use as the source)\nYou can define a new resource type to reuse a portion of the param multiple times:\ndefine foo {\n  file {\n    \"/tmp/app/${title}\":\n      ensure => directory,\n      owner  => 'root',\n      mode   => 0755;\n    \"/tmp/otherapp/${title}\":\n      ensure => link,\n      target => \"/tmp/app/${title}\",\n      require => File[\"/tmp/app/${title}\"]\n  }\n}\n\nfoo { [\"app1\", \"app2\", \"app3\", \"app4\"]: } \n\nStarting with Puppet 2.6, there's a Ruby DSL available that has all the looping functionality you could ask for: http://www.puppetlabs.com/blog/ruby-dsl/ (I've never used it, however). In Puppet 3.2, they introduced some experimental loops, however those features may change or go away in later releases."
  },
  {
    "question": "I know about \npuppet agent --disable \"my message\"  --verbose\n\nbut I would like to know at some point on a given machine, what is its puppet agent status. I don't see how to do it from\nman puppet-agent\n\nIs there an command that would tell me if the agent is enabled or disabled ?\nThank you.\n-\n------------------- EDIT\nCentOS release 6.6 (Final)\n\nbash-4.1$ puppet --version\n3.7.4\nbash-4.1$ file /usr/bin/puppet \n/usr/bin/puppet: a /usr/bin/ruby script text executable\n\n------------------- EDIT2\nWhether it is enabled or disabled, I always get this:\n[root@p1al25 ~]# cat `sudo puppet agent --configprint agent_catalog_run_lockfile`\ncat: /var/lib/puppet/state/agent_catalog_run.lock: No such file or directory\n[root@p1al25 ~]# puppet agent --disable \"my message\"\n[root@p1al25 ~]# cat `sudo puppet agent --configprint agent_catalog_run_lockfile`\ncat: /var/lib/puppet/state/agent_catalog_run.lock: No such file or directory\n[root@p1al25 ~]# service puppet status\npuppet (pid  4387) is running...\n\n------------------- EDIT3\nThis one worked, thanks daxlerod\n[root@p1al25 ~]# service puppet status\npuppet (pid  4387) is running...\n[root@p1al25 ~]# puppet agent --disable \"my message\" --verbose\nNotice: Disabling Puppet.\n[root@p1al25 ~]# cat `puppet agent --configprint agent_disabled_lockfile` \n{\"disabled_message\":\"reason not specified\"}",
    "answer": "A one-liner to get the current status is:\ncat `puppet agent --configprint agent_disabled_lockfile`\n\nGenerally, this must be run as root, so I use:\nsudo cat `sudo puppet agent --configprint agent_disabled_lockfile`\n\nThere are a number of possible results.\n\ncat: \\path\\to\\lock: No such file or directory Puppet is not disabled.\nAny other text means that puppet is disabled, and the text is the reason provided when puppet was disabled by puppet agent --disable 'reason'"
  },
  {
    "question": "I have this in my Vagrantfile:\nVagrant.configure(\"2\") do |config|\n    config.vm.provision \"puppet\"\n  end\n\nYet, when I run puppet --version I get :\n[vagrant@vagrant-centos65 ~]$ puppet --version\n-bash: puppet: command not found\n\nDo I need to manually install puppet?",
    "answer": "No, (at the moment) Vagrant doesn't install it automatically.\nSo you either need to use a basebox which already has it installed (Puppet Labs provides boxes too), or you need to install it yourself. Probably the easiest way to install is to use shell provisioner before the puppet provisioner(s)."
  },
  {
    "question": "Anaconda python is installed (in linux) via a bash script.  I am trying to use Vagrant provisioning to get Anacaonda Python installed.\nIn the bash script (following the documentation bootstrap.sh example) I have a bootstrap.sh script that:\n\nwget the install script\nchmod +x to make it executable\n./<script>.sh to install. \n\nInstalling this way fails as the installation has a few prompts, one of which requires the non-default answer.\nIs it possible to automate the installation via a bash script?  If not, is it necessary to use something like Puppet?  I do not know Puppet at all, so have tried to avoid using...perhaps it is time to dig in?  \nThe end goal is to ship the Vagrantfile and not host a Vagrant box.\nP.S. My initial, feeble attempts made use of the linux yes command, but a better way has to exist!",
    "answer": "In your bootstrap.sh just include something like:\nminiconda=Miniconda3-3.7.4-Linux-x86_64.sh\ncd /vagrant\nif [[ ! -f $miniconda ]]; then\n    wget --quiet http://repo.continuum.io/miniconda/$miniconda\nfi\nchmod +x $miniconda\n./$miniconda -b -p /opt/anaconda\n\ncat >> /home/vagrant/.bashrc << END\n# add for anaconda install\nPATH=/opt/anaconda/bin:\\$PATH\nEND\n\nThe -b option runs in batch mode and is what you are looking for:\n>>>> ./Miniconda-3.7.0-Linux-x86_64.sh -h \nusage: ./Miniconda-3.7.0-Linux-x86_64.sh [options]\n\nInstalls Miniconda 3.7.0\n\n    -b           run install in batch mode (without manual intervention),\n                 it is expected the license terms are agreed upon\n    -f           no error if install prefix already exists\n    -h           print this help message and exit\n    -p PREFIX    install prefix, defaults to /Users/phil/miniconda\n\nI also typically put Miniconda (or a link to it) directly in the \"vagrant\" where the bootstrap.sh is.  That way, you are not downloading from the web during each vagrant up (after init or destroy)."
  },
  {
    "question": "I'm using puppet as my provisioner in one of my vagrant project. I'm trying to add a module for a custom bash_profile.\nThe module_path for puppet is set to: \n puppet.module_path = \"puppet/modules\"\n\nThe class for my bash_profile module looks like this:\nclass bash_profile\n{\n    file\n    {\n        \"/home/vagrant/bash_profile\":\n            ensure => present,\n            source => \"puppet:///modules/bash_profile/files/bash_profile\"\n    }\n}\n\nHere's the file structure for my puppet structure:\npuppet\n| manifests\n| | phpbase.pp // my main manifest file that has includes for modules\n| modules\n| | bash_profile\n| | | files\n| | | | bash_profile // the actual bash_profile file I want to ensure is present on my VM\n| | | manifests\n| | | | init.pp // the init file included for the bash_profile class\n\nWhen I run the provisioning for vagrant, I get the error \n\nerr: /Stage[main]/Bash_profile/File[/home/vagrant/bash_profile]: Could not evaluate: Could not retrieve information from environment production source(s) puppet:///modules/bash_profile/files/bash_profile at /tmp/vagrant-puppet-1/modules-0/bash_profile/manifests/init.pp:8\n\nI'm not sure why it can't retrieve the information. The path seems to be correct. Can anyone see what I'm missing?",
    "answer": "Yes, you are not supposed to include the literal files/ in the URL. Instead, it should just be\npuppet:///modules/bash_profile/bash_profile"
  },
  {
    "question": "I have worked out how to fire up a vagrant box and apt-get install packages\nI now want to git clone a node.js repo from github before running npm install and running the app with node app.js\nI expected to be able to achieve this by issuing BASH commands, but I see now that puppet requires this to be done in a puppety way. The results from Google on this issue are confusing and inconsistent.\nSo- how do you tell Puppet to git clone a node.js package from github and then install it with npm install?",
    "answer": "You can use the puppet-nodejs module to manage npm packages.  Take a look at https://forge.puppetlabs.com/puppetlabs/nodejs\nOtherwise, this article should explain how to clone a git repo. http://livecipher.blogspot.com.au/2013/01/deploy-code-from-git-using-puppet.html\nMore info can be found at https://github.com/puppetlabs/puppetlabs-vcsrepo.\nOnce installed you should be able to do something like:\nvcsrepo { \"/path/to/repo\":\n  ensure => present,\n  provider => git,\n  source => 'git://example.com/repo.git',\n  revision => 'master'\n}"
  },
  {
    "question": "I'm trying to work out the best way to set some environment variables with puppet.\nI could use exec and just do export VAR=blah.  However, that would only last for the current session. I also thought about just adding it onto the end of a file such as bashrc. However then I don't think there is a reliable method to check if it is all ready there; so it would end up getting added with every run of puppet.",
    "answer": "I would take a look at this related question. \n*.sh scripts in /etc/profile.d are read at user-login time (as the post says, at the same time /etc/profile is sourced)\nVariables export-ed in any script placed in /etc/profile.d will therefore be available to your users.\nYou can then use a file resource to ensure this action is idempotent. For example:\nfile { \"/etc/profile.d/my_test.sh\":\n  content => 'export MYVAR=\"123\"'\n}"
  },
  {
    "question": "I need to spin up a bunch of EC2 boxes for different users. Each user should be sandboxed from all the others, so each EC2 box needs its own SSH key.\nWhat's the best way to accomplish this in Terraform?\nAlmost all of the instructions I've found want me to manually create an SSH key and paste it into a terraform script.\n(Bad) Examples:\n\nhttps://github.com/hashicorp/terraform/issues/1243,\nhttp://2ninjas1blog.com/terraform-assigning-an-aws-key-pair-to-your-ec2-instance-resource/\nTerraform fails to import key pair with Amazon EC2)\n\nSince I need to programmatically generate unique keys for many users, this is impractical.\nThis doesn't seem like a difficult use case, but I can't find docs on it anywhere.\nIn a pinch, I could generate Terraform scripts and inject SSH keys on the fly using Bash. But that seems like exactly the kind of thing that Terraform is supposed to do in the first place.",
    "answer": "Terraform can generate SSL/SSH private keys using the tls_private_key resource.\nSo if you wanted to generate SSH keys on the fly you could do something like this:\nvariable \"key_name\" {}\n\nresource \"tls_private_key\" \"example\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"aws_key_pair\" \"generated_key\" {\n  key_name   = var.key_name\n  public_key = tls_private_key.example.public_key_openssh\n}\n\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"] # Canonical\n}\n\nresource \"aws_instance\" \"web\" {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type = \"t2.micro\"\n  key_name      = aws_key_pair.generated_key.key_name\n\n  tags {\n    Name = \"HelloWorld\"\n  }\n}\n\noutput \"private_key\" {\n  value     = tls_private_key.example.private_key_pem\n  sensitive = true\n}\n\n\nThis will create an SSH key pair that lives in the Terraform state (it is not written to disk in files other than what might be done for the Terraform state itself when not using remote state), creates an AWS key pair based on the public key and then creates an Ubuntu 20.04 instance where the ubuntu user is accessible with the private key that was generated.\nYou would then have to extract the private key from the state file and provide that to the users. You could use an output to spit this straight out to stdout when Terraform is applied.\nGetting the output from private key is via this command below:\nterraform output -raw private_key\n\nSecurity caveats\nI should point out here that passing private keys around is generally a bad idea and you'd be much better having developers create their own key pairs and provide you with the public key that you (or them) can use to generate an AWS key pair (potentially using the aws_key_pair resource as used in the above example) that can then be specified when creating instances.\nIn general I would only use something like the above way of generating SSH keys for very temporary dev environments that you are controlling so you don't need to pass private keys to anyone. If you do need to pass private keys to people you will need to make sure that you do this in a secure channel and that you make sure the Terraform state (which contains the private key in plain text) is also secured appropriately."
  },
  {
    "question": "We have cronjob and shell script which we want to copy or upload to aws ec2 instance while creating instance using terraform. \nwe tried\n\nfile provisioner : \nbut its not wokring , and read this option does not work with all terraform version\n\n      provisioner \"file\" {\n        source      = \"abc.sh\"\n        destination = \"/home/ec2-user/basic2.sh\"\n      }\n\n\ntried data template file option \n\n    data \"template_file\" \"userdata_line\" {\n      template = <<EOF\n    #!/bin/bash\n    mkdir /home/ec2-user/files2\n    cd /home/ec2-user/files2\n    sudo touch basic2.sh\n    sudo chmod 777 basic2.sh\n    base64 basic.sh |base64 -d >basic2.sh\n    EOF\n    }\n\ntried all option but none of them working.\ncould u please help or advise .\nI am new to terraform so struggling on this from long time.",
    "answer": "When starting from an AMI that has cloud-init installed (which is common in many official Linux distri), we can use cloud-init's write_files module to place arbitrary files into the filesystem, as long as they are small enough to fit within the constraints of the user_data argument along with all of the other cloud-init data.\nAs with all cloud-init modules, we configure write_files using cloud-init's YAML-based configuration format, which begins with the special marker string #cloud-config on a line of its own, followed by a YAML data structure. Because JSON is a subset of YAML, we can use Terraform's jsonencode to produce a valid value[1].\nlocals {\n  cloud_config_config = <<-END\n    #cloud-config\n    ${jsonencode({\n      write_files = [\n        {\n          path        = \"/etc/example.txt\"\n          permissions = \"0644\"\n          owner       = \"root:root\"\n          encoding    = \"b64\"\n          content     = filebase64(\"${path.module}/example.txt\")\n        },\n      ]\n    })}\n  END\n}\n\nThe write_files module can accept data in base64 format when we set encoding = \"b64\", so we use that in conjunction with Terraform's filebase64 function to include the contents of an external file. Other approaches are possible here, such as producing a string dynamically using Terraform templates and using base64encode to encode it as the file contents.\nIf you can express everything you want cloud-init to do in a single configuration file like the above then you can assign local.cloud_config_config directly as your instance user_data, and cloud-config will should recognize and process it on system boot:\n  user_data = local.cloud_config_config\n\nIf you instead need to combine creating the file with some other actions, like running a shell script, you can use cloud-init's multipart archive format to encode multiple \"files\" for cloud-init to process. Terraform has a cloudinit provider that contains a data source for easily constructing a multipart archive for cloud-init:\ndata \"cloudinit_config\" \"examplecfg\" {\n  gzip          = false\n  base64_encode = false\n\n  part {\n    content_type = \"text/cloud-config\"\n    filename     = \"cloud-config.yaml\"\n    content      = local.cloud_config_config\n  }\n\n  part {\n    content_type = \"text/x-shellscript\"\n    filename     = \"example.sh\"\n    content  = <<-EOF\n      #!/bin/bash\n      echo \"Hello World\"\n    EOF\n  }\n}\n\nThis data source will produce a single string at cloudinit_config.example.rendered which is a multipart archive suitable for use as user_data for cloud-init:\n  user_data = data.cloudinit_config.examplecfg.rendered\n\nEC2 imposes a maximum user-data size of 64 kilobytes, so all of the encoded data together must fit within that limit. If you need to place a large file that comes close to or exceeds that limit, it would probably be best to use an intermediate other system to transfer that file, such as having Terraform write the file into an Amazon S3 bucket and having the software in your instance retrieve that data using instance profile credentials. That shouldn't be necessary for small data files used for system configuration, though.\nIt's important to note that from the perspective of Terraform and EC2 the content of user_data is just an arbitrary string. Any issues in processing the string must be debugged within the target operating system itself, by reading the cloud-init logs to see how it interpreted the configuration and what happened when it tried to take those actions.\n\n[1]: We could also potentially use yamlencode, but at the time I write this that function has a warning that its exact formatting may change in future Terraform versions, and that's undesirable for user_data because it would cause the instance to be replaced. If you are reading this in the future and that warning is no longer present in the yamldecode docs, consider using yamlencode instead."
  },
  {
    "question": "what I'm trying to accomplish is to run commands inside of a Docker container that has already been created on a Digital Ocean Ubuntu/Docker Droplet using Ansible.\nCan't seem to find anything on this, or I'm majorly missing something. This is my Ansible task in my play book. I'm very new to Ansible so any advice or wisdom would be greatly appreciated.\n- name: Test Deploy\n    hosts: [my-cluster-of-servers]\n\ntasks: \n  - name: Go Into Docker Container And Run Multiple Commands\n    docker:\n      name: [container-name]\n      image: [image-ive-created-container-with-on-server]\n      state: present\n      command: docker exec -it [container-name] bash",
    "answer": "After discussion with some very helpful developers on the ansible github project, a better way to do this is like so:\n- name: add container to inventory\n  add_host:\n    name: [container-name]\n    ansible_connection: docker\n  changed_when: false\n\n- name: run command in container\n  delegate_to: [container-name]\n  raw: bash\n\nIf you have python installed in your image, you can use the command module or any other module instead of raw.\nIf you want to do this on a remote docker host, add:\nansible_docker_extra_args: \"-H=tcp://[docker-host]:[api port]\"\n\nto the add_host block.\nSee the Ansible documentation for a more complete example."
  },
  {
    "question": "Background\nMy question seems simple, but it gets more complex really fast.\nBasically, I got really tired of maintaining my servers manually (screams in background) and I decided it was time to find a way to make being a server admin much more liveable. That's when I found Ansible. Great huh? Sure beats making bash scripts (louder scream) for everything I wanted to automate.\nWhat's the problem?\nI'm having a lot of trouble figuring out what user my Ansible playbook will run certain things as. I also need the ability to specify what user certain tasks will run as. Here are some specific use cases:\nCloning a repo as another user:\nMy purpose with this is it run my node.js webapp from another user, who we'll call bill (that can only use sudo to run a script that I made that starts the node server, as opposed to root or my user that can use sudo for all commands). To do this, I need the ability to have Ansible's git module clone my git repo as bill. How would I do that?\nKnowing how Ansible will gain root:\nAs far as I understand, you can set what user Ansible will connect to the server you're maintaining by defining 'user' and the beginning of the playbook file. Here's what I don't understand: if I tell it to connect via my username, joe, and ask it to update a package via the apt module, how will it gain root? Sudo usually prompts me for my password, and I'd prefer keeping it that way (for security).\nFinal request\nI've scoured the Ansible docs, done some (what I thought was thorough) Googling, and generally just tried to figure it out on my own, but this information continues to elude me.\nI am very new to Ansible, and while it's mostly straight-forwards, I would benefit greatly if I could understand exactly how Ansible runs, on which users it runs, and how/where I can specify what user to use at different times.\nThank you tons in advance",
    "answer": "You may find it useful to read the Hosts and Users section on Ansible's documentation site:\nhttp://docs.ansible.com/playbooks_intro.html#hosts-and-users\nIn summary, ansible will run all commands in a playbook as the user specified in the remote_user variable (assuming you're using ansible >= 1.4, user before that). You can specify this variable on a per-task basis as well, in case a task needs to run as a certain user.\nUse sudo: true in any playbook/task to use sudo to run it. Use the sudo_user variable to specify a user to sudo to if you don't want to use root.\nIn practice, I've found it easiest to run my playbook as a deploy user that has sudo privileges. I set up my SSH keys so I can SSH into any host as deploy without using a password. This means that I can run my playbook without using a password and even use sudo if I need to.\nI use this same user to do things like cloning git repos and starting/stopping services. If a service needs to run as a lower-privileged user, I let the init script take care of that. A quick Google search for a node.js init.d script revealed this one for CentOS:\nhttps://gist.github.com/nariyu/1211413\nDoing things this way helps to keep it simple, which I like.\nHope that helps."
  },
  {
    "question": "I would expect this to be pretty simple. I'm using the lineinfile module like so:\n- name: Update bashrc for PythonBrew for foo user\n  lineinfile:\n    dest=/home/foo/.bashrc\n    backup=yes\n    line=\"[[ -s ${pythonbrew.bashrc_path} ]] && source ${pythonbrew.bashrc_path}\"\n    owner=foo\n    regexp='^'\n    state=present\n    insertafter=EOF\n    create=True\n\nThe problem I'm having is that it's replacing the last line in the file (which is fi) with my new line rather than appending the line. This produces a syntax error.\nDo I have the parameters correct? I've tried setting regexp to both '^' and '' (blank). Is there another way to go about this?\nI'm using Ansible 1.3.3.",
    "answer": "The Ansible discussion group helped get me sorted out on this. The problem is the regexp parameter. \nSince I only want the line appended to the file once, I need the regexp to match the line as closely as possible. This is complicated in my case by the fact that my line includes variables. But, assuming the line started [[ -s $HOME/.pythonbrew, I found the following to be sufficient:\n- name: Update bashrc for PythonBrew for foo user\n  lineinfile:\n    dest: /home/foo/.bashrc\n    line: \"[[ -s ${pythonbrew.bashrc_path} ]] && source ${pythonbrew.bashrc_path}\"\n    regexp: \"^\\[\\[ -s \\\\$HOME/\\.pythonbrew\"\n    owner: foo\n    state: present\n    insertafter: EOF\n    create: True"
  },
  {
    "question": "This seems like it should be really simple:\ntasks:\n- name: install python packages\n  pip: name=${item} virtualenv=~/buildbot-env\n  with_items: [ buildbot ]\n- name: create buildbot master\n  command: buildbot create-master ~/buildbot creates=~/buildbot/buildbot.tac\n\nHowever, the command will not succeed unless the virtualenv's activate script is sourced first, and there doesn't seem to be provision to do that in the Ansible command module.\nI've experimented with sourcing the activate script in various of .profile, .bashrc, .bash_login, etc, with no luck. Alternatively, there's the shell command, but it seems like kind of an awkward hack:\n- name: create buildbot master\n  shell: source ~/buildbot-env/bin/activate && \\\n         buildbot create-master ~/buildbot \\\n         creates=~/buildbot/buildbot.tac executable=/bin/bash\n\nIs there a better way?",
    "answer": "The better way is to use the full path to installed script - it will run in its virtualenv automatically:\ntasks:\n- name: install python packages\n  pip: name={{ item }} virtualenv={{ venv }}\n  with_items: [ buildbot ]\n- name: create buildbot master\n  command: \"{{ venv }}/bin/buildbot create-master ~/buildbot\n            creates=~/buildbot/buildbot.tac\""
  },
  {
    "question": "I've created an Ansible playbook that creates a cloud instance and then installs some programs on the instance. I want to run this playbook multiple times (without using a bash script). Is it possible to use a loop to loop over those two tasks together (I.E. One loop for two tasks?). All I've been able to find so far is one loop for each individual task",
    "answer": "An update:\n\nIn 2.0 you are able to use with_ loops and task includes (but not playbook includes), this adds the ability to loop over the set of tasks in one shot. There are a couple of things that you need to keep in mind, a included task that has it’s own with_ loop will overwrite the value of the special item variable. So if you want access to both the include’s item and the current task’s item you should use set_fact to create a alias to the outer one.:\n\n\n- include_tasks: test.yml\n\n\n  with_items:\n    - 1\n    - 2\n    - 3\n\n\nin test.yml:\n\n\n- set_fact: outer_loop=\"{{item}}\"\n- debug: msg=\"outer item={{outer_loop}} inner item={{item}}\"\n\n\n  with_items:\n    - a\n    - b\n    - c\n\nSource: Ansible Docs"
  },
  {
    "question": "I noticed Ansible removes the temporary script using a semi-colon to separate the bash commands.  \nHere is an example command:\nEXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o \nControlPath=\"/Users/devuser/.ansible/cp/ansible-ssh-%h-%p-%r\" -o\nKbdInteractiveAuthentication=no -o\nPreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey\n-o PasswordAuthentication=no -o ConnectTimeout=10 build /bin/sh -c\n'LANG=en_US.UTF-8 LC_CTYPE=en_US.UTF-8 /usr/bin/python\n/home/ec2-user/.ansible/tmp/ansible-tmp-1430847489.81-75617096172775/docker;\nrm -rf\n/home/ec2-user/.ansible/tmp/ansible-tmp-1430847489.81-75617096172775/\n>/dev/null 2>&1'\n\nIs there a way to tell ansible to replace the semi-colon with a double ampersand or to tell it to save the script or output the contents when running ansible-playbook?\nI'm trying to debug an error in this script and right now the only thing that appears is this:\nfailed: [build] => {\"changed\": false, \"failed\": true}\nmsg: ConnectionError(ProtocolError('Connection aborted.', error(2, 'No such file or directory')),)",
    "answer": "I found the environment variable -\nexport ANSIBLE_KEEP_REMOTE_FILES=1\n\nSet this, then re-run ansible-playbook, and then ssh and cd over to ~/.ansible/tmp/ to find the files."
  },
  {
    "question": "I would like to be able to prompt for my super secure password variable if it is not already in the environment variables. (I'm thinking that I might not want to put the definition into .bash_profile or one of the other spots.)\nThis is not working. It always prompts me. \nvars:\n  THISUSER: \"{{ lookup('env','LOGNAME') }}\"\n  SSHPWD:   \"{{ lookup('env','MY_PWD') }}\"\n\nvars_prompt:\n  - name: \"release_version\"\n    prompt: \"Product release version\"\n    default: \"1.0\"\n    when: SSHPWD == null\n\nNOTE: I'm on a Mac, but I'd like for any solutions to be platform-independent.",
    "answer": "According to the replies from the devs and a quick test I've done with the latest version, the vars_prompt is run before \"GATHERING FACTS\". This means that the env var SSHPWD is always null at the time of your check with when.\nUnfortunately it seems there is no way of allowing the vars_prompt statement at task level.\nMichael DeHaan's reasoning for this is that allowing prompts at the task-level would open up the doors to roles asking a lot of questions.  This would make using Ansible Galaxy roles which do this difficult:\n\nThere's been a decided emphasis in automation in Ansible and asking questions at task level is not something we really want to do.\nHowever, you can still ask vars_prompt questions at play level and use those variables throughout tasks.   You just can't ask questions in roles.\nAnd really, that's what I would like to enforce -- if a lot of Galaxy roles start asking questions, I can see that being annoying :)"
  },
  {
    "question": "I have an ansible file (my_file.yml) that looks something like this:\n---\n- name: The name\n  hosts: all\n  tasks:\n\n    - include:my_tasks.yml\n      vars:\n          my_var: \"{{ my_var }}\"\n\nmy_tasks.yml looks like this:\n- name: Install Curl\n  apt: pkg=curl state=installed\n\n- name: My task\n  command: bash -c \"curl -sSL http://x.com/file-{{ my_var }} > /tmp/file.deb\"\n\nI'd like to pass my_var as a command-line argument to ansible so I do like this:\nansible-playbook my_file.yml --extra-vars \"my_var=1.2.3\"\n\nBut I end up with the following error:\n... Failed to template {{ my_var }}: Failed to template {{ my_var }}: recursive loop detected in template string: {{ my_var }}\n\nIf I the vars in my_file.yml to look like this:\n- include:my_tasks.yml\n  vars:\n      my_var: \"1.2.3\"\n\nit works! I've also tried changing the variable name to something that is not equal to my_var, for example:\n- include:my_tasks.yml\n  vars:\n      my_var: \"{{ my_var0 }}\"\n\nbut then I end up with an error. It seems to me that the variable is not expanded and instead the string \"{{ my_var }}\" or {{ my_var0 }} is passed to my_tasks.yml. How do I solve this?",
    "answer": "Faced the same issue in my project. It turns out that the variable name in the playbook and the task have to be different.\n---\n- name: The name\n  hosts: all\n  vars:\n    my_var_play: \"I need to send this value to the task\"\n    some_other_var: \"This is directly accessible in task\"\n  tasks:\n    - include:my_tasks.yml\n      vars:\n          my_var: \"{{ my_var_play }}\"\n\nAlso on a sidenote, all the variables in the playbook is accessible in the task. Just use {{ some_other_var }} in task and it should work fine."
  },
  {
    "question": "I am currently working through the beta book \"Terraform Up & Running, 2nd Edition\". In chapter 2, I created an auto scaling group and a load balancer in AWS.\nNow I made my backend server HTTP ports configurable. By default they listen on port 8080.\nvariable \"server_port\" {\n    …\n    default = 8080\n}\n\nresource \"aws_launch_configuration\" \"example\" {\n    …\n    user_data = <<-EOF\n                #!/bin/bash\n                echo \"Hello, World\" > index.html\n                nohup busybox httpd -f -p ${var.server_port} &\n                EOF\n    …\n}\n\nresource \"aws_security_group\" \"instance\" {\n    …\n    ingress {\n        from_port = var.server_port\n        to_port = var.server_port\n        …\n    }\n}\n\nThe same port also needs to be configured in the application load balancer's target group.\nresource \"aws_lb_target_group\" \"asg\" {\n    …\n    port = var.server_port\n    …\n}\n\nWhen my infrastructure is already deployed, for example with the configuration for the port set to 8080, and then I change the variable to 80 by running terraform apply --var server_port=80, the following error is reported:\n> Error: Error deleting Target Group: ResourceInUse: Target group\n> 'arn:aws:elasticloadbalancing:eu-central-1:…:targetgroup/terraform-asg-example/…'\n> is currently in use by a listener or a rule   status code: 400,\n\nHow can I refine my Terraform infrastructure definition to make this change possible? I suppose it might be related to a lifecycle option somewhere, but I didn't manage to figure it out yet.\n\nFor your reference I attach my whole infrastructure definition below:\nprovider \"aws\" {\n    region = \"eu-central-1\"\n}\n\noutput \"alb_location\" {\n    value = \"http://${aws_lb.example.dns_name}\"\n    description = \"The location of the load balancer\"\n}\n\nvariable \"server_port\" {\n    description = \"The port the server will use for HTTP requests\"\n    type = number\n    default = 8080\n}\n\nresource \"aws_lb_listener_rule\" \"asg\" {\n    listener_arn = aws_lb_listener.http.arn\n    priority = 100\n\n    condition {\n        field = \"path-pattern\"\n        values = [\"*\"]\n    }\n\n    action {\n        type = \"forward\"\n        target_group_arn = aws_lb_target_group.asg.arn\n    }\n}\n\nresource \"aws_lb_target_group\" \"asg\" {\n    name = \"terraform-asg-example\"\n    port = var.server_port\n    protocol = \"HTTP\"\n    vpc_id = data.aws_vpc.default.id\n\n    health_check {\n        path = \"/\"\n        protocol = \"HTTP\"\n        matcher = \"200\"\n        interval = 15\n        timeout = 3\n        healthy_threshold = 2\n        unhealthy_threshold = 2\n    }\n}\n\nresource \"aws_lb_listener\" \"http\" {\n    load_balancer_arn = aws_lb.example.arn\n    port = 80\n    protocol = \"HTTP\"\n\n    default_action {\n        type = \"fixed-response\"\n\n        fixed_response {\n            content_type = \"text/plain\"\n            message_body = \"404: page not found\"\n            status_code = 404\n        }\n    }\n}\n\nresource \"aws_lb\" \"example\" {\n    name = \"terraform-asg-example\"\n    load_balancer_type = \"application\"\n    subnets = data.aws_subnet_ids.default.ids\n    security_groups = [aws_security_group.alb.id]\n}\n\nresource \"aws_security_group\" \"alb\" {\n    name = \"terraform-example-alb\"\n\n    ingress {\n        from_port = 80\n        to_port = 80\n        protocol = \"tcp\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n\n    egress {\n        from_port = 0\n        to_port = 0\n        protocol = \"-1\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n}\n\nresource \"aws_autoscaling_group\" \"example\" {\n    launch_configuration = aws_launch_configuration.example.name\n    vpc_zone_identifier = data.aws_subnet_ids.default.ids\n\n    target_group_arns = [aws_lb_target_group.asg.arn]\n    health_check_type = \"ELB\"\n\n    min_size = 2\n    max_size = 10\n\n    tag {\n        key = \"Name\"\n        value = \"terraform-asg-example\"\n        propagate_at_launch = true\n    }\n}\n\nresource \"aws_launch_configuration\" \"example\" {\n    image_id = \"ami-0085d4f8878cddc81\"\n    instance_type = \"t2.micro\"\n    security_groups = [aws_security_group.instance.id]\n\n    user_data = <<-EOF\n                #!/bin/bash\n                echo \"Hello, World\" > index.html\n                nohup busybox httpd -f -p ${var.server_port} &\n                EOF\n    lifecycle {\n        create_before_destroy = true\n    }\n}\n\nresource \"aws_security_group\" \"instance\" {\n    name = \"terraform-example-instance\"\n\n    ingress {\n        from_port = var.server_port\n        to_port = var.server_port\n        protocol = \"tcp\"\n        cidr_blocks = [\"0.0.0.0/0\"]\n    }\n}\n\ndata \"aws_subnet_ids\" \"default\" {\n    vpc_id = data.aws_vpc.default.id\n}\n\ndata \"aws_vpc\" \"default\" {\n    default = true\n}",
    "answer": "From the issue link in the comment on Cannot rename ALB Target Group if Listener present:\nAdd a lifecycle rule to your target group so it becomes:\nresource \"aws_lb_target_group\" \"asg\" {\n    name     = \"terraform-asg-example\"\n    port     = var.server_port\n    protocol = \"HTTP\"\n    vpc_id   = data.aws_vpc.default.id\n\n    health_check {\n      path                = \"/\"\n      protocol            = \"HTTP\"\n      matcher             = \"200\"\n      interval            = 15\n      timeout             = 3\n      healthy_threshold   = 2\n      unhealthy_threshold = 2\n    }\n\n    lifecycle {\n      create_before_destroy = true\n    }\n}\n\nHowever you will need to choose a method for changing the name of your target group as well. There is further discussion and suggestions on how to do this.\nBut one possible solution is to simply use a guid but ignore changes to the name:\nresource \"aws_lb_target_group\" \"asg\" {\n    name     = \"terraform-asg-example-${substr(uuid(), 0, 3)}\"\n    port     = var.server_port\n    protocol = \"HTTP\"\n    vpc_id   = data.aws_vpc.default.id\n\n    health_check {\n      path                = \"/\"\n      protocol            = \"HTTP\"\n      matcher             = \"200\"\n      interval            = 15\n      timeout             = 3\n      healthy_threshold   = 2\n      unhealthy_threshold = 2\n    }\n\n    lifecycle {\n      create_before_destroy = true\n      ignore_changes        = [name]\n    }\n}"
  },
  {
    "question": "I am using AWS CodeBuild along with Terraform for automated deployment of a Lambda based service. I have a very simple buildscript.yml that accomplishes the following:\n\nGet dependencies\nRun Tests\nGet AWS credentials and save to file (detailed below)\nSource the creds file\nRun Terraform\n\nThe step \"source the creds file\" is where I am having my difficulty. I have a simply bash one-liner that grabs the AWS container creds off of curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI and then saves them to a file in the following format:\nexport AWS_ACCESS_KEY_ID=SOMEACCESSKEY\nexport AWS_SECRET_ACCESS_KEY=MYSECRETKEY\nexport AWS_SESSION_TOKEN=MYSESSIONTOKEN\n\nOf course, the obvious step is to simply source this file so that these variables can be added to my environment for Terraform to use. However, when I do source /path/to/creds_file.txt, CodeBuild returns: \n[Container] 2017/06/28 18:28:26 Running command source /path/to/creds_file.txt\n/codebuild/output/tmp/script.sh: 4: /codebuild/output/tmp/script.sh: source: not found\n\nI have tried to install source through apt but then I get an error saying that source cannot be found (yes, I've run apt update etc.). I am using a standard Ubuntu image with the Python 2.7 environment for CodeBuild. What can I do to either get Terraform working credentials for source this credentials file in Codebuild.\nThanks!",
    "answer": "Try using . instead of source. source is not POSIX compliant. ss64.com/bash/source.html"
  },
  {
    "question": "Is there a way in Terraform to check if a resource in Google Cloud exists prior to trying to create it?\nI want to check if the following resources below exist in my CircleCI CI/CD pipeline during a job. I have access to terminal commands, bash, and gcloud commands. If the resources do exist, I want to use them. If they do not exist, I want to create them. I am doing this logic in CircleCI's config.yml as steps where I have access to terminal commands and bash. My goal is to create my necessary infrastructure (resources) in GCP when they are needed, otherwise use them if they are created, without getting Terraform errors in my CI/CD builds.\nIf I try to create a resource that already exists, Terraform apply will result in an error saying something like, \"you already own this resource,\" and now my CI/CD job fails.\nBelow is pseudo code describing the resources I am trying to get.\nresource \"google_artifact_registry_repository\" \"main\" {\n  # this is the repo for hosting my Docker images\n  # it does not have a data source afaik because it is beta\n}\n\nFor my google_artifact_registry_repository resource. One approach I have is to do a Terraform apply using a data source block and see if a value is returned. The problem with this is that the google_artifact_registry_repository does not have a data source block. Therefore, I must create this resource once using a resource block and every CI/CD build thereafter can rely on it being there. Is there a work-around to read that it exists?\nresource \"google_storage_bucket\" \"bucket\" {\n  # bucket containing the folder below\n}\n\nresource \"google_storage_bucket_object\" \"content_folder\" {\n  # folder containing Terraform default.tfstate for my Cloud Run Service\n}\n\nFor my google_storage_bucket and google_storage_bucket_object resources. If I do a Terraform apply using a data source block to see if these exist, one issue I run into is when the resources are not found, Terraform takes forever to return that status. It would be great if I could determine if a resource exists within like 10-15 seconds or something, and if not assume these resources do not exist.\ndata \"google_storage_bucket\" \"bucket\" {\n  # bucket containing the folder below\n}\n\noutput bucket {\n  value = data.google_storage_bucket.bucket\n}\n\nWhen the resource exists, I can use Terraform output bucket to get that value. If it does not exist, Terraform takes too long to return a response. Any ideas on this?",
    "answer": "TF does not have any build in tools for checking if there are pre-existing resources, as this is not what TF is meant to do. However, you can create your own custom data source.\nUsing the custom data source you can program any logic you want, including checking for pre-existing resources and return that information to TF for future use."
  },
  {
    "question": "I'm having a terrible time getting Terraform to assume an IAM role with another account with MFA required. Here's my setup\nAWS Config\n[default]\nregion = us-west-2\noutput = json\n\n[profile GEHC-000]\nregion = us-west-2\noutput = json\n\n....\n\n[profile GEHC-056]\nsource_profile = GEHC-000\nrole_arn = arn:aws:iam::~069:role/hc/hc-master\nmfa_serial = arn:aws:iam::~183:mfa/username\nexternal_id = ~069\n\nAWS Credentials\n[default]\naws_access_key_id = xxx\naws_secret_access_key = xxx\n\n\n[GEHC-000]\naws_access_key_id = same as above\naws_secret_access_key = same as above\n\nPolicies assigned to IAM user\nSTS Policy\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AssumeRole\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::*:role/hc/hc-master\"\n            ]\n        }\n    ]\n}\n\nUser Policy\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"iam:*AccessKey*\",\n                \"iam:*MFA*\",\n                \"iam:*SigningCertificate*\",\n                \"iam:UpdateLoginProfile*\",\n                \"iam:RemoveUserFromGroup*\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:iam::~183:mfa/${aws:username}\",\n                \"arn:aws:iam::~183:mfa/*/${aws:username}\",\n                \"arn:aws:iam::~183:mfa/*/*/${aws:username}\",\n                \"arn:aws:iam::~183:mfa/*/*/*${aws:username}\",\n                \"arn:aws:iam::~183:user/${aws:username}\",\n                \"arn:aws:iam::~183:user/*/${aws:username}\",\n                \"arn:aws:iam::~183:user/*/*/${aws:username}\",\n                \"arn:aws:iam::~183:user/*/*/*${aws:username}\"\n            ],\n            \"Sid\": \"Write\"\n        },\n        {\n            \"Action\": [\n                \"iam:*Get*\",\n                \"iam:*List*\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Sid\": \"Read\"\n        },\n        {\n            \"Action\": [\n                \"iam:CreateUser*\",\n                \"iam:UpdateUser*\",\n                \"iam:AddUserToGroup\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"*\"\n            ],\n            \"Sid\": \"CreateUser\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n\nForce MFA Policy\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"BlockAnyAccessOtherThanAboveUnlessSignedInWithMFA\",\n            \"Effect\": \"Deny\",\n            \"NotAction\": \"iam:*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"BoolIfExists\": {\n                    \"aws:MultiFactorAuthPresent\": \"false\"\n                }\n            }\n        }\n    ]\n}\n\nmain.tf\nprovider \"aws\" {\n  profile                 = \"GEHC-056\"\n  shared_credentials_file = \"${pathexpand(\"~/.aws/config\")}\"\n  region                  = \"${var.region}\"\n}\n\ndata \"aws_iam_policy_document\" \"test\" {\n  statement {\n    sid    = \"TestAssumeRole\"\n    effect = \"Allow\"\n\n    actions = [\n      \"sts:AssumeRole\",\n    ]\n\n    principals = {\n      type = \"AWS\"\n\n      identifiers = [\n        \"arn:aws:iam::~183:role/hc-devops\",\n      ]\n    }\n\n    sid    = \"BuUserTrustDocument\"\n    effect = \"Allow\"\n\n    principals = {\n      type = \"Federated\"\n\n      identifiers = [\n        \"arn:aws:iam::~875:saml-provider/ge-saml-for-aws\",\n      ]\n    }\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"SAML:aud\"\n      values   = [\"https://signin.aws.amazon.com/saml\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"test_role\" {\n  name               = \"test_role\"\n  path               = \"/\"\n  assume_role_policy = \"${data.aws_iam_policy_document.test.json}\"\n}\n\nGet Caller Identity\nbash-4.4$ aws --profile GEHC-056 sts get-caller-identity\nEnter MFA code for arn:aws:iam::772660252183:mfa/503072343:\n{\n  \"UserId\": \"AROAIWCCLC2BGRPQMJC7U:botocore-session-1537474244\",\n  \"Account\": \"730993910069\",\n  \"Arn\": \"arn:aws:sts::730993910069:assumed-role/hc-master/botocore-session-1537474244\"\n}\n\nAnd the error:\nbash-4.4$ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\n\nError: Error refreshing state: 1 error(s) occurred:\n\n* provider.aws: Error creating AWS session: AssumeRoleTokenProviderNotSetError: assume role with MFA enabled, but AssumeRoleTokenProvider session option not set.",
    "answer": "Terraform doesn't currently support prompting for the MFA token when being ran as it is intended to be ran in a less interactive fashion as much as possible and it would apparently require significant rework of the provider structure to support this interactive provider configuration. There's more discussion about this in this issue.\nAs also mentioned in that issue the best bet is to use some form of script/tool that already assumes the role prior to running Terraform.\nI personally use AWS-Vault and have written a small shim shell script that I symlink to from terraform (and other things such as aws that I want to use AWS-Vault to grab credentials for) that detects what it's being called as, finds the \"real\" binary using which -a, and then uses AWS-Vault's exec to run the target command with the specified credentials.\nMy script looks like this:\n#!/bin/bash\n\nset -eo pipefail\n\n# Provides a shim to override target executables so that it is executed through aws-vault\n# See https://github.com/99designs/aws-vault/blob/ae56f73f630601fc36f0d68c9df19ac53e987369/USAGE.md#overriding-the-aws-cli-to-use-aws-vault for more information about using it for the AWS CLI.\n\n# Work out what we're shimming and then find the non shim version so we can execute that.\n# which -a returns a sorted list of the order of binaries that are on the PATH so we want the second one.\nINVOKED=$(basename $0)\nTARGET=$(which -a ${INVOKED} | tail -n +2 | head -n 1)\n\nif [ -z ${AWS_VAULT} ]; then\n    AWS_PROFILE=\"${AWS_DEFAULT_PROFILE:-read-only}\"\n    (>&2 echo \"Using temporary credentials from ${AWS_PROFILE} profile...\")\n\n    exec aws-vault exec \"${AWS_PROFILE}\" --assume-role-ttl=60m -- \"${TARGET}\" \"$@\"\nelse\n    # If AWS_VAULT is already set then we want to just use the existing session instead of nesting them\n    exec \"${TARGET}\" \"$@\"\nfi\n\nIt will use a profile in your ~/.aws/config file that matches the AWS_DEFAULT_PROFILE environment variable you have set, defaulting to a read-only profile which may or may not be a useful default for you. This makes sure that AWS-Vault assumes the IAM role, grabs the credentials and sets them as environment variables for the target process.\nThis means that as far as Terraform is concerned it is being given credentials via environment variables and this just works."
  },
  {
    "question": "I'm currently writing an Ansible play that follows this general format and is run via a cron job: \npre_tasks:\n  -Configuration / package installation\n\ntasks:\n  -Work with installed packages\n\npost_tasks:\n  -Cleanup / uninstall packages\n\nThe problem with the above is that sometimes a command in the tasks section fails, and when it does the post_tasks section doesn't run, leaving the system in a messy state. Is it possible to force the commands in post_tasks to run even if a failure or fatal error occurs?\nMy current approach is to apply ignore_errors: yes to everything under the tasks section, and then apply a when: conditional to each task to individually check if the prior command succeeded. \nThis solution seems like a hack, but it gets worse because even with ignore_errors: yes set, if a Fatal error is encountered for a task the entire play will still immediately fail, so I have to also run a cron'd bash script to manually check on things after reach play execution. \nAll I want is a guarantee that even if tasks fails, post_tasks will still run. I'm sure there is a way to do this without resorting to bash script wrappers.",
    "answer": "This feature became available in Ansible 2.0:\nThis is the documentation for the new stanza markers block, rescue, and always."
  },
  {
    "question": "I am trying to wget a file from a web server from within an Ansible playbook. \nHere is the Ansible snippet:\n---\n- hosts: all\n  sudo: true\n  tasks:\n  - name: Prepare Install folder\n    sudo: true\n    action: shell sudo mkdir -p /tmp/my_install/mysql/ && cd /tmp/my_install/mysql/\n  - name: Download MySql\n    sudo: true\n    action: shell sudo wget http://{{ repo_host }}/MySQL-5.6.15-1.el6.x86_64.rpm-bundle.tar \n\nInvoking it via:\nansible-playbook my_3rparties.yml -l vsrv644 --extra-vars \"repo_host=vsrv656\" -K -f 10 \n\nIt fails with the following:\nCannot write to `MySQL-5.6.15-1.el6.x86_64.rpm-bundle.tar' (Permission denied).\nFATAL: all hosts have already failed -- aborting\n\nPLAY RECAP ******************************************************************** \n           to retry, use: --limit @/usr2/ihazan/vufroria_3rparties.retry\n\nvsrv644                : ok=2    changed=1    unreachable=0    failed=1   \n\nWhen trying to do the command that fail via regular remote ssh to mimic what ansible would do, it doesn't work as follows:\n-bash-4.1$ ssh ihazan@vsrv644 'cd /tmp/my_install/mysql && sudo wget http://vsrv656/MySQL-5.6.15-1.el6.x86_64.rpm-bundle.tar'\nEnter passphrase for key '/usr2/ihazan/.ssh/id_rsa': \nsudo: sorry, you must have a tty to run sudo\n\nBut I can solve it using -t as follows:\n-bash-4.1$ ssh -t ihazan@vsrv644 'cd /tmp/my_install/mysql && sudo wget http://vsrv656/MySQL-5.6.15-1.el6.x86_64.rpm-bundle.tar'\n\nThen it works.\nIs there a way to set the -t (pseudo tty option) on ansible? \nP.S: I could solve it by editing the sudoers file as others propose but that is a manual step I am trying to avoid.",
    "answer": "Don't use shell-module when there is specialized modules available. In your case:\nCreate directories with file-module:\n- name: create project directory {{ common.project_dir }}\n  file: state=directory path={{ common.project_dir }}\n\nDownload files with get_url-module:\n- name: download sources\n  get_url: url={{ opencv.url }} dest={{ common.project_dir }}/{{ opencv.file }}\n\nNote the new module call syntax in the examples above.\nIf you have to use sudo with password remember to give --ask-sudo-pass when needed (see e.g. Remote Connection Information)."
  },
  {
    "question": "---\n# file: main.yml\n\n- hosts: fotk\n  remote_user: fakesudo\n  tasks:\n  - name: create a developer user\n    user: name={{ user }}\n          password={{ password }}\n          shell=/bin/bash\n          generate_ssh_key=yes\n          state=present\n  roles:\n  - { role: create_developer_environment, sudo_user: \"{{ user }}\" }\n  - { role: vim, sudo_user: \"{{ user }}\" }\n\nFor some reason the create user task is not running.  I have searched every key phrase I can think of on Google to find an answer without success.\nThe roles are running which is odd.\nIs it possible for a playbook to contain both tasks and roles?",
    "answer": "You can also do pre_tasks:  and post_tasks: if you need to do things before or after.  From the Docs https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html\n- hosts: localhost\n\n  pre_tasks:\n    - shell: echo 'hello in pre'\n\n  roles:\n    - { role: some_role }\n\n  tasks:\n    - shell: echo 'in tasks'\n\n  post_tasks:\n    - shell: echo 'goodbye in post'\n\n\nGives the output: PLAY [localhost]\n\nGATHERING FACTS\n***************************************************************  ok: [localhost]\nTASK: [shell echo 'hello in pre']\n*********************************************  changed: [localhost]\nTASK: [some_role | shell echo 'hello from the role']\n**************************  changed: [localhost]\nTASK: [shell echo 'in tasks']\n*************************************************  changed: [localhost]\nTASK: [shell echo 'goodbye in post']\n******************************************  changed: [localhost]\nPLAY RECAP\n********************************************************************  localhost                  : ok=5    changed=4    unreachable=0\nfailed=0\n\nThis is with ansible 1.9.1"
  },
  {
    "question": "I have the following tasks in a playbook I'm writing (results listed next to the debug statement in <>):\n  - debug: var=nrpe_installed.stat.exists <true>\n  - debug: var=force_install <true>\n  - debug: var=plugins_installed.stat.exists <true>\n\n  - name: Run the prep \n    include: prep.yml\n    when: (nrpe_installed.stat.exists == false or plugins_installed.stat.exists == true or force_install == true)\n    tags: ['prep']\n\n  - debug: var=nrpe_installed.stat.exists <true>\n  - debug: var=force_install <true>\n  - debug: var=force_nrpe_install <false>\n\n  - name: Install NRPE\n    include: install-nrpe.yml\n    when: (nrpe_installed.stat.exists == false or force_install == true or force_nrpe_install == true)\n    tags: ['install_nrpe']\n    vars:\n      nrpe_url: 'http://url.goes.here'\n      nrpe_md5: 3921ddc598312983f604541784b35a50\n      nrpe_version: 2.15\n      nrpe_artifact: nrpe-{{ nrpe_version }}.tar.gz\n      nagios_ip: {{ nagios_ip }}\n      config_dir: /home/ansible/config/\n\nAnd I'm running it with the following command:\nansible-playbook install.yml -i $invFile --extra-vars=\"hosts=webservers force_install=True\"\n\nThe first include runs, but the second skips with this output:\n skipping: [server1] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}\n\nI'm under the impression that the conditional check should pass for all of them as force_install == true evaluates to true which should make the whole when evaluate to true (since it's a series of 'OR's).\nHow do I get the when to run when the variables are set appropriately?\n\nEdit:\nChanging the second when for the Install NRPE include to the following works, but doesn't explain why the other one, Run the prep runs appropriately:\nWorking:\nwhen: (not nrpe_installed.stat.exists or force_install or force_nrpe_install)\n\nAlso working:\nwhen: (nrpe_installed.stat.exists == false or plugins_installed.stat.exists == true or force_install == true)\n\nNot working:\nwhen: (nrpe_installed.stat.exists == false or force_install == true or force_nrpe_install == true)\n\n\nThe truncated (duplicates removed) output of that particular section of the play is:\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"nrpe_installed.stat.exists\": true\n}\n\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"plugins_installed.stat.exists\": true\n}\n\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"force_install\": true\n}\n\nTASK [Run the prep] ************************************************************\nincluded: /tasks/nrpe-install/prep.yml for server2, server3, server4, server5, server6, server7\n\nTASK [Prep and configure for installation | Install yum packages] **************\nok: [server6] => (item=[u'gcc', u'glibc', u'glibc-common', u'gd', u'gd-devel', u'make', u'net-snmp', u'openssl-devel', u'unzip', u'tar', u'gzip', u'xinetd']) => {\"changed\": false, \"item\": [\"gcc\", \"glibc\", \"glibc-common\", \"gd\", \"gd-devel\", \"make\", \"net-snmp\", \"openssl-devel\", \"unzip\", \"tar\", \"gzip\", \"xinetd\"], \"msg\": \"\", \"rc\": 0, \"results\": [\"gcc-4.1.2-55.el5.x86_64 providing gcc is already installed\", \"glibc-2.5-123.el5_11.3.i686 providing glibc is already installed\", \"glibc-common-2.5-123.el5_11.3.x86_64 providing glibc-common is already installed\", \"gd-2.0.33-9.4.el5_4.2.x86_64 providing gd is already installed\", \"gd-devel-2.0.33-9.4.el5_4.2.i386 providing gd-devel is already installed\", \"make-3.81-3.el5.x86_64 providing make is already installed\", \"net-snmp-5.3.2.2-20.el5.x86_64 providing net-snmp is already installed\", \"openssl-devel-0.9.8e-40.el5_11.x86_64 providing openssl-devel is already installed\", \"unzip-5.52-3.el5.x86_64 providing unzip is already installed\", \"tar-1.15.1-32.el5_8.x86_64 providing tar is already installed\", \"gzip-1.3.5-13.el5.centos.x86_64 providing gzip is already installed\", \"xinetd-2.3.14-20.el5_10.x86_64 providing xinetd is already installed\"]}\n\nTASK [Prep and configure for installation | Make nagios group] *****************\nok: [server2] => {\"changed\": false, \"gid\": 20002, \"name\": \"nagios\", \"state\": \"present\", \"system\": false}\n\nTASK [Prep and configure for installation | Make nagios user] ******************\nok: [server6] => {\"append\": false, \"changed\": false, \"comment\": \"User for Nagios NRPE\", \"group\": 20002, \"home\": \"/home/nagios\", \"move_home\": false, \"name\": \"nagios\", \"shell\": \"/bin/bash\", \"state\": \"present\", \"uid\": 20002}\n\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"nrpe_installed.stat.exists\": true\n}\n\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"force_install\": true\n}\n\nTASK [debug] *******************************************************************\nok: [server2] => {\n    \"force_nrpe_install\": false\n}\n\nTASK [Install NRPE] ************************************************************\nskipping: [server2] => {\"changed\": false, \"skip_reason\": \"Conditional check failed\", \"skipped\": true}",
    "answer": "You need to convert the variable to a boolean:\nforce_install|bool == true\n\nI don't claim I understand the logic behind it. In python any non-empty string should be truthy. But when directly used in a condition it evaluates to false. \nThe bool filter then again interprets the strings 'yes', 'on', '1', 'true' (case-insensitive) and 1 as true (see source). Any other string is false.\nYou might want to also set a default value in case force_install is not defined, since it would result in an undefined variable error:\nforce_install|default(false)|bool == true"
  },
  {
    "question": "Here I am trying to test my bash script where it is prompting four times.\n#!/bin/bash\ndate >/opt/prompt.txt\nread -p \"enter one: \" one\necho $one\necho $one >>/opt/prompt.txt\nread -p \"enter two: \" two\necho $two\necho $two >>/opt/prompt.txt\nread -p \"enter three: \" three\necho $three\necho $three >>/opt/prompt.txt\nread -p \"enter password: \" password\necho $password\necho $password >>/opt/prompt.txt\n\nfor this script I wrote the code below, and it is working fine\n- hosts: \"{{ hosts }}\"\n  tasks:\n  - name: Test Script\n    expect:\n      command: sc.sh\n      responses:\n        enter one: 'one'\n        enter two: 'two'\n        enter three: 'three'\n        enter password: 'pass'\n      echo: yes\n\nBut if I am doing the same for mysql_secure_installation command it not working\n- hosts: \"{{ hosts }}\"\n  tasks:\n  - name: mysql_secure_installation Command Test\n    expect:\n      command: mysql_secure_installation\n      responses:\n        'Enter current password for root (enter for none):': \"\\n\"\n        'Set root password? [Y/n]:': 'y'\n        'New password:': '123456'\n        'Re-enter new password:': '123456'\n        'Remove anonymous users? [Y/n]:': 'y'\n        'Disallow root login remotely? [Y/n]:': 'y'\n        'Remove test database and access to it? [Y/n]:': 'y'\n        'Reload privilege tables now? [Y/n]:': 'y'\n\n      echo: yes\n\nand its trackback is here:\nPLAY [S1] **********************************************************************\n\nTASK [setup] *******************************************************************\nok: [S1]\n\nTASK [mysql_secure_installation Command Test] **********************************\nfatal: [S1]: FAILED! => {\"changed\": true, \"cmd\": \"mysql_secure_installation\", \"delta\": \"0:00:30.139266\", \"end\": \"2016-07-15 15:36:32.549415\", \"failed\": true, \"rc\": 1, \"start\": \"2016-07-15 15:36:02.410149\", \"stdout\": \"\\r\\n\\r\\n\\r\\n\\r\\nNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL\\r\\n      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!\\r\\n\\r\\n\\r\\nIn order to log into MySQL to secure it, we'll need the current\\r\\npassword for the root user.  If you've just installed MySQL, and\\r\\nyou haven't set the root password yet, the password will be blank,\\r\\nso you should just press enter here.\\r\\n\\r\\nEnter current password for root (enter for none): \", \"stdout_lines\": [\"\", \"\", \"\", \"\", \"NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL\", \"      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!\", \"\", \"\", \"In order to log into MySQL to secure it, we'll need the current\", \"password for the root user.  If you've just installed MySQL, and\", \"you haven't set the root password yet, the password will be blank,\", \"so you should just press enter here.\", \"\", \"Enter current password for root (enter for none): \"]}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @/home/jackson/AnsibleWorkSpace/AnsibleTest/example1.retry\n\nPLAY RECAP *********************************************************************\nS1                         : ok=1    changed=0    unreachable=0    failed=1  \n\nI have also tried blank '' instead of \"\\n\" for the first answer but it is not working either. I also visited Ansible expect doc but they show only very simple example and explanation. I am also trying regex match for multiple different responses but it is also not working.\nPlease do not recommend me to use mysql module of Ansible, because here my purpose is to learn this module for future use.",
    "answer": "The reason is that the questions are interpreted as regexps. Hence you must escape characters with a special meaning in regular expressions, such as -()[]\\?*. et cetara.\nHence:\n'Enter current password for root (enter for none):'\n\nshould instead be:\n'Enter current password for root \\(enter for none\\):'\n\nGood luck!"
  },
  {
    "question": "so I have a problem getting keycloak 3.2.1 to work behind kong (0.10.3), a reverse proxy based on nginx. \nScenario is:\nI call keycloak via my gateway-route via https://{gateway}/auth and it shows me the entrypoint with keycloak logo, link to admin console etc. - so far so good. \nBut when clicking on administration console -> calling https://{gateway}/auth/admin/master/console/ , keycloak tries to load its css/js via http (see screenie below), which my browser blocks because mixed content.\nI searched around and found this thread: keycloak apache server configuration with 'Mixed Content' problems which lead to this github repo: https://github.com/dukecon/keycloak_postgres_https\nFrom there on, I tried to integrate its' cli into my dockerfile with success (did not change the files' contents, just copied them into my repo and add/run them from dockerfile). This is my dockerfile right now:\nFROM jboss/keycloak-postgres:3.2.1.Final\n\nUSER root\n\nADD config.sh /tmp/\nADD batch.cli /tmp/\n\nRUN bash /tmp/config.sh\n\n#Give correct permissions when used in an OpenShift environment.\nRUN chown -R jboss:0 $JBOSS_HOME/standalone && \\\n    chmod -R g+rw $JBOSS_HOME/standalone\n\nUSER jboss\nEXPOSE 8080\n\nSadly, my problem still exists:\n\nSo I am out of ideas for now and hope you could help me out:\n\nHow do I tell keycloak to call its' css-files via https here? \ndo I have to change something in the cli script? \n\nHere's the content of the script:\nconfig.sh:\n#!/bin/bash -x\n\nset -e\n\nJBOSS_HOME=/opt/jboss/keycloak\nJBOSS_CLI=$JBOSS_HOME/bin/jboss-cli.sh\nJBOSS_MODE=${1:-\"standalone\"}\nJBOSS_CONFIG=${2:-\"$JBOSS_MODE.xml\"}\n\necho \"==> Executing...\"\ncd /tmp\n\n$JBOSS_CLI --file=`dirname \"$0\"`/batch.cli\n\n# cf. http://stackoverflow.com/questions/34494022/permissions-error-when-using-cli-in-jboss-wildfly-and-docker\n/bin/rm -rf ${JBOSS_HOME}/${JBOSS_MODE}/configuration/${JBOSS_MODE}_xml_history/current\n\nand batch.cli:\nembed-server --std-out=echo\n\n# http://keycloak.github.io/docs/userguide/keycloak-server/html/server-installation.html\n# 3.2.7.2. Enable SSL on a Reverse Proxy\n# First add proxy-address-forwarding and redirect-socket to the http-listener element.\n# Then add a new socket-binding element to the socket-binding-group element.\n\nbatch\n\n/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=proxy-address-forwarding,value=true)\n\n/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=redirect-socket,value=proxy-https)\n\n/socket-binding-group=standard-sockets/socket-binding=proxy-https:add(port=443)\n\nrun-batch\n\nstop-embedded-server\n\nIt may be of interest too, that kong is deployed on openshift with a route using a redirect from http to https ( \"insecureEdgeTerminationPolicy\": \"Redirect\" ).",
    "answer": "This sounds somehow like a duplicate of Keycloak Docker behind loadbalancer with https fails\nSet the request headers X-Forwarded-For and X-Forwarded-Proto in nginx. Then you have to configure Keycloak (Wildfly, Undertow) to work together with the SSL terminating reverse proxy (aka load balancer). See http://www.keycloak.org/docs/latest/server_installation/index.html#_setting-up-a-load-balancer-or-proxy for a detailed description.\nThe point is that nginx is terminating SSL and is forwarding the requests to Keycloak as pure http. Therefore Keycloak/Wildfly must be told that the incoming http requests from nginx must be handled like they were https."
  },
  {
    "question": "I'm trying to copy an entire directory from my docker image to my local machine.\nThe image is a keycloak image, and I'd like to copy the themes folder so I can work on a custom theme.\nI am running the following command -\ndocker cp 143v73628670f:keycloak/themes ~/Development/Code/Git/keycloak-recognition-login-branding\n\nHowever I am getting the following response -\nError response from daemon: Could not find the file keycloak/themes in container 143v73628670f\n\nWhen I connect to my container using -\n docker exec -t -i 143v73628670f /bin/bash\n\nI can navigate to the themes by using -\ncd keycloak/themes/\n\nI can see it is located there and the files are as expected in the terminal.\nI'm running the instance locally on a Mac.\nHow do I copy that entire themes folder to my local machine? What am I doing wrong please?",
    "answer": "EDIT\nAs a result of running 'pwd' your should run the Docker cp command as follows:\ndocker cp 143v73628670f:/opt/jboss/keycloak/themes ~/Development/Code/Git/keycloak-recognition-login-branding\n\nYou are forgetting the trailing ' / '. Therefore your command should look like this:\ndocker cp 143v73628670f:/keycloak/themes/ ~/Development/Code/Git/keycloak-recognition-login-branding\n\nAlso, you could make use of Docker volumes, which allows you to pass a local directory into the container when you run the container"
  },
  {
    "question": "I'm trying to export a realm file into keycloak docker container, I'm not able to do that because the server is runing when I execute this command:\nbin/standalone.sh -Dkeycloak.migration.action=export\n-Dkeycloak.migration.provider=dir -Dkeycloak.migration.dir=<DIR TO EXPORT TO>\n\nI tried to modify the docker-entrypoint.sh and I delete the command which executes the server to launch:\n#!/bin/bash\n\nif [ $KEYCLOAK_USER ] && [ $KEYCLOAK_PASSWORD ]; then\n    keycloak/bin/add-user-keycloak.sh --user $KEYCLOAK_USER --password $KEYCLOAK_PASSWORD\nfi\n\nif [ \"$DB_VENDOR\" == \"POSTGRES\" ]; then\n  databaseToInstall=\"postgres\"\nelif [ \"$DB_VENDOR\" == \"MYSQL\" ]; then\n  databaseToInstall=\"mysql\"\nelif [ \"$DB_VENDOR\" == \"H2\" ]; then\n  databaseToInstall=\"\"\nelse\n    if (printenv | grep '^POSTGRES_' &>/dev/null); then\n      databaseToInstall=\"postgres\"\n    elif (printenv | grep '^MYSQL_' &>/dev/null); then\n      databaseToInstall=\"mysql\"\n    fi\nfi\n\nif [ \"$databaseToInstall\" != \"\" ]; then\n    echo \"[KEYCLOAK DOCKER IMAGE] Using the external $databaseToInstall database\"\n    /bin/sh /opt/jboss/keycloak/bin/change-database.sh $databaseToInstall\nelse\n    echo \"[KEYCLOAK DOCKER IMAGE] Using the embedded H2 database\"\nfi\n\nexit $?\n\nHowever I got a caschLoopBack when I run the pod of keycloak. Is there any solution to make the export inside the docker container and stop the server from running?",
    "answer": "You can start a temporary container. I'm using swarm and attachable network, but replacing the --network flag with some --link to the DB container should do it for a vanilla docker container :\ndocker run --rm --network=naq\\\n    --name keycloak_exporter\\\n    -v /tmp:/tmp/keycloak-export\\\n    -e POSTGRES_DATABASE=keycloak\\\n    -e POSTGRES_PASSWORD=password\\\n    -e POSTGRES_USER=keycloak\\\n    -e DB_VENDOR=POSTGRES\\\n    -e POSTGRES_PORT_5432_TCP_ADDR=keycloakdb\\\n    jboss/keycloak:3.4.3.Final\\\n    -Dkeycloak.migration.action=export\\\n    -Dkeycloak.migration.provider=dir\\\n    -Dkeycloak.migration.dir=/tmp/keycloak-export\\\n    -Dkeycloak.migration.usersExportStrategy=SAME_FILE\\\n    -Dkeycloak.migration.realmName=Naq\\\n\nYou'll then find export files in the /tmp dir on your host."
  },
  {
    "question": "I'm new to Terraform world. I'm trying to run a shell script using Terraform.\nBelow is the main.tf file\n#Executing shell script via Null Resource\n\nresource \"null_resource\" \"install_istio\" {\n provisioner \"local-exec\" {\n    command = <<EOT\n      \"chmod +x install-istio.sh\"\n      \"./install-istio.sh\"\n    EOT\n    interpreter = [\"/bin/bash\", \"-c\"]\n    working_dir = \"${path.module}\"\n  }\n}\n\nBelow is the install-istio.sh file which it needs to run\n#!/bin/sh\n\n# Download and install the Istio istioctl client binary\n\n# Specify the Istio version that will be leveraged throughout these instructions\nISTIO_VERSION=1.7.3\n\ncurl -sL \"https://github.com/istio/istio/releases/download/$ISTIO_VERSION/istioctl-$ISTIO_VERSION-linux-amd64.tar.gz\" | tar xz\n\nsudo mv ./istioctl /usr/local/bin/istioctl\nsudo chmod +x /usr/local/bin/istioctl\n\n# Install the Istio Operator on EKS\nistioctl operator init\n\n# The Istio Operator is installed into the istio-operator namespace. Query the namespace.\nkubectl get all -n istio-operator\n\n# Install Istio components\nistioctl profile dump default\n\n# Create the istio-system namespace and deploy the Istio Operator Spec to that namespace.\nkubectl create ns istio-system\nkubectl apply -f istio-eks.yaml\n\n# Validate the Istio installation\nkubectl get all -n istio-system\n\n\nI'm getting below warning:\nWarning: Interpolation-only expressions are deprecated\n  on .terraform/modules/istio_module/Istio-Operator/main.tf line 10, in resource \"null_resource\" \"install_istio\":\n  10:     working_dir = \"${path.module}\"\nTerraform 0.11 and earlier required all non-constant expressions to be\nprovided via interpolation syntax, but this pattern is now deprecated. To\nsilence this warning, remove the \"${ sequence from the start and the }\"\nsequence from the end of this expression, leaving just the inner expression.\nTemplate interpolation syntax is still used to construct strings from\nexpressions when the template includes multiple interpolation sequences or a\nmixture of literal strings and interpolations. This deprecation applies only\nto templates that consist entirely of a single interpolation sequence.\n\nThe above script in main.tf does run command in the background.\nCan someone help me with the missing part? How can I run multiple commands using local exec Also, How can I get rid of the warning message?\nAppreciate all your help, Thanks!",
    "answer": "I think there are two separate things going on here which are actually not related.\nThe main problem here is in how you've written your local-exec script:\n    command = <<EOT\n      \"chmod +x install-istio.sh\"\n      \"./install-istio.sh\"\n    EOT\n\nThis will become the following shell script to run:\n\"chmod +x install-istio.sh\"\n\"./install-istio.sh\"\n\nBy putting the first command line in quotes, you're telling the shell to try to run a program that is called chmod +x install-istio.sh without any arguments. That is, the shell will try to find an executable in your PATH called chmod +x install-istio.sh, rather than trying to run a command called just chmod with some arguments as I think you intended.\nRemove the quotes around the command lines to make this work. Quotes aren't needed here because neither of these commands contain any special characters that would require quoting:\n    command = <<-EOT\n      chmod +x install-istio.sh\n      ./install-istio.sh\n    EOT\n\n\nThe warning message about interpolation-only expressions is unrelated to the problem of running these commands. This is telling you that you've used a legacy syntax that is still supported for backward compatibility but no longer recommended.\nIf you are using the latest version of Terraform at the time of writing (one of the v0.15 releases, or later) then you may be able to resolve this and other warnings like it by switching into this module directory and running terraform fmt, which is a command that updates your configuration to match the expected style conventions.\nAlternatively, you could manually change what that command would update automatically, which is to remove the redundant interpolation markers around path.module:\n    working_dir = path.module"
  },
  {
    "question": "I want a stage in an Azure DevOps pipeline to be executed depending on the content of a variable set in a previous stage.\nHere is my pipeline:\nstages:\n  - stage: plan_dev\n    jobs:\n    - job: terraform_plan_dev\n      steps:\n      - bash: echo '##vso[task.setvariable variable=terraform_plan_exitcode;isOutput=true]2'\n        name: terraform_plan\n\n  - stage: apply_dev\n    dependsOn: plan_dev\n    condition: eq(stageDependencies.plan_dev.terraform_plan_dev.outputs['terraform_plan.terraform_plan_exitcode'], '2')\n    jobs:\n    - deployment: \"apply_dev\"\n      ...\n\nThe idea is to skip the apply_dev stage, if the plan_dev stage shows no changes. Background is that we have manual approval for the deployment in the plan_dev stage that we want to skip if there are no changes to be approved.\nUnfortunately this doesn't seem to work. No matter whether the variable terraform_plan_exitcode is set with the expected value (2) or not, the apply_dev stage is skipped.\nFor the syntax, I followed the documentation here that says:\nstageDependencies.StageName.JobName.outputs['StepName.VariableName']",
    "answer": "I have seen this same issue. You need to use the dependencies variable instead of the stageDependencies:\nstages:\n- stage: plan_dev\njobs:\n- job: terraform_plan_dev\n  steps:\n  - bash: echo '##vso[task.setvariable variable=terraform_plan_exitcode;isOutput=true]2'\n    name: terraform_plan\n\n- stage: apply_dev\ndependsOn: plan_dev\ncondition: eq(dependencies.plan_dev.outputs['terraform_plan_dev.terraform_plan.terraform_plan_exitcode'], '2')\njobs:\n- deployment: \"apply_dev\"\n\nThe following is a more complete example of something I have working with Terraform Plan + conditional Apply:\n\nstages: \n  - stage: Build_zip_plan\n    displayName: Build portal, zip files and terraform plan\n    jobs:\n    - job: Build_portal_zip_files_terraform_plan\n      pool:\n        vmImage: 'ubuntu-latest'\n      steps:\n        - task: Cache@2\n          displayName: 'Register TF cache'\n          inputs:\n            key: terraform | $(Agent.OS) | $(Build.BuildNumber) | $(Build.BuildId) | $(Build.SourceVersion) | $(prefix)\n            path: ${{ parameters.tfExecutionDir }}\n\n        - task: TerraformInstaller@0\n          displayName: 'Install Terraform'\n          inputs:\n            terraformVersion: ${{ parameters.tfVersion }}\n\n        - task: TerraformTaskV1@0\n          displayName: 'Terraform Init'\n          inputs:\n            provider: 'azurerm'\n            command: 'init'\n            workingDirectory: ${{ parameters.tfExecutionDir }}\n            backendServiceArm: ${{ parameters.tfStateServiceConnection }}\n            backendAzureRmResourceGroupName: ${{ parameters.tfStateResourceGroup }}\n            backendAzureRmStorageAccountName: ${{ parameters.tfStateStorageAccount }}\n            backendAzureRmContainerName: ${{ parameters.tfStateStorageContainer }}\n            backendAzureRmKey: '$(prefix)-$(environment).tfstate'\n\n        - task: TerraformTaskV1@0\n          displayName: 'Terraform Plan'\n          inputs:\n            provider: 'azurerm'\n            command: 'plan'\n            commandOptions: '-input=false -out=deployment.tfplan -var=\"environment=$(environment)\" -var=\"prefix=$(prefix)\" -var=\"tenant=$(tenant)\" -var=\"servicenow={username=\\\"$(servicenowusername)\\\",instance=\\\"$(servicenowinstance)\\\",password=\\\"$(servicenowpassword)\\\",assignmentgroup=\\\"$(servicenowassignmentgroup)\\\",company=\\\"$(servicenowcompany)\\\"}\" -var=\"clientid=$(clientid)\" -var=\"username=$(username)\" -var=\"password=$(password)\" -var=\"clientsecret=$(clientsecret)\" -var=\"mcasapitoken=$(mcasapitoken)\" -var=\"portaltenantid=$(portaltenantid)\" -var=\"portalclientid=$(portalclientid)\" -var=\"customerdisplayname=$(customerdisplayname)\" -var=\"reportonlymode=$(reportonlymode)\"'\n            workingDirectory: ${{ parameters.tfExecutionDir }}\n            environmentServiceNameAzureRM: ${{ parameters.tfServiceConnection }}\n\n        - task: PowerShell@2\n          displayName: 'Check Terraform plan'\n          name: \"Check_Terraform_Plan\"\n          inputs:\n            filePath: '$(Build.SourcesDirectory)/Pipelines/Invoke-CheckTerraformPlan.ps1'\n            arguments: '-TfPlan ''${{ parameters.tfExecutionDir }}/deployment.tfplan'''\n            pwsh: true\n  \n\n  - stage:\n    dependsOn: Build_zip_plan\n    displayName: Terraform apply\n    condition: eq(dependencies.Build_zip_plan.outputs['Build_portal_zip_files_terraform_plan.Check_Terraform_Plan.TFChangesPending'], 'yes')\n    jobs:\n    - deployment: DeployHub\n      displayName: Apply\n      pool:\n        vmImage: 'ubuntu-latest'\n      environment: '$(prefix)'\n      strategy:\n        runOnce:\n          deploy:\n            steps:\n            - checkout: self\n\n            - task: Cache@2\n              displayName: 'Get Cache for TF Artifact'\n              inputs:\n                key: terraform | $(Agent.OS) | $(Build.BuildNumber) | $(Build.BuildId) | $(Build.SourceVersion) | $(prefix)\n                path: ${{ parameters.tfExecutionDir }}\n                \n            - task: TerraformInstaller@0\n              displayName: 'Install Terraform'\n              inputs:\n                terraformVersion: ${{ parameters.tfVersion }}\n\n            - task: TerraformTaskV1@0\n              displayName: 'Terraform Apply'\n              inputs:\n                provider: 'azurerm'\n                command: 'apply'\n                commandOptions: 'deployment.tfplan'\n                workingDirectory: ${{ parameters.tfExecutionDir }}\n                environmentServiceNameAzureRM: ${{ parameters.tfServiceConnection }}"
  },
  {
    "question": "I'm using azurerm_virtual_machine_extension to bootstrap some virtual machines in azure.\nAll examples i've found show using something similar to: \nsettings = <<SETTINGS\n    {   \n    \"fileUris\": [ \"https://my.bootstrapscript.com/script.sh}\" ],\n    \"commandToExecute\": \"bash script.sh\"\n    }\nSETTINGS\n\nWhile this works, my issue is i'm having to publicly host script for use with fileUris. Is there an option within settings that will allow me to send local file contents from my terraform folder? \nSomething like:\nsettings = <<SETTINGS\n    {   \n    \"file\": [ ${file(\"./script.txt\")} ],\n    \"commandToExecute\": \"bash script.sh\"\n    }\nSETTINGS\n\nThanks.",
    "answer": "Yes We Can!\nIntroduction\nIn protected_settings, use \"script\".\nScripts\nterraform script\nprovider \"azurerm\" {\n}\n\nresource \"azurerm_virtual_machine_extension\" \"vmext\" {\n    resource_group_name     = \"${var.resource_group_name}\"\n    location                = \"${var.location}\"\n    name                    = \"${var.hostname}-vmext\"\n\n    virtual_machine_name = \"${var.hostname}\"\n    publisher            = \"Microsoft.Azure.Extensions\"\n    type                 = \"CustomScript\"\n    type_handler_version = \"2.0\"\n\n    protected_settings = <<PROT\n    {\n        \"script\": \"${base64encode(file(var.scfile))}\"\n    }\n    PROT\n}\n\nvariables\nvariable resource_group_name {\n    type = string\n    default = \"ORA\"\n}\n\nvariable location {\n    type = string\n    default = \"eastus\"\n}\n\nvariable hostname {\n    type = string\n    default = \"ora\"\n}\n\nvariable scfile{\n    type = string\n    default = \"yum.bash\"\n}\n\nbash script\n#!/bin/bash\n\nmkdir -p ~/download\ncd ~/download\nwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nrpm -ivh epel-release-latest-7.noarch.rpm\nyum -y install cowsay\ncowsay ExaGridDba\n\nOutput\napply\n[terraform@terra stackoverflow]$ terraform apply\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # azurerm_virtual_machine_extension.vmex0 will be created\n  + resource \"azurerm_virtual_machine_extension\" \"vmex0\" {\n      + id                   = (known after apply)\n      + location             = \"eastus\"\n      + name                 = \"ora-vmext\"\n      + protected_settings   = (sensitive value)\n      + publisher            = \"Microsoft.Azure.Extensions\"\n      + resource_group_name  = \"ORA\"\n      + tags                 = (known after apply)\n      + type                 = \"CustomScript\"\n      + type_handler_version = \"2.0\"\n      + virtual_machine_name = \"ora\"\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\nazurerm_virtual_machine_extension.vmex0: Creating...\nazurerm_virtual_machine_extension.vmex0: Still creating... [10s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [20s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [30s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [40s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [50s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m0s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m10s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m20s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m30s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m40s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [1m50s elapsed]\nazurerm_virtual_machine_extension.vmex0: Still creating... [2m0s elapsed]\nazurerm_virtual_machine_extension.vmex0: Creation complete after 2m1s [id=/subscriptions/7fe8a9c3-0812-42e2-9733-3f567308a0d0/resourceGroups/ORA/providers/Microsoft.Compute/virtualMachines/ora/extensions/ora-vmext]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nstdout on the target\n[root@ora ~]# cat /var/lib/waagent/custom-script/download/0/stdout\nPreparing...                          ########################################\nUpdating / installing...\nepel-release-7-12                     ########################################\nLoaded plugins: langpacks, ulninfo\nResolving Dependencies\n--> Running transaction check\n---> Package cowsay.noarch 0:3.04-4.el7 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package          Arch             Version                 Repository      Size\n================================================================================\nInstalling:\n cowsay           noarch           3.04-4.el7              epel            42 k\n\nTransaction Summary\n================================================================================\nInstall  1 Package\n\nTotal download size: 42 k\nInstalled size: 77 k\nDownloading packages:\nPublic key for cowsay-3.04-4.el7.noarch.rpm is not installed\nRetrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : cowsay-3.04-4.el7.noarch                                     1/1\n  Verifying  : cowsay-3.04-4.el7.noarch                                     1/1\n\nInstalled:\n  cowsay.noarch 0:3.04-4.el7\n\nComplete!\n\n< ExaGridDba >\n ------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n\nRemarks\n\nThe script size limit is 262144 bytes base64 encoded, or 196608 bytes.\n\"#!\" determines the interpreter. \"#!/bin/python\" would start a python script.\nThese azurerm_virtual_machine_extension parameters are not required:\n\n\nsettings\nfileUris\ncommandToExecute\nstorageAccountName\nstorageAccountKey \n\nprotected_settings parameter \"script\" might not be mentioned in the Terraform documentation. Please refer to  Use the Azure Custom Script Extension Version 2 with Linux virtual machines\nazurerm_virtual_machine_extension may be used during VM creation, or as a standalone administrative tool.\n\nConclusion\nIn Azure VM, it is possible to run a script without referring to a blob storage account."
  },
  {
    "question": "I'm using Ansible to add a user to a variety of servers.  Some of the servers have different UNIX groups defined. I'd like to find a way for Ansible to check for the existence of a group that I specify, and if that group exists, add it to a User's secondary groups list (but ignore the statement it if the group does not exist). \nAny thoughts on how I might do this with Ansible? \nHere is my starting point.\nCommand\nansible-playbook -i 'localhost,' -c local ansible_user.yml\n\nansible_user.yml\n---\n\n- hosts: all\n  user: root\n  become: yes\n  vars:\n    password: \"!\"\n    user: testa\n  tasks:\n    - name: add user\n      user: name=\"{{user}}\"\n            state=present\n            password=\"{{password}}\"\n            shell=/bin/bash\n            append=yes\n            comment=\"test User\"\n\nUpdated: based on the solution suggested by @udondan, I was able to get this working with the following additional tasks.\n    - name: Check if user exists\n      shell: /usr/bin/getent group | awk -F\":\" '{print $1}'\n      register: etc_groups\n\n    - name: Add secondary Groups to user\n      user: name=\"{{user}}\" groups=\"{{item}}\" append=yes\n      when: '\"{{item}}\" in etc_groups.stdout_lines'\n      with_items: \n          - sudo\n          - wheel",
    "answer": "The getent module can be used to read /etc/group\n- name: Determine available groups\n  getent:\n    database: group\n\n- name: Add additional groups to user\n  user: name=\"{{user}}\" groups=\"{{item}}\" append=yes\n  when: item in ansible_facts.getent_group\n  with_items: \n      - sudo\n      - wheel"
  },
  {
    "question": "I know there are thousands of questsions like this one here on SO but i've seen them all and i'm still not able to deal with my problem.\nI'm doing everything using ansible so it's quite automated but, anyway, here are my files:\npg_hba.conf\nlocal   all             all                                     trust\nhost    all             all             127.0.0.1/32            md5\nhost    all             all             ::1/128                 md5\nhost    all             all             10.11.12.0/24           md5\n\ndatabase.yml\nproduction:\n  database: my_db\n  adapter: postgresql\n  host: localhost\n  username: deploy\n  encoding: unicode\n  min_messages: WARNING\n  template: template0\n\nAnd i have a deploy user (and postgres user without a password set) in my system created. And now, while i'm totally able to sign in to postgres from bash using psql -d my_db (on server), i'm not able to connect to the db with my rails app. Running rake db:migrateMigration gives me \nPG::ConnectionBad: fe_sendauth: no password supplied\n\nI'm quite terrible at beeing a devop and i'm fighting with that issue from the day before yesterday's morning and it's still here so if there is anyone who can help me with that, i would be be more than grateful.",
    "answer": "psql is using a local socket connection, rails is using localhost over TCP/IP. Local is trusted, localhost requires a password (using md5). You could setup a pgpass file for your rails user: http://www.postgresql.org/docs/current/static/libpq-pgpass.html"
  },
  {
    "question": "I have the following Procfile:\nweb: bundle exec unicorn -p $PORT -c ./config/unicorn.rb\nredis: bundle exec redis-server /usr/local/etc/redis.conf\nworker: bundle exec sidekiq\n\nRunning $ foreman start starts up Unicorn, Redis and Sidekiq, but how should i stop them again?\nKilling Foreman leaves all three up. I can see this using ps:\n$ ps aux | grep redis | grep -v grep\n    me           61560   0.0  0.0  2506784   1740 s000  S+    9:36am   0:01.28 redis-server /usr/local/etc/redis.conf\n$ ps aux | grep sidekiq  | grep -v grep\n    me           61561   0.0  1.0  2683796 173284 s000  S+    9:36am   0:14.18 sidekiq 2.17.0 pathways [0 of 25 busy]\n$ ps aux | grep unicorn | grep -v grep\n    me           61616   0.0  0.2  2615284  28312 s000  S+    9:37am   0:00.06 unicorn worker[2] -p 5000 -c ./config/unicorn.rb\n    me           61615   0.0  0.2  2615284  27920 s000  S+    9:37am   0:00.06 unicorn worker[1] -p 5000 -c ./config/unicorn.rb\n    me           61614   0.0  0.2  2615284  27772 s000  S+    9:37am   0:00.06 unicorn worker[0] -p 5000 -c ./config/unicorn.rb\n    me           61559   0.0  1.0  2615284 160988 s000  S+    9:36am   0:09.87 unicorn master -p 5000 -c ./config/unicorn.rb\n\nSo obviously I can manually kill each process, but how can I kill all at once? It doesn't seem like Foreman supports this.",
    "answer": "To kill them all with a one-liner:\n$ kill $(ps aux | grep -E 'redis|sidekiq|unicorn' | grep -v grep | awk '{print $2}')"
  },
  {
    "question": "I am using Ansible to deploy my project and I trying to check if an specified package is installed, but I have a problem with it task, here is the task:\n- name: Check if python-apt is installed\n  command: dpkg -l | grep python-apt\n  register: python_apt_installed\n  ignore_errors: True\n\nAnd here is the problem:\n$ ansible-playbook -i hosts idempotent.yml\n\nPLAY [lxc-host] *************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [10.0.3.240]\n\nTASK: [idempotent | Check if python-apt is installed] ************************* \nfailed: [10.0.3.240] => {\"changed\": true, \"cmd\": [\"dpkg\", \"-l\", \"|\", \"grep\", \"python-apt\"], \"delta\": \"0:00:00.015524\", \"end\": \"2014-07-10 14:41:35.207971\", \"rc\": 2, \"start\": \"2014-07-10 14:41:35.192447\"}\nstderr: dpkg-query: error: package name in specifier '|' is illegal: must start with an alphanumeric character\n...ignoring\n\nPLAY RECAP ******************************************************************** \n10.0.3.240                 : ok=2    changed=1    unreachable=0    failed=0 \n\nWhy is illegal this character '|' .",
    "answer": "From the doc:\n\ncommand - Executes a command on a remote node\nThe command module takes the command name followed by a list of\nspace-delimited arguments. The given command will be executed on all\nselected nodes. It will not be processed through the shell, so\nvariables like $HOME and operations like \"<\", \">\", \"|\", and \"&\" will\nnot work (use the shell module if you need these features).\nshell - Executes a commands in nodes\nThe shell module takes the command name followed by a list of space-delimited arguments.\nIt is almost exactly like the command module but runs the command\nthrough a shell (/bin/sh) on the remote node.\n\nTherefore you have to use shell: dpkg -l | grep python-apt."
  },
  {
    "question": "In ansible, I need to check whether a particular line present in a file or not. Basically, I need to convert the following command to an ansible task. My goal is to only check.\ngrep -Fxq \"127.0.0.1\" /tmp/my.conf",
    "answer": "Use check_mode, register and failed_when in concert. This fails the task if the lineinfile module would make any changes to the file being checked. Check_mode ensures nothing will change even if it otherwise would.\n- name: \"Ensure /tmp/my.conf contains '127.0.0.1'\"\n  lineinfile:\n    name: /tmp/my.conf\n    line: \"127.0.0.1\"\n    state: present\n  check_mode: yes\n  register: conf\n  failed_when: (conf is changed) or (conf is failed)"
  },
  {
    "question": "I have an Ansible playbook for deploying a Java app as an init.d daemon.\nBeing a beginner in both Ansible and Linux I'm having trouble to conditionally execute tasks on a host based on the host's status.\nNamely I have some hosts having the service already present and running where I want to stop it before doing anything else. And then there might be new hosts, which don't have the service yet. So I can't simply use service: name={{service_name}} state=stopped, because this will fail on new hosts.\nHow I can I achieve this? Here's what I have so far:\n  - name: Check if Service Exists\n    shell: \"if chkconfig --list | grep -q my_service;   then echo true;   else echo false; fi;\"\n    register: service_exists\n\n# This should only execute on hosts where the service is present\n  - name: Stop Service\n    service: name={{service_name}} state=stopped\n    when: service_exists\n    register: service_stopped\n\n# This too\n  - name: Remove Old App Folder\n    command: rm -rf {{app_target_folder}}\n    when: service_exists\n\n# This should be executed on all hosts, but only after the service has stopped, if it was present\n  - name: Unpack App Archive\n    unarchive: src=../target/{{app_tar_name}} dest=/opt",
    "answer": "See the service_facts module, new in Ansible 2.5.\n- name: Populate service facts\n  service_facts:\n- debug:\n    msg: Docker installed!\n  when: \"'docker' in services\""
  },
  {
    "question": "I'm trying to create puppet module which automates installation of zend server CE, this is not important here, but steps are as following\n\nupdate /etc/apt/source.list\ndownload repos key via wget\ndo apt-get update\ndo apt-get install zend-server-ce-5.2\n\nI have init.pp file\nclass zendserverce {\n\n# https://github.com/puppetlabs/puppetlabs-stdlib\nfile_line { 'debian_package':\n    path => '/etc/apt/sources.list',\n    line => 'deb http://repos.zend.com/zend-server/deb server non-free'\n}\n\nexec { \"wget http://repos.zend.com/zend.key -O- |apt-key add -\":\n    path => [\"/usr/bin\", \"/usr/sbin\"]\n}\n\nexec { \"apt-get update\":\n    command => \"/usr/bin/apt-get update\",\n    onlyif  => \"/bin/sh -c '[ ! -f /var/cache/apt/pkgcache.bin ] || /usr/bin/find /etc/apt/* -cnewer /var/cache/apt/pkgcache.bin | /bin/grep . > /dev/null'\",\n}\n\npackage { \"zend-server-ce-php-5.2\":\n    ensure => \"latest\"\n}\n\n}\n\nSeems that puppet runs commands in different order then I need. Is there any way how to for tell him to run in my desired order?\nThe output of such snippet is\n  [0;36mnotice: /Stage[main]/Mc/Package[mc]/ensure: ensure changed 'purged' to 'latest'[0m\n  [1;35merr: /Stage[main]/Zendserverce/Package[zend-server-ce-php-5.2]/ensure: change from purged to latest failed: Could not update: Execution of '/usr/bin/apt-get -q -y -o DPkg::Options::=--force-confold install zend-server-ce-php-5.2' returned 100: Reading package lists...\n  Building dependency tree...\n  Reading state information...\n  E: Couldn't find package zend-server-ce-php-5.2 at /tmp/vagrant-puppet/modules 0/zendserverce/manifests/init.pp:28[0m\n  [0;36mnotice: /Stage[main]/Zendserverce/Exec[wget http://repos.zend.com/zend.key -O- |apt-key add -]/returns: executed successfully[0m\n  [0;36mnotice: /Stage[main]/Zendserverce/File_line[debian_package]/ensure: created[0m\n  [0;36mnotice: Finished catalog run in 6.75 seconds[0m\n\nSo it says: Couldn't find package zend-server-ce-php-5.2\nCan anyone guide me what is wrong?",
    "answer": "Since Puppet 2.6.0 a new feature \"relationship syntax\" was introduced.\nAn example in Puppet 2.6.0 and above would look like this:\nexec { \"apt-update\":\n    command => \"/usr/bin/apt-get update\"\n}\n\nExec[\"apt-update\"] -> Package <| |>\n\nEvery time a package command is executed, the dependency (in our case 'apt-update') will be triggered fist.\nYou can even define longer chains."
  },
  {
    "question": "I'm working on automating a task which needs to append the latest version of software to a file. I don't want it to do this multiple times for the same version.\nIt looks at the following example file:\nvar software releases = new Array(\n    \"4.3.0\",\n    \"4.4.0\",\n    \"4.5.0\",\n    \"4.7.0\",\n    \"4.8.0\",\n    \"4.11.0\",\n    \"4.12.1\",\n    \"4.14.0\",\n    \"4.15.0\",\n    \"4.16.0\",\n);\n\nthe defaults main.yml would pass in something like\nVERSION: 4.16.2\n\ncode\n- name: register version check\n  shell: cat /root/versions.js | grep -q {{VERSION}}\n  register: current_version\n\n- debug: msg=\"The registered variable output is {{ current_version.rc }}\"\n\n- name: append to versions.js\n  lineinfile:\n    dest: /root/versions.js\n    regexp: '^\\);'\n    insertbefore: '^#\\);'\n    line: \"    \\\"{{VERSION}}\\\",\\n);\"\n    owner: root\n    state: present\n    when: current_version.rc == 1\n\nproblem: the debug message is evaluating current_version.rc and showing me boolean values based on the grep commands output, but I can't re-use this in the when conditional to determine if the task should be run.\nEdit: the output:\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [test | register version check] *****************************************\nfailed: [localhost] => {\"changed\": true, \"cmd\": \"cat /root/versions.js | grep -q 3.19.2\", \"delta\": \"0:00:00.003570\", \"end\": \"2015-12-17 00:24:49.729078\", \"rc\": 1, \"start\": \"2015-12-17 00:24:49.725508\", \"warnings\": []}\n\nFATAL: all hosts have already failed -- aborting\n\nPLAY RECAP ********************************************************************\n           to retry, use: --limit @/root/site.retry\n\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=1",
    "answer": "As nikobelia pointed out in the comments, grep returns an exit code of 1 when it doesn't match any lines. Ansible then interprets this (actually any status code other than 0 from a shell/command task) as an error and so promptly fails.\nYou can tell Ansible to ignore the response code from the shell/command task by using ignore_errors. Although with grep this will ignore actual errors (given by a return code of 2) so instead you might want to use failed_when like this:\n- name: register version check\n  shell: cat /root/versions.js | grep -q {{VERSION}}\n  register: current_version\n  failed_when: current_version.rc == 2"
  },
  {
    "question": "I have two ansible tasks as follows \n  tasks:\n - shell: ifconfig -a | sed 's/[ \\t].*//;/^\\(lo\\|\\)$/d'\n   register: var1\n - debug: var=var1\n\n - shell: ethtool -i {{ item }} | grep bus-info | cut -b 16-22\n   with_items: var1.stdout_lines\n   register: var2\n - debug: var=var2\n\nwhich is used to get a list of interfaces in a machine (linux) and get the bus address for each. I have one more task as follows in tha same playbook \n - name: Binding the interfaces\n   shell: echo {{ item.item }}\n   with_flattened: var2.results\n   register: var3\n\nwhich I expect to iterate over value from var2 and then print the bus numbers. \nvar2.results is as follows \n\"var2\": {\n    \"changed\": true,\n    \"msg\": \"All items completed\",\n    \"results\": [\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i br0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005778\",\n            \"end\": \"2015-04-14 20:29:47.122203\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i br0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"br0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:47.116425\",\n            \"stderr\": \"\",\n            \"stdout\": \"\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp13s0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005862\",\n            \"end\": \"2015-04-14 20:29:47.359749\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp13s0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp13s0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:47.353887\",\n            \"stderr\": \"\",\n            \"stdout\": \"0d:00.0\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp14s0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005805\",\n            \"end\": \"2015-04-14 20:29:47.576674\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp14s0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp14s0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:47.570869\",\n            \"stderr\": \"\",\n            \"stdout\": \"0e:00.0\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp15s0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005873\",\n            \"end\": \"2015-04-14 20:29:47.875058\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp15s0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp15s0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:47.869185\",\n            \"stderr\": \"\",\n            \"stdout\": \"0f:00.0\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005870\",\n            \"end\": \"2015-04-14 20:29:48.112027\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp5s0f1: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp5s0f1:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:48.106157\",\n            \"stderr\": \"\",\n            \"stdout\": \"05:00.1\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005863\",\n            \"end\": \"2015-04-14 20:29:48.355733\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp5s0f2: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp5s0f2:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:48.349870\",\n            \"stderr\": \"\",\n            \"stdout\": \"05:00.2\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005829\",\n            \"end\": \"2015-04-14 20:29:48.591244\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp5s0f3: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp5s0f3:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:48.585415\",\n            \"stderr\": \"\",\n            \"stdout\": \"05:00.3\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005943\",\n            \"end\": \"2015-04-14 20:29:48.910992\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp9s0f0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp9s0f0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:48.905049\",\n            \"stderr\": \"\",\n            \"stdout\": \"09:00.0\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005863\",\n            \"end\": \"2015-04-14 20:29:49.143706\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i enp9s0f1: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"enp9s0f1:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:49.137843\",\n            \"stderr\": \"\",\n            \"stdout\": \"09:00.1\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i lo: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005856\",\n            \"end\": \"2015-04-14 20:29:49.386044\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i lo: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"lo:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:49.380188\",\n            \"stderr\": \"Cannot get driver information: Operation not supported\",\n            \"stdout\": \"\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i virbr0: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.005859\",\n            \"end\": \"2015-04-14 20:29:49.632356\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i virbr0: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"virbr0:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:49.626497\",\n            \"stderr\": \"\",\n            \"stdout\": \"\",\n            \"warnings\": []\n        },\n        {\n            \"changed\": true,\n            \"cmd\": \"ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22\",\n            \"delta\": \"0:00:00.024850\",\n            \"end\": \"2015-04-14 20:29:49.901539\",\n            \"invocation\": {\n                \"module_args\": \"ethtool -i virbr0-nic: | grep bus-info | cut -b 16-22\",\n                \"module_name\": \"shell\"\n            },\n            \"item\": \"virbr0-nic:\",\n            \"rc\": 0,\n            \"start\": \"2015-04-14 20:29:49.876689\",\n            \"stderr\": \"\",\n            \"stdout\": \"\",\n            \"warnings\": []\n        }\n    ]\n\nMy objective is to get the value of stdout in each item above for example (\"stdout\": \"09:00.0\") . I tried giving something like \n     - name: Binding the interfaces\n       shell: echo {{ item.item.stdout}}\n       with_flattened: var2.results\n#       with_indexed_items: var2.results\n       register: var3\n\nBut this is not giving the bus values in stdout correctly. Appreciate help in listing the variable of variable value in task as given below when the second variable is and indexed list. I am trying to avoid direct index numbering such as item[0] because the number of interfaces are dynamic and direct indexing may result in unexpected outcomes. \nThanks",
    "answer": "Is this what you're looking for:\n\nVariables registered for a task that has with_items have different format, they contain results for all items.\n\n- hosts: localhost\n  tags: s21\n  gather_facts: no\n  vars:\n    images:\n      - foo\n      - bar\n  tasks:\n    - shell: \"echo result-{{item}}\"\n      register: \"r\"\n      with_items: \"{{images}}\"\n\n    - debug: var=r\n\n    - debug: msg=\"item.item={{item.item}}, item.stdout={{item.stdout}}, item.changed={{item.changed}}\"\n      with_items: \"{{r.results}}\"\n\n    - debug: msg=\"Gets printed only if this item changed - {{item}}\"\n      when: \"{{item.changed == true}}\"\n      with_items: \"{{r.results}}\"\n\n\nSource: Register variables in with_items loop in Ansible playbook"
  },
  {
    "question": "I am using Ansible's shell module to find a particular string and store it in a variable. But if grep did not find anything I am getting an error.\nExample:\n- name: Get the http_status\n  shell: grep \"http_status=\" /var/httpd.txt\n  register: cmdln\n  check_mode: no\n\nWhen I run this Ansible playbook if http_status string is not there, playbook is stopped. I am not getting stderr.\nHow can I make Ansible run without interruption even if the string is not found?",
    "answer": "grep by design returns code 1 if the given string was not found. Ansible by design stops execution if the return code is different from 0. Your system is working properly.\nTo prevent Ansible from stopping playbook execution on this error, you can:\n\nadd ignore_errors: yes parameter to the task\nuse failed_when: parameter with a proper condition\n\nBecause grep returns error code 2 for exceptions, the second method seems more appropriate, so:\n- name: Get the http_status\n  shell: grep \"http_status=\" /var/httpd.txt\n  register: cmdln\n  failed_when: \"cmdln.rc == 2\"\n  check_mode: no\n\nYou might also consider adding changed_when: false so that the task won't be reported as \"changed\" every single time.\nAll options are described in the Error Handling In Playbooks document."
  },
  {
    "question": "I have written a simple playbook to print java process ID and other information of that PID \n[root@server thebigone]# cat check_java_pid.yaml\n---\n- hosts: all\n  gather_facts: no\n\n  tasks:\n    - name: Check PID of existing Java process\n      shell: \"ps -ef | grep [j]ava\"\n      register: java_status\n\n    - debug: var=java_status.stdout\n\nAnd when I am calling this with ansible-playbook check_java_pid.yamlit's working fine. \nNow I am trying to call the above playbook from another one but only for a specific host. So I have written the 2nd playbook as below \n    [root@server thebigone]# cat instance_restart.yaml\n    ---\n    - hosts: instance_1\n      gather_facts: no\n\n      tasks:\n        - include: check_java_pid.yaml\n\nBut while doing ansible-playbook instance_restart.yaml, I am getting below errors\n    ERROR! no action detected in task. This often indicates a misspelled \n    module name, or incorrect module path.\n\n    The error appears to have been in \n    '/home/root/ansible/thebigone/check_java_pid.yaml': line 2, column 3, but \n    may be elsewhere in the file depending on the exact syntax problem.\n\n    The offending line appears to be:\n\n     ---\n     - hosts: all\n       ^ here\n\n\n      The error appears to have been in \n      '/home/root/ansible/thebigone/check_java_pid.yaml': line 2, column 3, \n      but may be elsewhere in the file depending on the exact syntax problem.\n\n     The offending line appears to be:\n\n      ---\n      - hosts: all\n        ^ here\n\nIts saying syntax error but there isn't one really AFAIK as I have executed Playbook check_java_pid.yaml without any issues.\nRequesting your help on understanding this issue.",
    "answer": "Here you have examples in official documentation.\nhttps://docs.ansible.com/ansible/2.4/playbooks_reuse_includes.html\nI had same error as yours after applying the aproved answer. I resolved problem by creating master playbook like this:\n---\n- import_playbook: master-okd.yml\n- import_playbook: infra-okd.yml\n- import_playbook: compute-okd.yml"
  },
  {
    "question": "Is it possible to check if a string exists in a file using Ansible?\nI want to check is a user has access to a server. This can be done on the server using cat /etc/passwd | grep username, but I want Ansible to stop if the user is not there.\nI have tried to use the lineinfile but can't seem to get it to return.\n- name: find\n  lineinfile: \n    dest: /etc/passwd\n    regexp: [user]\n    state: present\n    line: \"user\"\n\nThe code above adds user to the file if he is not there. All I want to do is check. I don't want to modify the file in any way, is this possible.",
    "answer": "It's a tricky one. the lineinfile module is specifically intended for modifying the content of a file, but you can use it for a validation check as well.\n- name: find\n  lineinfile: \n    dest: /etc/passwd\n    line: \"user\"\n  check_mode: yes\n  register: presence\n  failed_when: presence.changed\n\ncheck_mode ensures it never updates the file.\nregister saves the variable as noted.\nfailed_when allows you to set the failure condition i.e. by adding the user because it was not found in the file.\nThere are multiple iterations of this that you can use based on what you want the behavior to be. lineinfile docs particular related to state and regexp should allow you to determine whether or not presence or absence is failure etc, or you can do the not presence.changed etc."
  },
  {
    "question": "I try to use Jasypt with Bouncy Castle crypro provides (128Bit AES) in a Spring Application to decrypt entity properties while saving them with Hibernate. But I always get this org.jasypt.exceptions.EncryptionOperationNotPossibleException when try to save the entrity.\norg.jasypt.exceptions.EncryptionOperationNotPossibleException\n  Encryption raised an exception. A possible cause is you are using strong encryption\n  algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited\n  Strength Jurisdiction Policy Files in this Java Virtual Machine\nat org.jasypt.encryption.pbe.StandardPBEByteEncryptor.handleInvalidKeyException(StandardPBEByteEncryptor.java:1073)\nat org.jasypt.encryption.pbe.StandardPBEByteEncryptor.encrypt(StandardPBEByteEncryptor.java:924)\nat org.jasypt.encryption.pbe.StandardPBEStringEncryptor.encrypt(StandardPBEStringEncryptor.java:642)\nat org.jasypt.hibernate4.type.AbstractEncryptedAsStringType.nullSafeSet(AbstractEncryptedAsStringType.java:155)\nat org.hibernate.type.CustomType.nullSafeSet(CustomType.java:158)\n\n(full stacktrace below)\nI do not use Java Cryptography Extension (JCE), thats why I try to use Bouncy Castle\nI think there is something wrong with the spring configuration, does anybody find the problem?\nMy spring Configuration is:\n<bean id=\"bouncyCastleProvider\" class=\"org.bouncycastle.jce.provider.BouncyCastleProvider\"/>\n<bean class=\"org.jasypt.hibernate4.encryptor.HibernatePBEStringEncryptor\" depends-on=\"bouncyCastleProvider\">\n\n    <property name=\"provider\" ref=\"bouncyCastleProvider\"/>  \n    <property name=\"providerName\" value=\"BC\"/>\n\n    <property name=\"saltGenerator\">\n        <bean class=\"org.jasypt.salt.RandomSaltGenerator\"/>\n    </property>\n\n    <property name=\"registeredName\" value=\"STRING_ENCRYPTOR\"/>      \n    <property name=\"algorithm\" value=\"PBEWITHSHA256AND128BITAES-CBC-BC\"/>\n    <property name=\"password\" value=\"sEcRET1234\"/>\n</bean>\n\nUsage:\n@Entity\n@TypeDef(name = \"encryptedString\", typeClass = EncryptedStringType.class, parameters = { @Parameter(name = \"encryptorRegisteredName\", value = \"STRING_ENCRYPTOR\") })\npublic class SubscriptionProcess {\n  ...      \n   @Type(type = \"encryptedString\")\n   private String debitAccountIban;\n  ...\n}\n\npom/dependenies\n    <dependency>\n        <groupId>org.jasypt</groupId>\n        <artifactId>jasypt</artifactId>\n        <version>1.9.2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.jasypt</groupId>\n        <artifactId>jasypt-hibernate4</artifactId>\n        <version>1.9.2</version>\n    </dependency>\n ...\n    <dependency>\n        <groupId>org.bouncycastle</groupId>\n        <!-- I use an older version of bouncy castle that is also used by tika  -->\n        <artifactId>bcprov-jdk15</artifactId>           \n        <version>1.45</version>\n    </dependency>\n    <dependency>\n        <groupId>org.bouncycastle</groupId>\n        <artifactId>bcmail-jdk15</artifactId>           \n        <version>1.45</version>\n    </dependency>\n\nFull Stack Trace\norg.jasypt.exceptions.EncryptionOperationNotPossibleException: Encryption raised an exception. A possible cause is you are using strong encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files in this Java Virtual Machine\n    at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.handleInvalidKeyException(StandardPBEByteEncryptor.java:1073)\n    at org.jasypt.encryption.pbe.StandardPBEByteEncryptor.encrypt(StandardPBEByteEncryptor.java:924)\n    at org.jasypt.encryption.pbe.StandardPBEStringEncryptor.encrypt(StandardPBEStringEncryptor.java:642)\n    at org.jasypt.hibernate4.type.AbstractEncryptedAsStringType.nullSafeSet(AbstractEncryptedAsStringType.java:155)\n    at org.hibernate.type.CustomType.nullSafeSet(CustomType.java:158)\n    at org.hibernate.persister.entity.AbstractEntityPersister.dehydrate(AbstractEntityPersister.java:2843)\n    at org.hibernate.persister.entity.AbstractEntityPersister.dehydrate(AbstractEntityPersister.java:2818)\n    at org.hibernate.persister.entity.AbstractEntityPersister$4.bindValues(AbstractEntityPersister.java:3025)\n    at org.hibernate.id.insert.AbstractReturningDelegate.performInsert(AbstractReturningDelegate.java:57)\n    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3032)\n    at org.hibernate.persister.entity.AbstractEntityPersister.insert(AbstractEntityPersister.java:3556)\n    at org.hibernate.action.internal.EntityIdentityInsertAction.execute(EntityIdentityInsertAction.java:97)\n    at org.hibernate.engine.spi.ActionQueue.execute(ActionQueue.java:480)\n    at org.hibernate.engine.spi.ActionQueue.addResolvedEntityInsertAction(ActionQueue.java:191)\n    at org.hibernate.engine.spi.ActionQueue.addInsertAction(ActionQueue.java:175)\n    at org.hibernate.engine.spi.ActionQueue.addAction(ActionQueue.java:210)\n    at org.hibernate.event.internal.AbstractSaveEventListener.addInsertAction(AbstractSaveEventListener.java:324)\n    at org.hibernate.event.internal.AbstractSaveEventListener.performSaveOrReplicate(AbstractSaveEventListener.java:288)\n    at org.hibernate.event.internal.AbstractSaveEventListener.performSave(AbstractSaveEventListener.java:194)\n    at org.hibernate.event.internal.AbstractSaveEventListener.saveWithGeneratedId(AbstractSaveEventListener.java:125)\n    at org.hibernate.jpa.event.internal.core.JpaPersistEventListener.saveWithGeneratedId(JpaPersistEventListener.java:84)\n    at org.hibernate.event.internal.DefaultPersistEventListener.entityIsTransient(DefaultPersistEventListener.java:206)\n    at org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:149)\n    at org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:75)\n    at org.hibernate.internal.SessionImpl.firePersist(SessionImpl.java:807)\n    at org.hibernate.internal.SessionImpl.persist(SessionImpl.java:780)\n    at org.hibernate.internal.SessionImpl.persist(SessionImpl.java:785)\n    at org.hibernate.jpa.spi.AbstractEntityManagerImpl.persist(AbstractEntityManagerImpl.java:1181)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.springframework.orm.jpa.SharedEntityManagerCreator$SharedEntityManagerInvocationHandler.invoke(SharedEntityManagerCreator.java:291)\n    at com.sun.proxy.$Proxy78.persist(Unknown Source)\n    at com.demo.base.user.BaseUserDomainCreatorUtil$Persistent.postCreate(BaseUserDomainCreatorUtil.java:424)\n    at com.demo.base.user.BaseUserDomainCreatorUtil.createSafeCustodyAccount(BaseUserDomainCreatorUtil.java:321)\n    at com.demo.base.user.BaseUserDomainCreatorUtil.createSafeCustodyAccount(BaseUserDomainCreatorUtil.java:329)\n    at com.demo.base.user.BaseUserDomainCreatorUtil.createSafeCustodyAccount(BaseUserDomainCreatorUtil.java:333)\n    at com.demo.base.user.BaseUserDomainCreatorUtil.createUserWithSafeCustodyAccount(BaseUserDomainCreatorUtil.java:128)\n    at com.demo.app.asset.AssetTestScenario.<init>(AssetTestScenario.java:66)\n    at com.demo.app.asset.dao.SubscriptionProcessDaoSpringTest.testPersistence_aroundBody0(SubscriptionProcessDaoSpringTest.java:62)\n    at com.demo.app.asset.dao.SubscriptionProcessDaoSpringTest$AjcClosure1.run(SubscriptionProcessDaoSpringTest.java:1)\n    at org.springframework.transaction.aspectj.AbstractTransactionAspect.ajc$around$org_springframework_transaction_aspectj_AbstractTransactionAspect$1$2a73e96cproceed(AbstractTransactionAspect.aj:60)\n    at org.springframework.transaction.aspectj.AbstractTransactionAspect$AbstractTransactionAspect$1.proceedWithInvocation(AbstractTransactionAspect.aj:66)\n    at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:267)\n    at org.springframework.transaction.aspectj.AbstractTransactionAspect.ajc$around$org_springframework_transaction_aspectj_AbstractTransactionAspect$1$2a73e96c(AbstractTransactionAspect.aj:64)\n    at com.demo.app.asset.dao.SubscriptionProcessDaoSpringTest.testPersistence(SubscriptionProcessDaoSpringTest.java:61)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)\n    at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)\n    at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)\n    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n    at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)\n    at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)",
    "answer": "Jasypt is designed to be used with JCE providers, the terminology that this project uses on its web may be confusing you since there is the follow sentence:\n\nOpen API for use with any JCE provider, and not only the default Java\n  VM one. Jasypt can be easily used with well-known providers like\n  Bouncy Castle\n\nFrom this sentence maybe you're understanding that Jasypt can be used with JCE or with BouncyCastle like both are working differently or something like that; however, what this sentence means is that there are many JCE providers, default providers which come with default java installation and non-default ones, however both accomplish the JCA/JCE specification  and both can work with Jasypt.\nAs I said BouncyCastle has a JCE provider, from the bouncycastle you can see:\n\nA provider for the Java Cryptography Extension and the Java\n  Cryptography Architecture.\n\nSo if you're trying to make encrypt/decrypt operations using org.bouncycastle.jce.provider.BouncyCastleProvider as provider you've got the same restrictions that all JCE providers have, respect to available algorithms and key length. \nTo avoid this restrictions about key length and algorithms and to pass the errors you have, you must install Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files for your jvm version. \nFor example for java 1.7 you can download the files from here. And then copy the jars in $JAVA_HOME\\jre\\lib\\security overwriting the existing local_policy.jar and US_export_policy.jar.\nHope this helps."
  },
  {
    "question": "I'm using packer with ansible provisioner to build an ami, and terraform to setup the infrastructure with that ami as a source - somewhat similar to this article: http://www.paulstack.co.uk/blog/2016/01/02/building-an-elasticsearch-cluster-in-aws-with-packer-and-terraform\nWhen command packer build pack.json completes successfully I get the output ami id in this format:\neu-central-1: ami-12345678\n\nIn my terraform variables variables.tf I need to specify the source ami id, region etc. The problem here is that I don't want to specify them manually or multiple times. For region (that I know beforehand) it's easy since I can use environment variables in both situations, but what about the output ami? Is there a built-in way to chain these products or some not so hacky approach to do it?\nEDIT: Hacky approach for anyone who might be interested. In this solution I'm greping the aws region & ami from packer output and use a regular expression in perl to write the result into a terraform.tfvars file:\nvars=$(pwd)\"/terraform.tfvars\"\npacker build pack.json | \\\n    tee /dev/tty | \\\n    grep -E -o '\\w{2}-\\w+-\\w{1}: ami-\\w+' | \\\n    perl -ne '@parts = split /[:,\\s]+/, $_; print \"aws_amis.\" . $parts[0] .\" = \\\"\" . $parts[1] . \"\\\"\\n\"' > ${vars}",
    "answer": "You should consider using Terraform's Data Source for aws_ami.  With this, you can rely on custom tags that you set on the AMI when it is created (for example a version number or timestamp).  Then, in the Terraform configuration, you can simply filter the available AMIs for this account and region to get the AMI ID that you need.\nhttps://www.terraform.io/docs/providers/aws/d/ami.html\ndata \"aws_ami\" \"nat_ami\" {\n  most_recent = true\n  executable_users = [\"self\"]\n  filter {\n    name = \"owner-alias\"\n    values = [\"amazon\"]\n  }\n  filter {\n    name = \"name\"\n    values = [\"amzn-ami-vpc-nat*\"]\n  }\n  name_regex = \"^myami-\\\\d{3}\"\n  owners = [\"self\"]\n}\n\nNOTE: in the example above (from the docs), the combination of filters is probably excessive. You can probably get by just fine with something like:\ndata \"aws_ami\" \"image\" {\n  most_recent = true\n  owners = [\"self\"]\n  filter {                       \n    name = \"tag:Application\"     \n    values = [\"my-app-name\"]\n  }                              \n}\n\noutput \"ami_id\" {\n  value = \"${data.aws_ami.image.id}\"\n}\n\nAn additional benefit of this is that you can deploy to multiple regions with the same configuration and no variable map!"
  },
  {
    "question": "I want to run an Ansible action on the condition that a shell command doesn't return the expected output. ogr2ogr --formats pretty-prints a list of compatible file formats. I want to grep the formats output, and if my expected file formats aren't in the output, I want to run a command to install these components. Does anyone know how to do this?\n- name: check if proper ogr formats set up\n  command: ogr2ogr --formats | grep $item\n  with_items:\n    - PostgreSQL\n    - FileGDB\n    - Spatialite\n  register: ogr_check\n\n# If grep from ogr_check didn't find a certain format from with_items, run this\n- name: install proper ogr formats\n  action: DO STUFF\n  when: Not sure what to do here",
    "answer": "First, please make sure you are using Ansible 1.3 or later. Ansible is still changing pretty quickly from what I can see, and a lot of awesome features and bug fixes are crucial.\nAs for checking, you can try something like this, taking advantage of grep's exit code:\n- name: check if proper ogr formats set up\n  shell: ogr2ogr --formats | grep $item\n  with_items:\n    - PostgreSQL\n    - FileGDB\n    - Spatialite\n  register: ogr_check\n  # grep will exit with 1 when no results found. \n  # This causes the task not to halt play.\n  ignore_errors: true\n\n- name: install proper ogr formats\n  action: DO STUFF\n  when: ogr_check|failed\n\nThere are some other useful register variables, namely item.stdout_lines. If you'd like to see what's registered to the variable in detail, try the following task:\n- debug: msg={{ogr_check}}\n\nand then run the task in double verbose mode via ansible-playbook my-playbook.yml -vv. It will spit out a lot of useful dictionary values."
  },
  {
    "question": "I have an ansible playbook to kill running processes and works great most of the time!, however, from time to time we find processes that just can't be killed so, \"wait_for\" gets to the timeout, throws an error and it stops the process. \nThe current workaround is to manually go into the box, use \"kill -9\" and run the ansible playbook again so I was wondering if there is any way to handle this scenario from ansible itself?, I mean, I don't want to use kill -9 from the beginning but I maybe a way to handle the timeout?, even to use kill -9 only if process hasn't been killed in 300 seconds? but what would be the best way to do it?\nThese are the tasks I currently have: \n- name: Get running processes\n  shell: \"ps -ef | grep -v grep | grep -w {{ PROCESS }} | awk '{print $2}'\"\n  register: running_processes\n\n- name: Kill running processes\n  shell: \"kill {{ item }}\"\n  with_items: \"{{ running_processes.stdout_lines }}\"\n\n- name: Waiting until all running processes are killed\n  wait_for:\n    path: \"/proc/{{ item }}/status\"\n    state: absent\n  with_items: \"{{ running_processes.stdout_lines }}\"\n\nThanks!",
    "answer": "You could ignore errors on wait_for and register the result to force kill failed items:\n- name: Get running processes\n  shell: \"ps -ef | grep -v grep | grep -w {{ PROCESS }} | awk '{print $2}'\"\n  register: running_processes\n\n- name: Kill running processes\n  shell: \"kill {{ item }}\"\n  with_items: \"{{ running_processes.stdout_lines }}\"\n\n- wait_for:\n    path: \"/proc/{{ item }}/status\"\n    state: absent\n  with_items: \"{{ running_processes.stdout_lines }}\"\n  ignore_errors: yes\n  register: killed_processes\n\n- name: Force kill stuck processes\n  shell: \"kill -9 {{ item }}\"\n  with_items: \"{{ killed_processes.results | select('failed') | map(attribute='item') | list }}\""
  },
  {
    "question": "I'm running proxmox and I try to remove a pool which I created wrong.\nHowever it keeps giving this error:\nmon_command failed - pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool1_U (500)\nOK\n\nBut:\nroot@kvm-01:~# ceph -n mon.0 --show-config | grep mon_allow_pool_delete\nmon_allow_pool_delete = true\nroot@kvm-01:~# ceph -n mon.1 --show-config | grep mon_allow_pool_delete\nmon_allow_pool_delete = true\nroot@kvm-01:~# ceph -n mon.2 --show-config | grep mon_allow_pool_delete\nmon_allow_pool_delete = true\n\nroot@kvm-01:~# cat /etc/ceph/ceph.conf\n[global]\n         auth client required = cephx\n         auth cluster required = cephx\n         auth service required = cephx\n         cluster network = 10.0.0.0/24\n         filestore xattr use omap = true\n         fsid = 41fa3ff6-e751-4ebf-8a76-3f4a445823d2\n         keyring = /etc/pve/priv/$cluster.$name.keyring\n         osd journal size = 5120\n         osd pool default min size = 1\n         public network = 10.0.0.0/24\n[osd]\n         keyring = /var/lib/ceph/osd/ceph-$id/keyring\n[mon.0]\n         host = kvm-01\n         mon addr = 10.0.0.1:6789\n         mon allow pool delete = true\n\n[mon.2]\n         host = kvm-03\n         mon addr = 10.0.0.3:6789\n         mon allow pool delete = true\n[mon.1]\n         host = kvm-02\n         mon addr = 10.0.0.2:6789\n         mon allow pool delete = true\n\nSo that's my full config. Any idea why I am unable to delete my pools?",
    "answer": "Another approach:\nceph tell mon.\\* injectargs '--mon-allow-pool-delete=true'\nceph osd pool rm test-pool test-pool --yes-i-really-really-mean-it"
  },
  {
    "question": "I'm using Keycloak version 1.6.1, newly installed as a standalone application.\nKeycloak should act as an IdP (Identity provider) for an SP (Service Provider) called Tableau.\nI have read from this page: http://blog.keycloak.org/2015/03/picketlink-and-keycloak-projects-are.html\n\n... Keycloak from being Identity Broker grew into being fully fledged\n  Identity Provider\n\nWhile it was an Identity Broker, it is now also an Identity Provider.\nMy question is then:\nI have exported the SP XML Metadata from Tableau, which I imported into Keycloak, but when it comes to the export of the IdP XML Metadata from Keycloak (which should be imported into Tableau) I cannot find the button/command/guide anything about how to export this XML file.\nI have worked with other IdPs and they all support this export of IdP Metadata which you can see an example of here: https://docs.oracle.com/cd/E19636-01/819-7664/g2enua/index.html\nIf I search for Keycloak and the keyword IDPSSODescriptor I find this:\ngrepcode.com/file/repo1.maven.org/maven2/org.keycloak/keycloak-saml-protocol/1.1.0.Beta2/idp-metadata-template.xml\nWhich is exactly the 'template' I need, with the correct links on all ${idp.sso.HTTP-POST} etc. places.\nShould I create the file manually - if so how do I find the correct POST, REDIRECT etc. URLs?\nOr is there some way of exporting this file I haven't seen?",
    "answer": "Sometimes it's a good thing to specify in writing what you need - which I did here on Stack Overflow.\nI found the URL to where on Keycloak one can export the IdP XML\nhttps://keycloak-url/realms/{REALM-NAME}/protocol/saml/descriptor\n\nThat gave me the IDPSSODescriptor.\nI'll leave this thread here, so people can benefit from my mistakes."
  },
  {
    "question": "We run Keycloak docker image in AWS ECS and we need a way to export a realm and all users for automation purposes using ansible. We can run the following command with ansible to run the export \ndocker exec -i 702f2fd7858d \\\n  /bin/bash -c \"export JDBC_PARAMS=?currentSchema=keycloak_service && \n  /opt/jboss/keycloak/bin/standalone.sh \\\n  -Djboss.socket.binding.port-offset=100 \\\n  -Dkeycloak.migration.action=export \\\n  -Dkeycloak.migration.provider=singleFile \\\n  -Dkeycloak.migration.realmName=API \\\n  -Dkeycloak.migration.usersExportStrategy=REALM_FILE \\\n  -Dkeycloak.migration.file=/tmp/my_realm.json\"\n\nbut the docker container continues to run after the export. We cannot grep the logs looking for the export process finishing as we use an AWS Log Driver for Docker that prevents access to any logs. It's a pity that the Keycloak REST API does not support the inclusion of users in the existing partial-export endpoint or at least to have an endpoint that triggers the export of a realm including users into a mounted filed system.",
    "answer": "I was facing the same problem a few days ago and implemented a working solution:\n#!/usr/bin/env bash\n#\n# backup-keycloak.sh\n\n# Copy the export bash script to the (already running) keycloak container\n# to perform an export\ndocker cp docker-exec-cmd.sh keycloak:/tmp/docker-exec-cmd.sh\n# Execute the script inside of the container\ndocker exec -it keycloak /tmp/docker-exec-cmd.sh\n# Grab the finished export from the container\ndocker cp keycloak:/tmp/realms-export-single-file.json .\n\nThe Bash script to perform the export inside of the container is the following:\n#!/usr/bin/env bash\n#\n# docker-exec-cmd.sh\n\nset -o errexit\nset -o errtrace\nset -o nounset\nset -o pipefail\n\n# If something goes wrong, this script does not run forever, but times out\nTIMEOUT_SECONDS=300\n# Logfile for the keycloak export instance\nLOGFILE=/tmp/standalone.sh.log\n# destionation export file\nJSON_EXPORT_FILE=/tmp/realms-export-single-file.json\n\n# Remove files from old backups inside the container\n# You could also move the files or change the name with timestamp prefix\nrm -f ${LOGFILE} ${JSON_EXPORT_FILE}\n\n# Start a new keycloak instance with exporting options enabled.\n# Use the port offset argument to prevent port conflicts\n# with the \"real\" keycloak instance.\ntimeout ${TIMEOUT_SECONDS}s \\\n    /opt/jboss/keycloak/bin/standalone.sh \\\n        -Dkeycloak.migration.action=export \\\n        -Dkeycloak.migration.provider=singleFile \\\n        -Dkeycloak.migration.file=${JSON_EXPORT_FILE} \\\n        -Djboss.socket.binding.port-offset=99 \\\n    > ${LOGFILE} &\n\n# Grab the keycloak export instance process id\nPID=\"${!}\"\n\n# Wait for the export to finish\n# It will wait till it sees the string, which indicates\n# a successful finished backup.\n# If it will take too long (>TIMEOUT_SECONDS), it will be stopped.\ntimeout ${TIMEOUT_SECONDS}s \\\n    grep -m 1 \"Export finished successfully\" <(tail -f ${LOGFILE})\n\n# Stop the keycloak export instance\nkill ${PID}"
  },
  {
    "question": "I'm looking for a way to tell (from within a script) when a Kubernetes Job has completed. I want to then get the logs out of the containers and perform cleanup. \nWhat would be a good way to do this? Would the best way be to run kubectl describe job <job_name> and grep for 1 Succeeded or something of the sort?",
    "answer": "Since version 1.11, you can do:\nkubectl wait --for=condition=complete job/myjob\n\nand you can also set a timeout:\nkubectl wait --for=condition=complete --timeout=30s job/myjob"
  },
  {
    "question": "I am trying to get the namespace of the currently used Kubernetes context using kubectl.\nI know there is a command kubectl config get-contexts but I see that it cannot output in json/yaml. The only script I've come with is this:\nkubectl config get-contexts --no-headers | grep '*' | grep -Eo '\\S+$'",
    "answer": "This works if you have a namespace selected in your context:\nkubectl config view --minify -o jsonpath='{..namespace}'\n\nAlso, kube-ps1 can be used to display your current context and namespace in your shell prompt."
  },
  {
    "question": "I am trying to create a Kafka Streams Application which processes Avro records, but I am getting the following error:\nException in thread \"streams-application-c8031218-8de9-4d55-a5d0-81c30051a829-StreamThread-1\" org.apache.kafka.streams.errors.StreamsException: Deserialization exception handler is set to fail upon a deserialization error. If you would rather have the streaming pipeline continue after a deserialization error, please set the default.deserialization.exception.handler appropriately.\nat org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:74)\nat org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:91)\nat org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:117)\nat org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:567)\nat org.apache.kafka.streams.processor.internals.StreamThread.addRecordsToTasks(StreamThread.java:900)\nat org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:801)\nat org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)\nat org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:719)\nCaused by: org.apache.kafka.common.errors.SerializationException: Error deserializing Avro message for id -1\nCaused by: org.apache.kafka.common.errors.SerializationException: Unknown magic byte!\n\nI am not sure what is causing this error. I am just trying to get Avro records into the application first where they then will be processed and then output to another topic but it doesn't not seem to be working. I have included the code from the application below. Can anyone see why it is not working?\n    Properties props = new Properties();\n    props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"streams-application\");\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n    props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"http://localhost:8081\");\n    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n    Serde<String> stringSerde = Serdes.String();\n    Serde<trackingReport> specificAvroTrackingReportSerde = new SpecificAvroSerde<trackingReport>();\n\n    specificAvroTrackingReportSerde.configure(Collections.singletonMap(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"http://localhost:8081\"), false);\n\n\n    StreamsBuilder builder = new StreamsBuilder();\n    KStream<String, trackingReport> inputreports = builder.stream(\"intesttopic\", Consumed.with(stringSerde, specificAvroTrackingReportSerde));\n\n\n    KStream<String, trackingReport> outputreports = inputreports;\n\n    String outputTopic = \"outtesttopic\";\n    outputreports.to(outputTopic, Produced.with(stringSerde, specificAvroTrackingReportSerde));\n\n    Topology topology = builder.build();\n\n    KafkaStreams streams = new KafkaStreams(topology, props);\n    streams.start();",
    "answer": "Unknown magic byte!\n\nMeans your data does not adhere to the wire format that's expected for the Schema Registry. \nOr, in other words, the data you're trying to read, is not Avro, as expected by the Confluent Avro deserializer. \nYou can expect the same error by running kafka-avro-console-consumer, by the way, so you may want to debug using that too \nIf you are sure your data is indeed Avro, and the schema is actually sent as part of the message (would need to see your producer code), then you shouldn't use the Confluent Avro deserializers that are expecting a specific byte format in the message. Instead, you could use ByteArrayDesrializer and read the Avro record yourself, then pass it to the Apache Avro BinaryDecoder class. As a bonus, you can extract that logic into your own Deserialzer class \nAlso, if the input topic is Avro, I don't think you should be using this property for reading strings \nDEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());"
  },
  {
    "question": "I know for a fact that Flask, in debug mode, will detect changes to .py source code files and will reload them when new requests come in.\nI used to see this in my app all the time.  Change a little text in an @app.route decoration section in my views.py file, and I could see the changes in the browser upon refresh.\nBut all of a sudden (can't remember what changed), this doesn't seem to work anymore.\nQ: Where am I going wrong?\nI am running on a OSX 10.9 system with a VENV setup using Python 2.7.  I use foreman start in my project root to start it up.\nApp structure is like this:\n[Project Root]\n+-[app]\n| +-__init__.py\n| +- views.py\n| +- ...some other files...\n+-[venv]\n+- config.py\n+- Procfile\n+- run.py\n\nThe files look like this:\n\n# Procfile\nweb: gunicorn --log-level=DEBUG run:app\n\n# config.py\ncontains some app specific configuration information.\n\n# run.py\nfrom app import app\n\nif __name__ == \"__main__\":\n    app.run(debug = True, port = 5000)\n\n# __init__.py\nfrom flask import Flask\nfrom flask.ext.login import LoginManager\nfrom flask.ext.sqlalchemy import SQLAlchemy\nfrom flask.ext.mail import Mail\nimport os\n\napp = Flask(__name__)\napp.config.from_object('config')\n\ndb = SQLAlchemy(app)\n\n#mail sending\nmail = Mail(app)\n\nlm = LoginManager()\nlm.init_app(app)\nlm.session_protection = \"strong\"\n\nfrom app import views, models\n\n# app/views.py\n@app.route('/start-scep')\ndef start_scep():\n    startMessage = '''\\\n<html>\n<header>\n<style>\nbody { margin:40px 40px;font-family:Helvetica;}\nh1 { font-size:40px; }\np { font-size:30px; }\na { text-decoration:none; }\n</style>\n</header>\n\n<p>Some text</p>\n</body>\n</html>\\\n'''\n    response = make_response(startMessage)\n    response.headers['Content-Type'] = \"text/html\"\n    print response.headers\n    return response",
    "answer": "The issue here, as stated in other answers, is that it looks like you moved from python run.py to foreman start, or you changed your Procfile from\n# Procfile\nweb: python run.py\n\nto\n# Procfile\nweb: gunicorn --log-level=DEBUG run:app\n\nWhen you run foreman start, it simply runs the commands that you've specified in the Procfile. (I'm going to guess you're working with Heroku, but even if not, this is nice because it will mimic what's going to run on your server/Heroku dyno/whatever.)\nSo now, when you run gunicorn --log-level=DEBUG run:app (via foreman start) you are now running your application with gunicorn rather than the built in webserver that comes with Flask. The run:app argument tells gunicorn to look in run.py for a Flask instance named app, import it, and run it. This is where it get's fun: since the run.py is being imported, __name__ == '__main__' is False (see more on that here), and so app.run(debug = True, port = 5000) is never called.\nThis is what you want (at least in a setting that's available publicly) because the webserver that's built into Flask that's used when app.run() is called has some pretty serious security vulnerabilities. The --log-level=DEBUG may also be a bit deceiving since it uses the word \"DEBUG\" but it's only telling gunicorn which logging statements to print and which to ignore (check out the Python docs on logging.)\nThe solution is to run python run.py when running the app locally and working/debugging on it, and only run foreman start when you want to mimic a production environment. Also, since gunicorn only needs to import the app object, you could remove some ambiguity and change your Procfile to\n# Procfile\nweb: gunicorn --log-level=DEBUG app:app\n\nYou could also look into Flask Script which has a built in command python manage.py runserver that runs the built in Flask webserver in debug mode."
  },
  {
    "question": "How can I test that stderr is non empty::\n- name: Check script\n  shell: . {{ venv_name }}/bin/activate && myscritp.py\n  args:\n    chdir: \"{{ home }}\"\n  sudo_user: \"{{ user }}\"\n  register: test_myscript\n\n- debug: msg='myscritp is Ok'\n  when: not test_myscript.stderr\n\nSo if there is no error I could read::\n  TASK: [deploy | debug msg='critp is Ok] *******\n  ok: [vagrant] => {\n      \"msg\": \"myscritp is Ok\"\n  }\n\nIn case the stderr is not empty a FATAl error occurs.",
    "answer": "(ansible 2.9.6 ansible-lint 5.3.2)\nSee ansible-lint rules. The condition below results in error: 'empty-string-compare: Don't compare to empty string'\n    when: test_myscript.stderr != \"\"\n\nCorrect syntax is\n    when: test_myscript.stderr | length > 0\n\nQuoting from source code\n\nUse when: var|length > 0 rather than when: var != \"\" (or '\n'conversely when: var|length == 0 rather than when: var == \"\")\n\n\nNotes\n\nTest empty bare variable e.g.\n\n    - debug:\n        msg: \"Empty string '{{ var }}' evaluates to False\"\n      when: not var\n      vars:\n        var: ''\n\n    - debug:\n        msg: \"Empty list {{ var }} evaluates to False\"\n      when: not var\n      vars:\n        var: []\n\ngive\n  msg: Empty string '' evaluates to False\n  msg: Empty list [] evaluates to False\n\n\nBut, testing non-empty bare variable string depends on CONDITIONAL_BARE_VARS. Setting ANSIBLE_CONDITIONAL_BARE_VARS=false the condition works fine but setting ANSIBLE_CONDITIONAL_BARE_VARS=true the condition will fail\n\n    - debug:\n        msg: \"String '{{ var }}' evaluates to True\"\n      when: var\n      vars:\n        var: 'abc'\n\ngives\nfatal: [localhost]: FAILED! => \n  msg: |-\n    The conditional check 'var' failed. The error was: error while \n    evaluating conditional (var): 'abc' is undefined\n\nExplicit cast to Boolean prevents the error but evaluates to False i.e. will be always skipped (unless var='True'). When the filter bool is used the options ANSIBLE_CONDITIONAL_BARE_VARS=true and ANSIBLE_CONDITIONAL_BARE_VARS=false have no effect\n    - debug:\n        msg: \"String '{{ var }}' evaluates to True\"\n      when: var|bool\n      vars:\n        var: 'abc'\n\ngives\nskipping: [localhost]\n\n\nQuoting from Porting guide 2.8 Bare variables in conditionals\n\n  - include_tasks: teardown.yml\n    when: teardown\n\n  - include_tasks: provision.yml\n    when: not teardown\n\n\n\" based on a variable you define as a string (with quotation marks around it):\"\n\n\nIn Ansible 2.7 and earlier, the two conditions above are evaluated as True and False respectively if teardown: 'true'\n\nIn Ansible 2.7 and earlier, both conditions were evaluated as False if teardown: 'false'\n\nIn Ansible 2.8 and later, you have the option of disabling conditional bare variables, so when: teardown always evaluates as True, and when: not teardown always evaluates as False when teardown is a non-empty string (including 'true' or 'false')\n\n\n\nQuoting from CONDITIONAL_BARE_VARS\n\n\nExpect that this setting eventually will be deprecated after 2.12"
  },
  {
    "question": "This is hopefully a quick one to answer, I'm trying to provision a box on AWS with puppet and one of the steps involves a pip install from a requirements file. Something like this: -\n/usr/local/venv/ostcms/bin/pip install -r /vagrant/requirements.txt\n\nThe step basically fails because it can't find any of the packages in the requirements file, but when I open the AWS box's security group up to allow \"All Traffic\" the pip step works.\nI'm trying to find the port that pip uses so I can basically have that port, http and ssh open on the box and live happily ever after.",
    "answer": "Pip runs on 3128 so make sure you have that open in your AWS console.  Otherwise pip will get blocked when attempting to talk to PyPi (or anywhere else it cares to download from)."
  },
  {
    "question": "I'm installing a previously built website on a new server. I'm not the original developer.\nI've used Gunicorn + nginx in the past to keep the app alive (basically following this tutorial), but am having problems with it here.\nI source venv/bin/activate, then ./manage.py runserver 0.0.0.0:8000 works well and everything is running as expected. I shut it down and run gunicorn --bind 0.0.0.0:8000 myproject.wsgi:application, and get the following:\n[2016-09-13 01:11:47 +0000] [15259] [INFO] Starting gunicorn 19.6.0\n[2016-09-13 01:11:47 +0000] [15259] [INFO] Listening at: http://0.0.0.0:8000 (15259)\n[2016-09-13 01:11:47 +0000] [15259] [INFO] Using worker: sync\n[2016-09-13 01:11:47 +0000] [15262] [INFO] Booting worker with pid: 15262\n[2016-09-13 01:11:47 +0000] [15262] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/arbiter.py\", line 557, in spawn_worker\n    worker.init_process()\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/workers/base.py\", line 126, in init_process\n    self.load_wsgi()\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/workers/base.py\", line 136, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/app/base.py\", line 67, in wsgi\n    self.callable = self.load()\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/var/www/myproject/venv/lib/python3.5/site-packages/gunicorn/util.py\", line 357, in import_app\n    __import__(module)\nImportError: No module named 'myproject.wsgi'\n[2016-09-13 01:11:47 +0000] [15262] [INFO] Worker exiting (pid: 15262)\n[2016-09-13 01:11:47 +0000] [15259] [INFO] Shutting down: Master\n[2016-09-13 01:11:47 +0000] [15259] [INFO] Reason: Worker failed to boot.\n\nI believe it has something to do with the structure of the whole application. Before, I've built apps with the basic structure of:\nmyproject\n├── manage.py\n├── myproject\n│   ├── urls.py\n│   ├── views.py\n│   ├── component1\n│   │   ├── urls.py\n│   │   └── views.py\n│   ├── component2\n│   │   ├── urls.py\n│   │   └── views.py\n├── venv\n│   ├── bin\n│   └── ...\n\nThis one, instead, has a structure like:\nmyproject\n├── apps\n│   ├── blog\n│   │   ├── urls.py\n│   │   ├── views.py\n│   │     └── ...\n│   ├── catalogue\n│   │   ├── urls.py\n│   │   ├── views.py\n│   │     └── ...\n│   ├── checkout\n│   │   ├── urls.py\n│   │   ├── views.py\n│   │     └── ...\n│   ├── core\n│   │   ├── urls.py\n│   │   ├── views.py\n│   │     └── ...\n│   ├── customer\n│   ├── dashboard\n│   └──  __init__.py\n├── __init__.py\n├── manage.py\n├── project_static\n│   ├── assets\n│   ├── bower_components\n│   └── js\n├── public\n│   ├── emails\n│   ├── media\n│   └── static\n├── settings\n│   ├── base.py\n│   ├── dev.py\n│   ├── __init__.py\n│   ├── local.py\n│   └── production.py\n├── templates\n│   ├── base.html\n│   ├── basket\n│   ├── blog\n│   └── ....\n├── urls.py\n├── venv\n│   ├── bin\n│   ├── include\n│   ├── lib\n│   ├── pip-selfcheck.json\n│   └── share\n└── wsgi.py\n\nSo, there's no 'main' module running the show, which is what I expect gunicorn is looking for.\nAny thoughts?\nwsgi.py:\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"settings\")\n\napplication = get_wsgi_application()",
    "answer": "Your error message is \nImportError: No module named 'myproject.wsgi'\n\nYou ran the app with\ngunicorn --bind 0.0.0.0:8000 myproject.wsgi:application\n\nAnd wsgi.py has the line\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"settings\")\n\nThis is the disconnect.  In order to recognize the project as myproject.wsgi the parent directory would have to be on the python path... running \ncd .. && gunicorn --bind 0.0.0.0:8000 myproject.wsgi:application\n\nWould eliminate that error.  However, you would then get a different error because the wsgi.py file refers to settings instead of myproject.settings.  This implies that the app was intended to be run from the root directory instead of one directory up.  You can figure this out for sure by looking at the code- if it uses absolute imports, do they usually say from myproject.app import ... or from app import ....  If that guess is correct, your correct commmand is\ngunicorn --bind 0.0.0.0:8000 wsgi:application\n\nIf the app does use myproject in all of the paths, you'll have to modify your PYTHONPATH to run it properly...\nPYTHONPATH=`pwd`/.. gunicorn --bind 0.0.0.0:8000 myproject.wsgi:application"
  },
  {
    "question": "I'm trying to setup an application webserver using uWSGI + Nginx, which runs a Flask application using SQLAlchemy to communicate to a Postgres database.\nWhen I make requests to the webserver, every other response will be a 500 error.\nThe error is:\nTraceback (most recent call last):\n  File \"/var/env/argos/lib/python3.3/site-packages/sqlalchemy/engine/base.py\", line 867, in _execute_context\n    context)\n  File \"/var/env/argos/lib/python3.3/site-packages/sqlalchemy/engine/default.py\", line 388, in do_execute\n    cursor.execute(statement, parameters)\npsycopg2.OperationalError: SSL error: decryption failed or bad record mac\n\n\nThe above exception was the direct cause of the following exception:\n\nsqlalchemy.exc.OperationalError: (OperationalError) SSL error: decryption failed or bad record mac\n\nThe error is triggered by a simple Flask-SQLAlchemy method:\nresult = models.Event.query.get(id)\n\n\nuwsgi is being managed by supervisor, which has a config:\n[program:my_app]\ncommand=/usr/bin/uwsgi --ini /etc/uwsgi/apps-enabled/myapp.ini --catch-exceptions\ndirectory=/path/to/my/app\nstopsignal=QUIT\nautostart=true\nautorestart=true\n\nand uwsgi's config looks like:\n[uwsgi]\nsocket = /tmp/my_app.sock\nlogto = /var/log/my_app.log\nplugins = python3\nvirtualenv =  /path/to/my/venv\npythonpath = /path/to/my/app\nwsgi-file = /path/to/my/app/application.py\ncallable = app\nmax-requests = 1000\nchmod-socket = 666\nchown-socket = www-data:www-data\nmaster = true\nprocesses = 2\nno-orphans = true\nlog-date = true\nuid = www-data\ngid = www-data\n\nThe furthest that I can get is that it has something to do with uwsgi's forking. But beyond that I'm not clear on what needs to be done.",
    "answer": "The issue ended up being uwsgi's forking.\nWhen working with multiple processes with a master process, uwsgi initializes the application in the master process and then copies the application over to each worker process. The problem is if you open a database connection when initializing your application, you then have multiple processes sharing the same connection, which causes the error above.\nThe solution is to set the lazy configuration option for uwsgi, which forces a complete loading of the application in each process:\n\nlazy\nSet lazy mode (load apps in workers instead of master).\nThis option may have memory usage implications as Copy-on-Write semantics can not be used. When lazy is enabled, only workers will be reloaded by uWSGI’s reload signals; the master will remain alive. As such, uWSGI configuration changes are not picked up on reload by the master.\n\nThere's also a lazy-apps option:\n\nlazy-apps\nLoad apps in each worker instead of the master.\nThis option may have memory usage implications as Copy-on-Write semantics can not be used. Unlike lazy, this only affects the way applications are loaded, not master’s behavior on reload.\n\nThis uwsgi configuration ended up working for me:\n[uwsgi]\nsocket = /tmp/my_app.sock\nlogto = /var/log/my_app.log\nplugins = python3\nvirtualenv =  /path/to/my/venv\npythonpath = /path/to/my/app\nwsgi-file = /path/to/my/app/application.py\ncallable = app\nmax-requests = 1000\nchmod-socket = 666\nchown-socket = www-data:www-data\nmaster = true\nprocesses = 2\nno-orphans = true\nlog-date = true\nuid = www-data\ngid = www-data\n\n# the fix\nlazy = true\nlazy-apps = true"
  },
  {
    "question": "I've had a RabbitMQ server running for months. This morning I was unable to connect to it, my applications was timing out and the Management client was unresponsive. Rebooted the machine. Applications are still timing out. I'm able to login to the Management client but I see this message:\n\nVirtual host / experienced an error on node rabbit@MQT01 and may be inaccessible\n\nAll my queues are there but can't see any exchanges.\nI hope someone can help me figure out what going on. I've looked at the logs but can't find any good hint.\nHere a part of the log:\n\n\n2018-09-11 09:39:42 =ERROR REPORT====\r\n** Generic server <0.281.0> terminating\r\n** Last message in was {'$gen_cast',{submit_async,#Fun<rabbit_queue_index.36.122888644>}}\r\n** When Server state == undefined\r\n** Reason for termination == \r\n** {function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]}\r\n2018-09-11 09:39:42 =CRASH REPORT====\r\n  crasher:\r\n    initial call: worker_pool_worker:init/1\r\n    pid: <0.281.0>\r\n    registered_name: []\r\n    exception exit: {{function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]},[{gen_server2,terminate,3,[{file,\"src/gen_server2.erl\"},{line,1161}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}\r\n    ancestors: [worker_pool_sup,rabbit_sup,<0.262.0>]\r\n    message_queue_len: 0\r\n    messages: []\r\n    links: [<0.276.0>,<0.336.0>,#Port<0.31196>]\r\n    dictionary: [{fhc_age_tree,{1,{{10352640,#Ref<0.1077581647.1695285251.67028>},true,nil,nil}}},{worker_pool_worker,true},{rand_seed,{#{jump => #Fun<rand.16.15449617>,max => 288230376151711743,next => #Fun<rand.15.15449617>,type => exsplus},[257570830250844431|246837015578235662]}},{worker_pool_name,worker_pool},{{\"c:/Users/dfpsb/AppData/Roaming/RabbitMQ/db/RABBIT~1/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/queues/9GD33C2I2PKZ7A8QHZ4MWWCKE/journal.jif\",fhc_file},{file,1,true}},{{#Ref<0.1077581647.1695285251.67028>,fhc_handle},{handle,{file_descriptor,prim_file,{#Port<0.31196>,1808}},#Ref<0.1077581647.1695285251.67028>,240,false,0,infinity,[],<<>>,0,0,0,0,0,false,\"c:/Users/dfpsb/AppData/Roaming/RabbitMQ/db/RABBIT~1/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L/queues/9GD33C2I2PKZ7A8QHZ4MWWCKE/journal.jif\",[write,binary,raw,read],[{write_buffer,infinity}],true,true,10352640}}]\r\n    trap_exit: false\r\n    status: running\r\n    heap_size: 10958\r\n    stack_size: 27\r\n    reductions: 104391\r\n  neighbours:\r\n    neighbour: [{pid,<0.279.0>},{registered_name,[]},{initial_call,{worker_pool_worker,init,['Argument__1']}},{current_function,{gen,do_call,4}},{ancestors,[worker_pool_sup,rabbit_sup,<0.262.0>]},{message_queue_len,0},{links,[<0.276.0>,<0.336.0>]},{trap_exit,false},{status,waiting},{heap_size,4185},{stack_size,42},{reductions,21548},{current_stacktrace,[{gen,do_call,4,[{file,\"gen.erl\"},{line,169}]},{gen_server,call,3,[{file,\"gen_server.erl\"},{line,210}]},{file,call,2,[{file,\"file.erl\"},{line,1499}]},{rabbit_queue_index,get_journal_handle,1,[{file,\"src/rabbit_queue_index.erl\"},{line,881}]},{rabbit_queue_index,load_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,894}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,904}]},{rabbit_queue_index,scan_queue_segments,3,[{file,\"src/rabbit_queue_index.erl\"},{line,724}]},{rabbit_queue_index,queue_index_walker_reader,2,[{file,\"src/rabbit_queue_index.erl\"},{line,712}]}]}]\r\n    neighbour: [{pid,<0.278.0>},{registered_name,[]},{initial_call,{worker_pool_worker,init,['Argument__1']}},{current_function,{gen,do_call,4}},{ancestors,[worker_pool_sup,rabbit_sup,<0.262.0>]},{message_queue_len,0},{links,[<0.276.0>,<0.336.0>,#Port<0.31157>]},{trap_exit,false},{status,waiting},{heap_size,6772},{stack_size,102},{reductions,129623},{current_stacktrace,[{gen,do_call,4,[{file,\"gen.erl\"},{line,169}]},{gen_server2,call,3,[{file,\"src/gen_server2.erl\"},{line,323}]},{array,sparse_foldr_3,6,[{file,\"array.erl\"},{line,1848}]},{array,sparse_foldr_2,8,[{file,\"array.erl\"},{line,1837}]},{lists,foldr,3,[{file,\"lists.erl\"},{line,1276}]},{rabbit_queue_index,scan_queue_segments,3,[{file,\"src/rabbit_queue_index.erl\"},{line,725}]},{rabbit_queue_index,queue_index_walker_reader,2,[{file,\"src/rabbit_queue_index.erl\"},{line,712}]},{rabbit_queue_index,'-queue_index_walker/1-fun-0-',2,[{file,\"src/rabbit_queue_index.erl\"},{line,694}]}]}]\r\n    neighbour: [{pid,<0.280.0>},{registered_name,[]},{initial_call,{worker_pool_worker,init,['Argument__1']}},{current_function,{array,set_1,4}},{ancestors,[worker_pool_sup,rabbit_sup,<0.262.0>]},{message_queue_len,0},{links,[<0.276.0>,<0.336.0>,#Port<0.31170>]},{trap_exit,false},{status,runnable},{heap_size,121536},{stack_size,44},{reductions,122988},{current_stacktrace,[{array,set_1,4,[{file,\"array.erl\"},{line,590}]},{array,set_1,4,[{file,\"array.erl\"},{line,592}]},{array,set_1,4,[{file,\"array.erl\"},{line,592}]},{array,set,3,[{file,\"array.erl\"},{line,574}]},{rabbit_queue_index,parse_segment_publish_entry,5,[{file,\"src/rabbit_queue_index.erl\"},{line,1135}]},{rabbit_queue_index,segment_entries_foldr,3,[{file,\"src/rabbit_queue_index.erl\"},{line,1091}]},{lists,foldr,3,[{file,\"lists.erl\"},{line,1276}]},{rabbit_queue_index,scan_queue_segments,3,[{file,\"src/rabbit_queue_index.erl\"},{line,725}]}]}]\r\n    neighbour: [{pid,<0.336.0>},{registered_name,[]},{initial_call,{gatherer,init,['Argument__1']}},{current_function,{gen_server2,process_next_msg,1}},{ancestors,[<0.332.0>,<0.324.0>,<0.323.0>,rabbit_vhost_sup_sup,rabbit_sup,<0.262.0>]},{message_queue_len,2},{links,[<0.280.0>,<0.332.0>,<0.281.0>,<0.278.0>,<0.279.0>]},{trap_exit,false},{status,runnable},{heap_size,987},{stack_size,8},{reductions,73223},{current_stacktrace,[{gen_server2,process_next_msg,1,[{file,\"src/gen_server2.erl\"},{line,666}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}]\r\n2018-09-11 09:39:42 =CRASH REPORT====\r\n  crasher:\r\n    initial call: rabbit_msg_store:init/1\r\n    pid: <0.332.0>\r\n    registered_name: []\r\n    exception exit: {{{function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]},{gen_server2,call,[<0.336.0>,out,infinity]}},[{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,589}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}\r\n    ancestors: [<0.324.0>,<0.323.0>,rabbit_vhost_sup_sup,rabbit_sup,<0.262.0>]\r\n    message_queue_len: 1\r\n    messages: [{'EXIT',<0.336.0>,{function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]}}]\r\n    links: [<0.335.0>,<0.324.0>]\r\n    dictionary: []\r\n    trap_exit: true\r\n    status: running\r\n    heap_size: 2586\r\n    stack_size: 27\r\n    reductions: 57377\r\n  neighbours:\r\n    neighbour: [{pid,<0.335.0>},{registered_name,[]},{initial_call,{rabbit_msg_store_gc,init,['Argument__1']}},{current_function,{gen_server2,process_next_msg,1}},{ancestors,[<0.332.0>,<0.324.0>,<0.323.0>,rabbit_vhost_sup_sup,rabbit_sup,<0.262.0>]},{message_queue_len,0},{links,[<0.332.0>]},{trap_exit,false},{status,waiting},{heap_size,987},{stack_size,8},{reductions,174},{current_stacktrace,[{gen_server2,process_next_msg,1,[{file,\"src/gen_server2.erl\"},{line,666}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}]\r\n2018-09-11 09:39:42 =SUPERVISOR REPORT====\r\n     Supervisor: {local,worker_pool_sup}\r\n     Context:    child_terminated\r\n     Reason:     {function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]}\r\n     Offender:   [{pid,<0.281.0>},{id,4},{mfargs,{worker_pool_worker,start_link,[worker_pool]}},{restart_type,transient},{shutdown,4294967295},{child_type,worker}]\r\n\r\n2018-09-11 09:39:42 =CRASH REPORT====\r\n  crasher:\r\ninitial call: rabbit_vhost_process:init/1\r\npid: <0.325.0>\r\nregistered_name: []\r\nexception exit: {{error,{{{function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]},{gen_server2,call,[<0.336.0>,out,infinity]}},{child,undefined,msg_store_persistent,{rabbit_msg_store,start_link,[msg_store_persistent,\"c:/Users/dfpsb/AppData/Roaming/RabbitMQ/db/RABBIT~1/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L\",[],{#Fun<rabbit_queue_index.2.122888644>,{start,[{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Test1.InterchangeFragtbrevEnvelope.RET\">>},{resource,<<\"/\">>,queue,<<\"Test2.DfLoggingEvent.Debug\">>},{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Paw.DfLoggingEvent.Debug\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TruckLoadingEnvelope.UnitTest\">>},{resource,<<\"/\">>,queue,<<\"Test1.InterchangeFragtbrevEnvelope.RET_error\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TestMsg.UnitTest_error\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TestMsg.UnitTest\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeFragtbrevEnvelope.TurPlan_error\">>},{resource,<<\"/\">>,queue,<<\"Paw.TruckLoadingEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeFragtbrevEnvelope.TurPlan_error\">>},{resource,<<\"/\">>,queue,<<\"Test2.TruckLoadingEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Paw.DfLoggingEvent.Warning\">>},{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeFragtbrevEnvelope.RET\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Test2.DfLoggingEvent.Warning\">>}]}}]},transient,30000,worker,[rabbit_msg_store]}}},[{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,581}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}\r\nancestors: [<0.323.0>,rabbit_vhost_sup_sup,rabbit_sup,<0.262.0>]\r\nmessage_queue_len: 0\r\nmessages: []\r\nlinks: [<0.323.0>]\r\ndictionary: []\r\ntrap_exit: true\r\nstatus: running\r\nheap_size: 10958\r\nstack_size: 27\r\nreductions: 63314\r\n  neighbours:\r\n2018-09-11 09:39:42 =SUPERVISOR REPORT====\r\n Supervisor: {<0.323.0>,rabbit_vhost_sup_wrapper}\r\n Context:    start_error\r\n Reason:     {error,{{{function_clause,[{rabbit_queue_index,journal_minus_segment1,[{{true,<<172,190,166,92,192,205,125,125,36,223,114,188,53,139,128,108,0,0,0,0,0,0,0,0,0,0,26,151>>,<<>>},no_del,no_ack},{{true,<<89,173,78,227,188,37,119,171,231,189,220,236,244,79,138,177,0,0,0,0,0,0,0,0,0,0,23,40>>,<<>>},no_del,no_ack}],[{file,\"src/rabbit_queue_index.erl\"},{line,1231}]},{rabbit_queue_index,'-journal_minus_segment/3-fun-0-',4,[{file,\"src/rabbit_queue_index.erl\"},{line,1208}]},{array,sparse_foldl_3,7,[{file,\"array.erl\"},{line,1684}]},{array,sparse_foldl_2,9,[{file,\"array.erl\"},{line,1678}]},{rabbit_queue_index,'-recover_journal/1-fun-0-',1,[{file,\"src/rabbit_queue_index.erl\"},{line,915}]},{lists,map,2,[{file,\"lists.erl\"},{line,1239}]},{rabbit_queue_index,segment_map,2,[{file,\"src/rabbit_queue_index.erl\"},{line,1039}]},{rabbit_queue_index,recover_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,906}]}]},{gen_server2,call,[<0.336.0>,out,infinity]}},{child,undefined,msg_store_persistent,{rabbit_msg_store,start_link,[msg_store_persistent,\"c:/Users/dfpsb/AppData/Roaming/RabbitMQ/db/RABBIT~1/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L\",[],{#Fun<rabbit_queue_index.2.122888644>,{start,[{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Test1.InterchangeFragtbrevEnvelope.RET\">>},{resource,<<\"/\">>,queue,<<\"Test2.DfLoggingEvent.Debug\">>},{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Paw.DfLoggingEvent.Debug\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TruckLoadingEnvelope.UnitTest\">>},{resource,<<\"/\">>,queue,<<\"Test1.InterchangeFragtbrevEnvelope.RET_error\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TestMsg.UnitTest_error\">>},{resource,<<\"/\">>,queue,<<\"DevUnitTest.TestMsg.UnitTest\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeFragtbrevEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Paw.InterchangeFragtbrevEnvelope.TurPlan_error\">>},{resource,<<\"/\">>,queue,<<\"Paw.TruckLoadingEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeFragtbrevEnvelope.TurPlan_error\">>},{resource,<<\"/\">>,queue,<<\"Test2.TruckLoadingEnvelope.TurPlan\">>},{resource,<<\"/\">>,queue,<<\"Paw.DfLoggingEvent.Warning\">>},{resource,<<\"/\">>,queue,<<\"DF-9ID59RK-WS.InterchangeFragtbrevEnvelope.RET\">>},{resource,<<\"/\">>,queue,<<\"Test2.InterchangeTurEnvelope.DFMobil\">>},{resource,<<\"/\">>,queue,<<\"Test2.DfLoggingEvent.Warning\">>}]}}]},transient,30000,worker,[rabbit_msg_store]}}}\r\n Offender:   [{pid,undefined},{id,rabbit_vhost_process},{mfargs,{rabbit_vhost_process,start_link,[<<\"/\">>]}},{restart_type,permanent},{shutdown,30000},{child_type,worker}]",
    "answer": "I figured out what was going on. Someone on my team (me) was creating an unprecedented amount of connections to RabbitMQ. For each connection a file handler is created to one or more files, not sure which. The OS (Windows in my case) has a file handler limit, not sure what the limit is, but when reached, an error is thrown. This corrupted the virtual host (/) and I had to delete it and create it again. Good thing this wasn't production, because then all the messages would've been gone.\nEdit (June 2020):\nThere might be a fix for this so it can't happen again. A virtual host can be limited in how many connections they allow. This way Windows won't faulter on the many file handles. Look in the management portal -> Admin -> Limits, set max-connections to a number you think it plausible for your setup."
  },
  {
    "question": "We are working with celery at the last year, with ~15 workers, each one defined with concurrency between 1-4.\nRecently we upgraded our celery from v3.1 to v4.1\nNow we are having the following errors in each one of the workers logs, any ideas what can cause to such error?\n2017-08-21 18:33:19,780 94794  ERROR   Control command error: error(104, 'Connection reset by peer') [file: pidbox.py, line: 46]\nTraceback (most recent call last):\n  File \"/srv/dy/venv/lib/python2.7/site-packages/celery/worker/pidbox.py\", line 42, in on_message\n    self.node.handle_message(body, message)\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 129, in handle_message\n    return self.dispatch(**body)\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 112, in dispatch\n    ticket=ticket)\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 135, in reply\n    serializer=self.mailbox.serializer)\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/pidbox.py\", line 265, in _publish_reply\n    **opts\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/messaging.py\", line 181, in publish\n    exchange_name, declare,\n  File \"/srv/dy/venv/lib/python2.7/site-packages/kombu/messaging.py\", line 203, in _publish\n    mandatory=mandatory, immediate=immediate,\n  File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/channel.py\", line 1748, in _basic_publish\n    (0, exchange, routing_key, mandatory, immediate), msg\n  File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 64, in send_method\n    conn.frame_writer(1, self.channel_id, sig, args, content)\n  File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/method_framing.py\", line 178, in write_frame\n    write(view[:offset])\n  File \"/srv/dy/venv/lib/python2.7/site-packages/amqp/transport.py\", line 272, in write\n    self._write(s)\n  File \"/usr/lib64/python2.7/socket.py\", line 224, in meth\n    return getattr(self._sock,name)(*args)\nerror: [Errno 104] Connection reset by peer\n\nBTW: our tasks in the form:\n@app.task(name='EXAMPLE_TASK'],\n          bind=True,\n          base=ConnectionHolderTask)\ndef example_task(self, arg1, arg2, **kwargs):\n    # task code",
    "answer": "We are also having massive issues with celery... I spend 20% of my time just dancing around weird idle-hang/crash issues with our workers sigh \nWe had a similar case that was caused by a high concurrency combined with a high worker_prefetch_multiplier, as it turns out fetching thousands of tasks is a good way to frack the connection.\nIf that's not the case: try to disable the broker pool by setting   broker_pool_limit to None.\nJust some quick ideas that might (hopefully) help :-)"
  },
  {
    "question": "We're using TeamCity's command line build runner to call a bat-file. The bat-file builds our solution by calling the Visual Studio 2008's \"devenv.exe\" and then it executes the unit tests and creates the correct folder structure. \nWhat we would like to do is to stop executing the bat-file if the call to devenv fails and make the TeamCity to realize that the build failed. We can catch the failed devenv call by checking the ErrorLevel (which is 1 if the build failed) and we can exit our bat-file at that point. But how can we tell to the TeamCity that the build failed?\nThis is what we've tried:\ncall \"build.bat\"\nIF ERRORLEVEL 1 EXIT /B 1\n\nBut TeamCity doesn't recognize our exit code. Instead the build log looks like this:\n[08:52:12]: ========== Build: 28 succeeded or up-to-date, 1 failed, 0 skipped ==========\n[08:52:13]: C:\\_work\\BuildAgent\\work\\bcd14331c8d63b39\\Build>IF ERRORLEVEL 1 EXIT /B 1 \n[08:52:13]: Process exited with code 0\n[08:52:13]: Publishing artifacts\n[08:52:13]: [Publishing artifacts] Paths to publish: [build/install, teamcity-info.xml]\n[08:52:13]: [Publishing artifacts] Artifacts path build/install not found\n[08:52:13]: [Publishing artifacts] Publishing files\n[08:52:13]: Build finished\n\nSo TeamCity will report that the build was successful. How can we fix this?\nSolution:\nTeamCity provides a mechanism called Service Messages which can be used to handle situations like this. \nI've updated my build script to look like the following:\nIF %ERRORLEVEL% == 0 GOTO OK\necho ##teamcity[buildStatus status='FAILURE' text='{build.status.text} in compilation']\nEXIT /B 1\n:OK\n\nAs a result TeamCity will report my build as failed because of a \"Failure in compilation\".",
    "answer": "See Build Script Interaction with TeamCity topic.\n\nYou can report messages for build log in the following way:\n##teamcity[message text='<message text>' errorDetails='<error details>' status='<status value>']\nwhere:\n\nThe status attribute may take following values: NORMAL, WARNING,\n  FAILURE, ERROR. The default value is NORMAL.\nThe errorDetails\n  attribute is used only if status is ERROR, in other cases it is\n  ignored.\n\nThis message fails the build in case its status is ERROR and\n  \"Fail build if an error message is logged by build runner\" checkbox is\n  checked on build configuration general settings page. For example:\n##teamcity[message text='Exception text' errorDetails='stack trace' status='ERROR']\n\nUpdate 2013-08-30:\nAs of TeamCity 7.1 build failures should be reported using the buildProblem service message instead:\n##teamcity[buildProblem description='<description>' identity='<identity>']"
  },
  {
    "question": "I have the following configuration as .gitlab-ci.yml\nbut I found out after successfully pass build stage (which\nwould create a virtualenv called venv), it seems that \nin test stage you would get a brand new environment(there's\nno venv directory at all). So I wonder should I put setup\nscript in before_script therefor it would run in each phase(build/test/deploy). Is it a right way to do it ?\nbefore_script:\n  - uname -r \n\ntypes:\n  - build\n  - test\n  - deploy\n\njob_install:\n  type: build\n  script:\n    - apt-get update\n    - apt-get install -y libncurses5-dev\n    - apt-get install -y libxml2-dev libxslt1-dev\n    - apt-get install -y python-dev libffi-dev libssl-dev \n    - apt-get install -y python-virtualenv\n    - apt-get install -y python-pip\n    - virtualenv --no-site-packages venv\n    - source venv/bin/activate\n    - pip install -q -r requirements.txt\n    - ls -al\n  only:\n    - master\n\njob_test:\n  type: test\n  script:\n    - ls -al\n    - source venv/bin/activate\n    - cp crawler/settings.sample.py crawler/settings.py\n    - cd crawler \n    - py.test -s -v \n  only:\n    - master\n\nadasd",
    "answer": "Gitlab CI jobs supposed to be independent, because they could run on different runners. It is not issue. There two ways to pass files between stages:\n\nThe right way. Using artefacts.\nThe wrong way. Using cache. With cache key \"hack\". Still need same runner.\n\nSo yes, supposed by gitlab way to have everything your job depends on in before script.\nArtifacts example:\n  artifacts:\n   when: on_success\n   expire_in: 1 mos\n   paths:\n    - some_project_files/\n\nCache example:\ncache:\n  key: \"$CI_BUILD_REF_NAME\"\n  untracked: true\n  paths:\n   - node_modules/\n   - src/bower_components/\n\nFor correct running environment i suggest using docker with image containing apt-get dependencies. And use artefacts for passing job results between jobs. Note that artefact also uploaded to gitlab web interface and being able to download them. So if they are quite heavy use small expire_in time, for removing them after all jobs done."
  },
  {
    "question": "I'm in the process of evaluating if and how a CF .NET enterprise application can be ported to run on Android devices. The application on Windows Mobile phones are run in kiosk mode where the application autostart in fullscreen-mode after booting and with the users unable to accidentally or willingly access any other parts of the phone. \nIs it possible on Android to have only one application autostart after booting and prevent users from accidentally (or willingly) access any other parts of the Android device?",
    "answer": "You can autostart applications on boot by listening to the android.intent.action.BOOT_COMPLETED intent in a BroadcastReceiver and start your Activity from there. In the Activity you can register yourself as the new default homescreen[1] and handle the keys. \nI think there are some instances that you can't handle without modifying the framework (like longpress on Home to show currently active Applications) - I could also be mistaken though.\nBut for a prototype that could be sufficient.\nHave fun tinkering!\n[1]:\n<intent-filter>\n <action android:name=\"android.intent.action.MAIN\" />\n <category android:name=\"android.intent.category.HOME\" />\n <category android:name=\"android.intent.category.DEFAULT\" />\n</intent-filter>"
  },
  {
    "question": "I wish to set up what is usually called a Kiosk, running Firefox locked down to our own specific home page (and links from there). The base operating system is CentOs 5 (i.e. just like RedHat Enterprise 5).\nIdeally I want Firefox to start full screen (and I have installed the full-fullscreen addon to help with this), and to be locked as such (i.e. F11 does not work).\nI need to be able to install this system using one or more rpm files.\nI have tested my fullscreen Firefox setup rpm under Gnome, and it works fine - my Gnome desktop is 1024x768, and the selected home page comes up exactly filling the screen - looks great.\nHowever, I do not want to bother with a desktop environment (like Gnome or KDE), just run Firefox as the sole X client program, with a fixed screen size of 1024x768.\nI have built rpms to install X, configure it to run at 1024x768, and fire up X automatically from an autologin using shell scripts.\nMy main autologon script contains this:\nstartx ~/client/xClient.sh  -- :1 &\n\nxClient.sh contains this:\nwhile [ true ]\ndo\n    firefox\ndone\n\nMy problem is that Firefox does not come up full screen under this setup. The firefox window is smaller than the screen, and the top left corner is off the screen - this means the web page gets scrollbars, the top and left of the page does not show, and there is a black area along the bottom and right.\nDoes anyone know the reason for this behaviour?\nWhat solutions can you suggest?\nI suppose, if necessary, I could install Gnome on the machine, and then try to lock it down - but it seems silly to add something as complex as Gnome just to get the window to appear the right size, and in the right place! Plus there is the extra task of trying to lock Gnome down so the users can't do anything else with the machine.\nIf you think this question should not be on Stack Overflow, please tell me where it should go. (I think writing rpm and shell scripts are programming, but maybe they don't count? If not, sorry!)",
    "answer": "You have 2 options. \nYou install a kiosk plug-in, that allows you to start firefox automatically in full screen mode (amongst other things). One example would be R-kiosk\nOr you skip firefox and create a xul application that does what you want. You can find a sample application here. And you can find full screen code (not tested) here."
  },
  {
    "question": "When I run foreman I get the following:\n > foreman start\n 16:47:56 web.1     | started with pid 27122\n\nOnly if I stop it (via ctrl-c) it shows me what is missing:\n^CSIGINT received\n16:49:26 system    | sending SIGTERM to all processes\n16:49:26 web.1     | => Booting Thin\n16:49:26 web.1     | => Rails 3.0.0 application starting in development on http://0.0.0.0:5000\n16:49:26 web.1     | => Call with -d to detach\n16:49:26 web.1     | => Ctrl-C to shutdown server\n16:49:26 web.1     | >> Thin web server (v1.3.1 codename Triple Espresso)\n16:49:26 web.1     | >> Maximum connections set to 1024\n16:49:26 web.1     | >> Listening on 0.0.0.0:5000, CTRL+C to stop\n16:49:26 web.1     | >> Stopping ...\n16:49:26 web.1     | Exiting\n16:49:26 web.1     | >> Stopping ...\n\nHow do I fix it?",
    "answer": "I’ve been able to resolve this issue by 2 different ways:\n\nFrom https://github.com/ddollar/foreman/wiki/Missing-Output:\n\nIf you are not seeing any output from your program, there is a likely\n  chance that it is buffering stdout. Ruby buffers stdout by default. To\n  disable this behavior, add this code as early as possible in your\n  program:\n\n# ruby\n$stdout.sync = true\n\nBy installing foreman via the heroku toolbelt package\n\nBut I still don’t know what’s happening nor why this 2 ways above resolved the issue…"
  },
  {
    "question": "I want to be able to set environment variables in my Django app for tests to be able to run. For instance, my views rely on several API keys.\nThere are ways to override settings during testing, but I don't want them defined in settings.py as that is a security issue. \nI've tried in my setup function to set these environment variables, but that doesn't work to give the Django application the values.\nclass MyTests(TestCase):\n    def setUp(self):\n        os.environ['TEST'] = '123'  # doesn't propogate to app\n\nWhen I test locally, I simply have an .env file I run with \nforeman start -e .env web\n\nwhich supplies os.environ with values. But in Django's unittest.TestCase it does not have a way (that I know) to set that. \nHow can I get around this?",
    "answer": "The test.support.EnvironmentVarGuard is an internal API that might be changed from version to version with breaking (backward incompatible) changes. In fact, the entire test package is internal use only. It was explicitly stated on the test package documentation page that it's for internal testing of core libraries and NOT a public API. (see links below)\nYou should use patch.dict() in python's standard lib unittest.mock. It can be used as a context manager, decorator or class decorator.  See example code below copied from the official Python documentation.\nimport os\nfrom unittest.mock import patch\nwith patch.dict('os.environ', {'newkey': 'newvalue'}):\n    print(os.environ['newkey'])  # should print out 'newvalue'\n    assert 'newkey' in os.environ  # should be True\nassert 'newkey' not in os.environ  # should be True\n\nUpdate: for those who doesn't read the documentation thoroughly and might have missed the note, read more test package notes at\nhttps://docs.python.org/2/library/test.html or \nhttps://docs.python.org/3/library/test.html"
  },
  {
    "question": "I have this simple Procfile\nweb: myapp\n\nmyapp is in the path, but the processes home directory should be ./directory/. How can I specify in the Procfile where the process is to be started?\nhttps://github.com/ddollar/foreman/pull/101 doesn't help because it assumes, that this working directory should be the same for every process specified by the Procfile",
    "answer": "The shell is the answer. It's as simple as\nweb: sh -c 'cd ./directory/ && exec appname'"
  },
  {
    "question": "I installed redis this afternoon and it caused a few errors, so I uninstalled it but this error is persisting when I launch the app with foreman start. Any ideas on a fix? \nforeman start\n22:46:26 web.1  | started with pid 1727\n22:46:26 web.1  | 2013-05-25 22:46:26 [1727] [INFO] Starting gunicorn 0.17.4\n22:46:26 web.1  | 2013-05-25 22:46:26 [1727] [ERROR] Connection in use: ('0.0.0.0', 5000)",
    "answer": "Just type\nsudo fuser -k 5000/tcp\n\n.This will kill all process associated with port 5000"
  },
  {
    "question": "A web app I am writing in JavaScript using node.js. I use Foreman, but I don't want to manually restart the server every time I change my code. Can I tell Foreman to reload the entire web app before handling an HTTP request (i.e. restart the node process)?",
    "answer": "Here's an adjusted version of Pendlepants solution. Foreman looks for an .env file to read environment variables. Rather than adding a wrapper, you can just have Foreman switch what command it uses to start things up:\nIn .env:\nWEB=node app.js\n\nIn dev.env:\nWEB=supervisor app.js\n\nIn your Procfile:\nweb: $WEB\n\nBy default, Foreman will read from .env (in Production), but in DEV just run this:\nforeman start -e dev.env"
  },
  {
    "question": "I'm following the heroku tutorial for Heroku/Facebook integration (but I suspect this issue has nothing to do with facebook integration) and I got stuck on the stage where I was supposed to start foreman (I've installed the Heroku installbelt for windows, which includes foreman):\n> foreman start\n\ngives:\nC:/RailsInstaller/Ruby1.8.7/lib/ruby/site_ruby/1.8/rubygems/dependency.rb:247:in `to_specs': Could not find foreman (>= 0) amongst [POpen4-0.1.4, Platform-0.4.0, ZenTest-4.6.2, abstract-1.0.0, actionm\nailer-3.0.11, actionmailer-3.0.9, actionpack-3.0.11, actionpack-3.0.9, activemodel-3.0.11, activemodel-3.0.9, activerecord-3.0.11, activerecord-3.0.9, activerecord-sqlserver-adapter-3.0.15, activereso\nurce-3.0.11, activeresource-3.0.9, activesupport-3.0.11, activesupport-3.0.9, addressable-2.2.6, annotate-2.4.0, arel-2.0.10, autotest-4.4.6, autotest-growl-0.2.16, autotest-rails-pure-4.1.2, autotest\n-standalone-4.5.8, builder-2.1.2, bundler-1.0.15, diff-lcs-1.1.3, erubis-2.6.6, factory_girl-1.3.3, factory_girl_rails-1.0, faker-0.3.1, gravatar_image_tag-1.0.0.pre2, heroku-2.14.0, i18n-0.5.0, json-\n1.6.1, launchy-2.0.5, mail-2.2.19, mime-types-1.17.2, mime-types-1.16, nokogiri-1.5.0-x86-mingw32, open4-1.1.0, pg-0.11.0-x86-mingw32, polyglot-0.3.3, polyglot-0.3.1, rack-1.2.4, rack-1.2.3, rack-moun\nt-0.6.14, rack-test-0.5.7, rails-3.0.11, rails-3.0.9, railties-3.0.11, railties-3.0.9, rake-0.9.2.2, rake-0.8.7, rb-readline-0.4.0, rdoc-3.11, rdoc-3.8, rest-client-1.6.7, rspec-2.6.0, rspec-core-2.6.\n4, rspec-expectations-2.6.0, rspec-mocks-2.6.0, rspec-rails-2.6.1, rubygems-update-1.8.11, rubyzip-0.9.4, rubyzip2-2.0.1, spork-0.9.0.rc8-x86-mingw32, sqlite3-1.3.3-x86-mingw32, sqlite3-ruby-1.3.3, te\nrm-ansicolor-1.0.7, thor-0.14.6, tiny_tds-0.4.5-x86-mingw32, treetop-1.4.10, treetop-1.4.9, tzinfo-0.3.31, tzinfo-0.3.29, webrat-0.7.1, will_paginate-3.0.pre2, win32-api-1.4.8-x86-mingw32, win32-open3\n-0.3.2-x86-mingw32, win32-process-0.6.5, windows-api-0.4.0, windows-pr-1.2.1, zip-2.0.2] (Gem::LoadError)\n        from C:/RailsInstaller/Ruby1.8.7/lib/ruby/site_ruby/1.8/rubygems/dependency.rb:256:in `to_spec'\n        from C:/RailsInstaller/Ruby1.8.7/lib/ruby/site_ruby/1.8/rubygems.rb:1210:in `gem'\n        from C:/Program Files (x86)/ruby-1.9.3/bin/foreman:18\n\nSince I'm a complete noob in this I'm not sure if my question here is a duplicate for Error on 'foreman start' while following the Python/Flask Heroku tutorial (because it's not quite the same error). If so, does anyone have a method for deploying a development environment on windows (for Heruko, Python, Facebook app)? Or should I use Ubuntu for this?\nThanks",
    "answer": "Although this question doesn't seem to be of interest to anyone here (5 views in ~2 hours, 0 answers, 0 comments...), I have found the solution and ready to share it with anyone that will encounter it:\n\nInstall the latest ruby from rubyinstaller.org (1.9.3-p194) - Sometimes there is a collision installs of the same version, in my case I've just uninstalled all versions of ruby, but if you already have other application that needs older version then you have to be more careful\nCheck that your system is default to use this version by invoking ruby -v in command line prompt: and getting ruby 1.9.3p194 (2012-04-20) [i386-mingw32] (you may have to close and re-open cmd, to include the new environment variables)\nStill in cmd, invoke:\ngem install foreman\ngem install taps\n\nnow go to your Procfile app (e.g. your heroku example app from the tutorial) and execute foreman start, you should see something like this:\n18:23:52 web.1  | started with pid 7212\n18:23:54 web.1  |  * Running on http://0.0.0.0:5000/\n18:23:54 web.1  |  * Restarting with reloader"
  },
  {
    "question": "I simply followed the getting started with nodejs tutorial from Heroku.\nhttps://devcenter.heroku.com/articles/getting-started-with-nodejs#declare-process-types-with-procfile\nBut I get an error at the part \"declare process types with procfile\"\nMy problem is that my cmd (using windows 7) didn't find the command \"foreman\"\nAny solutions ?\nI downloaded/installed the heroku toolbelt, the login works fine, but foreman dont",
    "answer": "I had the same problem on Windows7 64-bit, using git's bash.  Here's what I did:\n\nuninstall the toolbelt, Ruby, and Git using Control Panel's \"Program and Features\"\nreinstall the toolbelt to C:\\Heroku (see known issue for more info)\nadd C:\\Program Files (x86)\\git\\bin;C:\\Heroku\\ruby-1.9.2\\bin to the system PATH variable: Control Panel, System, Advanced system settings, Environment Variables..., System variables, Variable Path, Edit...  (Change ruby-1.9.2 if a future version of the toolbelt includes a newer version of Ruby.)\nopen a git bash window and uninstall foreman version 0.63$ gem uninstall foreman\nthen install version 0.61 (see here for more info)$ gem install foreman -v 0.61\n\nNow foreman worked for me:\n$ foreman start"
  },
  {
    "question": "I am trying to use foreman to start my rails app. Unfortunately I have difficulties connecting my IDE for debugging. \nI read here about using\nDebugger.wait_connection = true\nDebugger.start_remote\n\nto start a remote debugging session, but that does not really work out.\nQuestion:\nIs there a way to debug a rails (3.2) app started by foreman? If so, what is the approach?",
    "answer": "If you use several workers with full rails environment you could use the following initializer:\n# Enabled debugger with foreman, see https://github.com/ddollar/foreman/issues/58\nif Rails.env.development?\n  require 'debugger'\n  Debugger.wait_connection = true\n\n  def find_available_port\n    server = TCPServer.new(nil, 0)\n    server.addr[1]\n  ensure\n    server.close if server\n  end\n\n  port = find_available_port\n  puts \"Remote debugger on port #{port}\"\n  Debugger.start_remote(nil, port)\nend\n\nAnd in the foreman's logs you'll be able to find debugger's ports:\n$ foreman start\n12:48:42 web.1     | started with pid 29916\n12:48:42 worker.1  | started with pid 29921\n12:48:44 web.1     | I, [2012-10-30T12:48:44.810464 #29916]  INFO -- : listening on addr=0.0.0.0:5000 fd=10\n12:48:44 web.1     | I, [2012-10-30T12:48:44.810636 #29916]  INFO -- : Refreshing Gem list\n12:48:47 web.1     | Remote debugger on port 59269\n12:48:48 worker.1  | Remote debugger on port 41301\n\nNow run debugger using:\nrdebug -c -p [PORT]"
  },
  {
    "question": "I am trying to deploy an Heroku app.\nI must be doing something wrong with the Procfile. When I run foreman check I get this error.\n\nERROR: no processes defined\n\nI get pretty much the same thing when deploying on Heroku\n-----> Building runtime environment\n-----> Discovering process types\n\n !     Push failed: cannot parse Procfile.\n\nThe Procfile looks like this\n\nweb: node app.js\n\nWhat did I miss?\n\nupdate I re-did all from the start, It works properly now. I think I might have issue with Unix line ending",
    "answer": "Just encounter \"Push failed: cannot parse Procfile.\" on Windows.\nI can conclude that \nIt IS \"Windows-file format\" problem, NOT the context of file itself. \nmake sure to create a clean file, maybe use Notepad++ or other advanced editor to check the file type."
  },
  {
    "question": "We have rails app that is running some foreman processes with bundle exec foreman start, and have googled a lot of different things, and found that the common suggestion is to set up another background process handler, and export the processes there.  So essentially let someone else do foreman's job of managing the processes.\nMy question is how do you simply stop or restart foreman processes, as I don't really want to try to export the processes to another manager.  \nShouldn't there be a simple: foreman restart\nSince there is a: foreman start\nIs there a snippet or some other command that anyone has used to restart these processes?\nAny help or explanation of the foreman tool would be appreciated.",
    "answer": "Used monit to control stop and start of foreman processes."
  },
  {
    "question": "I am having trouble getting my dynos to run multiple delayed job worker processes.\nMy Procfile looks like this:\nworker: bundle exec script/delayed_job -n 3 start\n\nand my delayed_job script is the default provided by the gem:\n#!/usr/bin/env ruby\n\nrequire File.expand_path(File.join(File.dirname(__FILE__), '..', 'config', 'environment'))\nrequire 'delayed/command'\nDelayed::Command.new(ARGV).daemonize\n\nWhen I try to run this either locally or on a Heroku dyno it exits silently and I can't tell what is going on.\nforeman start\n16:09:09 worker.1 | started with pid 75417\n16:09:15 worker.1 | exited with code 0\n16:09:15 system   | sending SIGTERM to all processes\nSIGTERM received\n\nAny help with either how to debug the issue or suggestions about other ways to go about running multiple workers on a single dyno it would be greatly appreciated.",
    "answer": "You can use foreman to start multiple processes on the same dyno.\nFirst, add foreman to your Gemfile.  \nThen add a worker line to your Procfile:\nworker: bundle exec foreman start -f Procfile.workers\n\nCreate a new file called Procfile.workers which contains:\ndj_worker: bundle exec rake jobs:work\ndj_worker: bundle exec rake jobs:work\ndj_worker: bundle exec rake jobs:work\n\nThat will start 3 delayed_job workers on your worker dyno."
  },
  {
    "question": "binding.pry not works(console input not available) if i start the server with bin/dev command. It only works with bin/rails s command.\nI understand it has something to do with foreman and Procfile.dev, but I don't know how.\nIs this a bug or is it supposed to be like this?",
    "answer": "With bin/dev, the Procfile.dev file is run with foreman. The pry issue is caused by the CSS and JS watchers:these just listen to changes in your CSS and JS files.\nWhat you can do is remove the web: unset PORT && bin/rails server command from your Procfile, so it will only have the CSS and JS watchers and look like this:\njs: yarn build --watch\ncss: yarn build:css --watch\n\nNow you'll have to open two terminals, one with bin/rails s and the other with foreman start -f Procfile.dev. This way your pry works in the server terminal as normal and the watchers are watching as normal."
  },
  {
    "question": "I have the following Rake task:\nnamespace :foreman do\n  task :dev do\n    `foreman start -f Procfile.dev`\n  end\nend\n\ndesc \"Run Foreman using Procfile.dev\"\ntask :foreman => 'foreman:dev'\n\nThe forman command works fine from the shell, however when I run rake foreman I get the following error:\n/Users/me/.gem/ruby/2.0.0/gems/bundler-1.5.2/lib/bundler/rubygems_integration.rb:240:in `block in replace_gem': foreman is not part of the bundle. Add it to Gemfile. (Gem::LoadError)\n    from /Users/me/.gem/ruby/2.0.0/bin/foreman:22:in `<main>'\n\nForman specifically states: \nRuby users should take care not to install foreman in their project's Gemfile\n\nSo how can I get this task to run?",
    "answer": "If you must make it work via rake, try changing the shell-out via backtick to use a hard-coded path to the system-wide foreman binary\n`/global/path/to/foreman start -f Procfile.dev`\n\nYou just need to use 'which' or 'locate' or a similar tool to determine the path that works outside your bundler context. If you are using rbenv, then this might be sufficient :\n$ rbenv which rake\n/home/name/.rbenv/versions/1.9.3-p448/bin/rake\n\nI hope that helps you move forward."
  },
  {
    "question": "we are trying to install couple of python packages without internet.\nFor ex : python-keystoneclient\n\nFor that we have the packages downloaded from https://pypi.python.org/pypi/python-keystoneclient/1.7.1 and kept it in server.\nHowever, while installing tar.gz and .whl packages , the installation is looking for dependent packages to be installed first. Since there is no internet connection in the server, it is getting failed. \nFor ex : For python-keystoneclient we have the following dependent packages\nstevedore (>=1.5.0)\nsix (>=1.9.0)\nrequests (>=2.5.2)\nPrettyTable (<0.8,>=0.7)\noslo.utils (>=2.0.0)\noslo.serialization (>=1.4.0)\noslo.i18n (>=1.5.0)\noslo.config (>=2.3.0)\nnetaddr (!=0.7.16,>=0.7.12)\ndebtcollector (>=0.3.0)\niso8601 (>=0.1.9)\nBabel (>=1.3)\nargparse\npbr (<2.0,>=1.6)\n\nWhen i try to install packages one by one from the above list, once again its looking for nested dependency . \nIs there any way we could list ALL the dependent packages for installing a python module like python-keystoneclient.",
    "answer": "This is how I handle this case:\nOn the machine where I have access to Internet:\nmkdir keystone-deps\npip download python-keystoneclient -d \"/home/aviuser/keystone-deps\"\ntar cvfz keystone-deps.tgz keystone-deps\n\nThen move the tar file to the destination machine that does not have Internet access and perform the following:\ntar xvfz keystone-deps.tgz\ncd keystone-deps\npip install python_keystoneclient-2.3.1-py2.py3-none-any.whl -f ./ --no-index\n\nYou may need to add --no-deps to the command as follows:\npip install python_keystoneclient-2.3.1-py2.py3-none-any.whl -f ./ --no-index --no-deps"
  },
  {
    "question": "What is the most efficient way to capture frames from a MTKView? If possible, I would like to save a .mov file from the frames in realtime. Is it possible to render into an AVPlayer frame or something?\nIt is currently drawing with this code (based on @warrenm PerformanceShaders project):\nfunc draw(in view: MTKView) {\n     _ = inflightSemaphore.wait(timeout: DispatchTime.distantFuture)\n             updateBuffers()\n\n    let commandBuffer = commandQueue.makeCommandBuffer()\n\n    commandBuffer.addCompletedHandler{ [weak self] commandBuffer in\n        if let strongSelf = self {\n            strongSelf.inflightSemaphore.signal()\n        }\n    }\n   // Dispatch the current kernel to perform the selected image filter\n    selectedKernel.encode(commandBuffer: commandBuffer,\n        sourceTexture: kernelSourceTexture!,\n        destinationTexture: kernelDestTexture!)\n\n    if let renderPassDescriptor = view.currentRenderPassDescriptor, let currentDrawable = view.currentDrawable\n    {\n        let clearColor = MTLClearColor(red: 0, green: 0, blue: 0, alpha: 1)\n        renderPassDescriptor.colorAttachments[0].clearColor = clearColor\n\n        let renderEncoder = commandBuffer.makeRenderCommandEncoder(descriptor: renderPassDescriptor)\n        renderEncoder.label = \"Main pass\"\n\n        renderEncoder.pushDebugGroup(\"Draw textured square\")\n        renderEncoder.setFrontFacing(.counterClockwise)\n        renderEncoder.setCullMode(.back)\n\n        renderEncoder.setRenderPipelineState(pipelineState)\n        renderEncoder.setVertexBuffer(vertexBuffer, offset: MBEVertexDataSize * bufferIndex, at: 0)\n        renderEncoder.setVertexBuffer(uniformBuffer, offset: MBEUniformDataSize * bufferIndex , at: 1)\n        renderEncoder.setFragmentTexture(kernelDestTexture, at: 0)\n        renderEncoder.setFragmentSamplerState(sampler, at: 0)\n        renderEncoder.drawPrimitives(type: .triangleStrip, vertexStart: 0, vertexCount: 4)\n\n        renderEncoder.popDebugGroup()\n        renderEncoder.endEncoding()\n\n        commandBuffer.present(currentDrawable)\n    }\n\n    bufferIndex = (bufferIndex + 1) % MBEMaxInflightBuffers\n\n    commandBuffer.commit()\n }",
    "answer": "Here's a small class that performs the essential functions of writing out a movie file that captures the contents of a Metal view:\nclass MetalVideoRecorder {\n    var isRecording = false\n    var recordingStartTime = TimeInterval(0)\n\n    private var assetWriter: AVAssetWriter\n    private var assetWriterVideoInput: AVAssetWriterInput\n    private var assetWriterPixelBufferInput: AVAssetWriterInputPixelBufferAdaptor\n\n    init?(outputURL url: URL, size: CGSize) {\n        do {\n            assetWriter = try AVAssetWriter(outputURL: url, fileType: .m4v)\n        } catch {\n            return nil\n        }\n\n        let outputSettings: [String: Any] = [ AVVideoCodecKey : AVVideoCodecType.h264,\n            AVVideoWidthKey : size.width,\n            AVVideoHeightKey : size.height ]\n\n        assetWriterVideoInput = AVAssetWriterInput(mediaType: .video, outputSettings: outputSettings)\n        assetWriterVideoInput.expectsMediaDataInRealTime = true\n\n        let sourcePixelBufferAttributes: [String: Any] = [\n            kCVPixelBufferPixelFormatTypeKey as String : kCVPixelFormatType_32BGRA,\n            kCVPixelBufferWidthKey as String : size.width,\n            kCVPixelBufferHeightKey as String : size.height ]\n\n        assetWriterPixelBufferInput = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: assetWriterVideoInput,\n                                                                           sourcePixelBufferAttributes: sourcePixelBufferAttributes)\n\n        assetWriter.add(assetWriterVideoInput)\n    }\n\n    func startRecording() {\n        assetWriter.startWriting()\n        assetWriter.startSession(atSourceTime: .zero)\n\n        recordingStartTime = CACurrentMediaTime()\n        isRecording = true\n    }\n\n    func endRecording(_ completionHandler: @escaping () -> ()) {\n        isRecording = false\n\n        assetWriterVideoInput.markAsFinished()\n        assetWriter.finishWriting(completionHandler: completionHandler)\n    }\n\n    func writeFrame(forTexture texture: MTLTexture) {\n        if !isRecording {\n            return\n        }\n\n        while !assetWriterVideoInput.isReadyForMoreMediaData {}\n\n        guard let pixelBufferPool = assetWriterPixelBufferInput.pixelBufferPool else {\n            print(\"Pixel buffer asset writer input did not have a pixel buffer pool available; cannot retrieve frame\")\n            return\n        }\n\n        var maybePixelBuffer: CVPixelBuffer? = nil\n        let status  = CVPixelBufferPoolCreatePixelBuffer(nil, pixelBufferPool, &maybePixelBuffer)\n        if status != kCVReturnSuccess {\n            print(\"Could not get pixel buffer from asset writer input; dropping frame...\")\n            return\n        }\n\n        guard let pixelBuffer = maybePixelBuffer else { return }\n\n        CVPixelBufferLockBaseAddress(pixelBuffer, [])\n        let pixelBufferBytes = CVPixelBufferGetBaseAddress(pixelBuffer)!\n\n        // Use the bytes per row value from the pixel buffer since its stride may be rounded up to be 16-byte aligned\n        let bytesPerRow = CVPixelBufferGetBytesPerRow(pixelBuffer)\n        let region = MTLRegionMake2D(0, 0, texture.width, texture.height)\n\n        texture.getBytes(pixelBufferBytes, bytesPerRow: bytesPerRow, from: region, mipmapLevel: 0)\n\n        let frameTime = CACurrentMediaTime() - recordingStartTime\n        let presentationTime = CMTimeMakeWithSeconds(frameTime, preferredTimescale: 240)\n        assetWriterPixelBufferInput.append(pixelBuffer, withPresentationTime: presentationTime)\n\n        CVPixelBufferUnlockBaseAddress(pixelBuffer, [])\n    }\n}\n\nAfter initializing one of these and calling startRecording(), you can add a scheduled handler to the command buffer containing your rendering commands and call writeFrame (after you end encoding, but before presenting the drawable or committing the buffer):\nlet texture = currentDrawable.texture\ncommandBuffer.addCompletedHandler { commandBuffer in\n    self.recorder.writeFrame(forTexture: texture)\n}\n\nWhen you're done recording, just call endRecording, and the video file will be finalized and closed.\nCaveats:\nThis class assumes the source texture to be of the default format, .bgra8Unorm. If it isn't, you'll get crashes or corruption. If necessary, convert the texture with a compute or fragment shader, or use Accelerate.\nThis class also assumes that the texture is the same size as the video frame. If this isn't the case (if the drawable size changes, or your screen autorotates), the output will be corrupted and you may see crashes. Mitigate this by scaling or cropping the source texture as your application requires."
  },
  {
    "question": "I tried using Metal in a simple app but when I call the device.newDefaultLibrary() function then I get an error in runtime:\n\n/BuildRoot/Library/Caches/com.apple.xbs/Sources/Metal/Metal-56.7/Framework/MTLLibrary.mm:1842: failed assertion `Metal default library not found'\n\nHas anyone any idea what cloud be the problem? I followed this tutorial.\nThe code is a little old but with tiny changes it work. Here is my ViewController code:\nimport UIKit\nimport Metal\nimport QuartzCore\n\nclass ViewController: UIViewController {\n\n    //11A\n        var device: MTLDevice! = nil\n    \n        //11B\n        var metalLayer: CAMetalLayer! = nil\n    \n        //11C\n        let vertexData:[Float] = [\n            0.0, 1.0, 0.0,\n            -1.0, -1.0, 0.0,\n            1.0, -1.0, 0.0]\n        var vertexBuffer: MTLBuffer! = nil\n    \n        //11F\n        var pipelineState: MTLRenderPipelineState! = nil\n    \n        //11G\n        var commandQueue: MTLCommandQueue! = nil\n    \n        //12A\n        var timer: CADisplayLink! = nil\n    \n    override func viewDidLoad() {\n        super.viewDidLoad()\n        // Do any additional setup after loading the view, typically from a nib.\n        \n        //11A\n        device = MTLCreateSystemDefaultDevice()\n        \n        //11B\n        metalLayer = CAMetalLayer()          // 1\n        metalLayer.device = device           // 2\n        metalLayer.pixelFormat = .BGRA8Unorm // 3\n        metalLayer.framebufferOnly = true    // 4\n        metalLayer.frame = view.layer.frame  // 5\n        view.layer.addSublayer(metalLayer)   // 6\n        \n        //11C\n        let dataSize = vertexData.count * sizeofValue(vertexData[0]) // 1\n        vertexBuffer = device.newBufferWithBytes(vertexData, length: dataSize, options: MTLResourceOptions.CPUCacheModeDefaultCache) // 2\n        \n        //11F\n        // 1\n        let defaultLibrary = device.newDefaultLibrary() //The error is generating here\n        let fragmentProgram = defaultLibrary!.newFunctionWithName(\"basic_fragment\")\n        let vertexProgram = defaultLibrary!.newFunctionWithName(\"basic_vertex\")\n        \n        // 2\n        let pipelineStateDescriptor = MTLRenderPipelineDescriptor()\n        pipelineStateDescriptor.vertexFunction = vertexProgram\n        pipelineStateDescriptor.fragmentFunction = fragmentProgram\n        pipelineStateDescriptor.colorAttachments[0].pixelFormat = .BGRA8Unorm\n        \n        // 3\n        do {\n            try pipelineState = device.newRenderPipelineStateWithDescriptor(pipelineStateDescriptor)\n        } catch _ {\n            print(\"Failed to create pipeline state, error\")\n        }\n        \n        //11G\n        commandQueue = device.newCommandQueue()\n        \n        //12A\n        timer = CADisplayLink(target: self, selector: Selector(\"gameloop\"))\n        timer.addToRunLoop(NSRunLoop.mainRunLoop(), forMode: NSDefaultRunLoopMode)\n\n    }\n    \n    override func didReceiveMemoryWarning() {\n        super.didReceiveMemoryWarning()\n        // Dispose of any resources that can be recreated.\n    }\n    \n    //MARK: Custom Methodes\n    \n    //12A\n    func render() {\n        \n        //12C\n        let commandBuffer = commandQueue.commandBuffer()\n        \n        //12B\n        let drawable = metalLayer.nextDrawable()\n        \n        let renderPassDescriptor = MTLRenderPassDescriptor()\n        renderPassDescriptor.colorAttachments[0].texture = drawable!.texture\n        renderPassDescriptor.colorAttachments[0].loadAction = .Clear\n        renderPassDescriptor.colorAttachments[0].clearColor = MTLClearColor(red: 0.0, green: 104.0/255.0, blue: 5.0/255.0, alpha: 1.0)\n        \n        //12D\n        let renderEncoderOpt = commandBuffer.renderCommandEncoderWithDescriptor(renderPassDescriptor)\n        \n        renderEncoderOpt.setRenderPipelineState(pipelineState)\n        renderEncoderOpt.setVertexBuffer(vertexBuffer, offset: 0, atIndex: 0)\n        renderEncoderOpt.drawPrimitives(.Triangle, vertexStart: 0, vertexCount: 3, instanceCount: 1)\n        renderEncoderOpt.endEncoding()\n        \n        //12E\n        commandBuffer.presentDrawable(drawable!)\n        commandBuffer.commit()\n        \n    }\n    \n    func gameloop() {\n        autoreleasepool {\n            self.render()\n        }\n    }\n\n}\n\nI use an iPhone 5s device with iOS 9.3 for testing.",
    "answer": "The default library is only included in your app when you have at least one .metal file in your app target's Compile Sources build phase. I assume you've followed the steps of the tutorial where you created the Metal shader source file and added the vertex and fragment functions, so you simply need to use the + icon in the build phases setting to add that file to your compilation phase:"
  },
  {
    "question": "Whenever I build a project that includes a metal shader to an x86_64 target (iOS simulator), I get a dependency analysis warning:\nwarning: no rule to process file '[File Path]/Shaders.metal' of type sourcecode.metal for architecture x86_64\nI know this isn't a huge issue but I like to keep my projects free from warnings when I build, so that when a real issue does arise, I actually notice the yellow warning triangle.\nAny quick way to get Xcode to ignore metal files for simulator targets?",
    "answer": "You can resolve this by precompiling your .metal file into a Metal library during the build step, and removing the .metal source code from your app target.\nRemove .metal file from target\nSelect your .metal file in the project navigator, and uncheck the target that is giving you the warning.\nMetal library compile script\nCreate a bash script called CompileMetalLib.sh in your project, alongside your .metal file, with contents like this:\nxcrun -sdk iphoneos metal -c MyShader.metal -o MyShader.air\nxcrun -sdk iphoneos metallib MyShader.air -o MyShader.metallib\nrm MyShader.air\n\nMake sure to give it executable permissions by running chmod +x CompileMetalLib.sh.\nMyShader.air is the intermediate compile step, and MyShader.metallib is the fully compiled metal library. Read all about compiling a Metal file here\nIf you're compiling for OS X, change iphoneos to macosx.\nRun compile script during build\nNow you'll want to trigger this script in your Build Phases.\nAdd a New Run Script Phase to your target. The contents should look like this:\ncd ${SRCROOT}/path/to/folder/containing/yourshader\n./CompileMetalLib.sh\n\nIt's important to drag this step so that it happens before the Copy Bundle Resources step.\nChange your code to use your compiled Metal library\nYou'll now load the compiled Metal library from your app bundle.\nHere's some pseudo-Swift:\nlet metalLibPath = Bundle.main.path(forResource: \"MyShader\", ofType: \"metallib\")\nlet myLibrary = try metalDevice.makeLibrary(filepath: metalLibPath)\n\nResult\nYou are manually compiling your .metal file with an external script, and copying the compiled library into your bundle resources. Your code loads this library. Now that you don't have any .metal files in your target, the simulator will no longer throw a warning about not being able to compile for x86_64."
  },
  {
    "question": "I am going through the Metal iOS Swift example trying to understand the triple buffering practice they suggest. This is shown inside of the demo for the uniform animations.\nAs I understand it aligned memory simply starts at a specefic increment that is a multiple of some byte amount that the device really likes. My confusion is this line of code\n// The 256 byte aligned size of our uniform structure\nlet alignedUniformsSize = (MemoryLayout<Uniforms>.size & ~0xFF) + 0x100\n\nthey use it to find the size and byte of the Uniforms struct. I am confused about why there are binary operations here I am really not sure what they do.\nIf it helps this aligned size is used to create a buffer like this. I am fairly sure that buffer allocates byte aligned memory automatically and is henceforth used as the memory storage location for the uniforms.\nlet buffer = self.device.makeBuffer(length:alignedUniformsSize * 3, options:[MTLResourceOptions.storageModeShared])\n\nSo essentially rather than going through the trouble of allocating byte aligned memory by yourself they let metal do it for them.\nIs there any reason that the strategy they used when they did let allignedUniformsSize = would not work for other types such as Int or Float etc?",
    "answer": "Let's talk first about why you'd want aligned buffers, then we can talk about the bitwise arithmetic.\nOur goal is to allocate a Metal buffer that can store three (triple-buffered) copies of our uniforms (so that we can write to one part of the buffer while the GPU reads from another). In order to read from each of these three copies, we supply an offset when binding the buffer, something like currentBufferIndex * uniformsSize. Certain Metal devices require these offsets to be multiples of 256, so we instead need to use something like currentBufferIndex * alignedUniformsSize as our offset.\nHow do we \"round up\" an integer to the next highest multiple of 256? We can do it by dropping the lowest 8 bits of the \"unaligned\" size, effectively rounding down, then adding 256, which gets us the next highest multiple. The rounding down part is achieved by bitwise ANDing with the 1's complement (~) of 255, which (in 32-bit) is 0xFFFFFF00. The rounding up is done by just adding 0x100, which is 256.\nInterestingly, if the base size is already aligned, this technique spuriously rounds up anyway (e.g., from 256 to 512). For the cost of an integer divide, you can avoid this waste:\nlet alignedUniformsSize = ((MemoryLayout<Uniforms>.size + 255) / 256) * 256"
  },
  {
    "question": "I am getting this error in my nginx-error.log file:\n2014/02/17 03:42:20 [crit] 5455#0: *1 connect() to unix:/tmp/uwsgi.sock failed (13: Permission denied) while connecting to upstream, client: xx.xx.x.xxx, server: localhost, request: \"GET /users HTTP/1.1\", upstream: \"uwsgi://unix:/tmp/uwsgi.sock:\", host: \"EC2.amazonaws.com\"\n\nThe browser also shows a 502 Bad Gateway Error. The output of a curl is the same, Bad Gateway html\nI've tried to fix it by changing permissions for /tmp/uwsgi.sock to 777. That didn't work. I also added myself to the www-data group (a couple questions that looked similar suggested that). Also, no dice. \nHere is my nginx.conf file:\nnginx.conf\nworker_processes 1;\nworker_rlimit_nofile 8192;\n\nevents {\n  worker_connections  3000; \n}\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                  '$status $body_bytes_sent \"$http_referer\" '\n                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on; \n    #tcp_nopush     on; \n\n    keepalive_timeout  65; \n\n    #gzip  on; \n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n}\n\nI am running a Flask application with Nginsx and Uwsgi, just to be thorough in my explanation. If anyone has any ideas, I would really appreciate them.\n\nEDIT\nI have been asked to provide my uwsgi config file. So, I never personally wrote my nginx or my uwsgi file. I followed the guide here which sets everything up using ansible-playbook. The nginx.conf file was generated automatically, but there was nothing in /etc/uwsgi except a README file in both apps-enabled and apps-available folders. Do I need to create my own config file for uwsgi? I was under the impression that ansible took care of all of those things. \nI believe that ansible-playbook figured out my uwsgi configuration since when I run this command\nuwsgi -s /tmp/uwsgi.sock -w my_app:app\n\nit starts up and outputs this:\n*** Starting uWSGI 2.0.1 (64bit) on [Mon Feb 17 20:03:08 2014] ***\ncompiled with version: 4.7.3 on 10 February 2014 18:26:16\nos: Linux-3.11.0-15-generic #25-Ubuntu SMP Thu Jan 30 17:22:01 UTC 2014\nnodename: ip-10-9-xxx-xxx\nmachine: x86_64\nclock source: unix\ndetected number of CPU cores: 1\ncurrent working directory: /home/username/Project\ndetected binary path: /usr/local/bin/uwsgi\n!!! no internal routing support, rebuild with pcre support !!!\n*** WARNING: you are running uWSGI without its master process manager ***\nyour processes number limit is 4548\nyour memory page size is 4096 bytes\ndetected max file descriptor number: 1024\nlock engine: pthread robust mutexes\nthunder lock: disabled (you can enable it with --thunder-lock)\nuwsgi socket 0 bound to UNIX address /tmp/uwsgi.sock fd 3\nPython version: 2.7.5+ (default, Sep 19 2013, 13:52:09)  [GCC 4.8.1]\n*** Python threads support is disabled. You can enable it with --enable-threads ***\nPython main interpreter initialized at 0x1f60260\nyour server socket listen backlog is limited to 100 connections\nyour mercy for graceful operations on workers is 60 seconds\nmapped 72760 bytes (71 KB) for 1 cores\n*** Operational MODE: single process ***\nWSGI app 0 (mountpoint='') ready in 3 seconds on interpreter 0x1f60260 pid: 26790 (default app)\n*** uWSGI is running in multiple interpreter mode ***\nspawned uWSGI worker 1 (and the only) (pid: 26790, cores: 1)",
    "answer": "The permission issue occurs because uwsgi resets the ownership and permissions of /tmp/uwsgi.sock to 755 and the user running uwsgi every time uwsgi starts.\nThe correct way to solve the problem is to make uwsgi change the ownership and/or permission of /tmp/uwsgi.sock such that nginx can write to this socket. Therefore, there are three possible solutions.\n\nRun uwsgi as the www-data user so that this user owns the socket file created by it.\nuwsgi -s /tmp/uwsgi.sock -w my_app:app --uid www-data --gid www-data\n\nChange the ownership of the socket file so that www-data owns it.\nuwsgi -s /tmp/uwsgi.sock -w my_app:app --chown-socket=www-data:www-data\n\nChange the permissions of the socket file, so that www-data can write to it.\nuwsgi -s /tmp/uwsgi.sock -w my_app:app --chmod-socket=666\n\n\nI prefer the first approach because it does not leave uwsgi running as root.\nThe first two commands need to be run as root user. The third command does not need to be run as root user.\nThe first command leaves uwsgi running as www-data user. The second and third commands leave uwsgi running as the actual user that ran the command.\nThe first and second command allow only www-data user to write to the socket. The third command allows any user to write to the socket.\nI prefer the first approach because it does not leave uwsgi running as root user and it does not make the socket file world-writeable ."
  },
  {
    "question": "I Have integrated keycloak with an angular app. Basically, both frontend and backend are on different server.Backend app is running on apache tomcat 8. Frontend app is running on JBoss welcome content folder.\nAngular config\nangular.element(document).ready(function ($http) {\n    var keycloakAuth = new Keycloak('keycloak.json');\n    auth.loggedIn = false;\n    keycloakAuth.init({ onLoad: 'login-required' }).success(function () {\n        keycloakAuth.loadUserInfo().success(function (userInfo) {\n            console.log(userInfo);  \n        });\n        auth.loggedIn = true;\n        auth.authz = keycloakAuth;\n        auth.logoutUrl = keycloakAuth.authServerUrl + \"/realms/app1/protocol/openid-connect/logout?redirect_uri=http://35.154.214.8/hrms-keycloak/index.html\";\n        module.factory('Auth', function() {\n            return auth;\n        });\n        angular.bootstrap(document, [\"themesApp\"]);\n    }).error(function () {\n            window.location.reload();\n        });\n\n});\nmodule.factory('authInterceptor', function($q, Auth) {\n    return {\n        request: function (config) {\n            var deferred = $q.defer();\n            if (Auth.authz.token) {\n                Auth.authz.updateToken(5).success(function() {\n                    config.headers = config.headers || {};\n                    config.headers.Authorization = 'Bearer ' + Auth.authz.token;\n                    deferred.resolve(config);\n                }).error(function() {\n                        deferred.reject('Failed to refresh token');\n                    });\n            }\n            return deferred.promise;\n        }\n    };\n});\nmodule.config([\"$httpProvider\", function ($httpProvider)  {\n    $httpProvider.interceptors.push('authInterceptor');\n}]);\n\nRequest Header\nAccept:*/*\nAccept-Encoding:gzip, deflate\nAccept-Language:en-US,en;q=0.8\nAccess-Control-Request-Headers:authorization\nAccess-Control-Request-Method:GET\nConnection:keep-alive\nHost:35.154.214.8:8080\nOrigin:http://35.154.214.8\nReferer:http://35.154.214.8/accounts-keycloak/\nUser-Agent:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36\n\nError on web console.\nXMLHttpRequest cannot load http://35.154.214.8:8080/company/loadCurrencyList. Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://35.154.214.8' is therefore not allowed access.\n\nCors filter on backend\n@Component\npublic class CORSFilter implements Filter {\n    static Logger logger = LoggerFactory.getLogger(CORSFilter.class);\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n    }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse res,\n            FilterChain chain) throws IOException, ServletException {\n        HttpServletResponse response = (HttpServletResponse) res;\n        response.setHeader(\"Access-Control-Allow-Origin\", \"*\");\n        response.setHeader(\"Access-Control-Allow-Methods\", \"*\");\n        response.setHeader(\"Access-Control-Max-Age\", \"3600\");\n        response.setHeader(\"Access-Control-Allow-Headers\", \"*\");\n        chain.doFilter(request, response);\n    }\n\n    public void destroy() {\n    }\n}",
    "answer": "I was fighting with KeyCloak and CORS and all of this for about two weeks, and this is my solution (for keycloak 3.2.1):\nIts all about configuring KeyCloak server.\nIt seems to be, that WebOrigin of your Realm needs to be *\nOnly one origin \"*\".\nThats all, what was needed for me.\nIf you enter your server as WebOrigin, the trouble begins.\nWhen you call keycloak.init in JavaScript, keycloak does not generate CORS headers, so you have to configure them manually, and as soon as you do so, and call keycloak.getUserInfo after successful init - you get double CORS headers, which is not allowed.\nSomewhere deep inside of keycloak mailing lists is stated, that you need to set enable-cors=true in your keycloak.json, but there is nothing about that on keycloak.gitbooks.io. So it seems not to be true.\nThey also don't mention CORS when describing JavaScript and Node.Js adapters, and I don't know why, seems not to be important at all.\nIt also seems to be, that you should not touch WildFly configuration to provide CORS headers.\nBesides, CORS in OIDC is a special KeyCloak feature (and not a bug).\nHopefully this answer serves you well."
  },
  {
    "question": "I'm trying to test Keycloak REST API.\nInstaled the version 2.1.0.Final.\nI can access the admin through browser with SSL without problems.\nI'm using the code above:\nKeycloak keycloakClient = KeycloakBuilder.builder()\n.serverUrl(\"https://keycloak.intra.rps.com.br/auth\")\n.realm(\"testrealm\")\n.username(\"development\")\n.password(\"development\")\n.clientId(\"admin-cli\")\n.resteasyClient(new ResteasyClientBuilder().connectionPoolSize(10).build())\n.build();\nList<RealmRepresentation> rr = keycloakClient.realms().findAll();\n\nAnd got the error:\njavax.ws.rs.ProcessingException: RESTEASY003145: Unable to find a MessageBodyReader of content-type application/json and type class org.keycloak.representations.AccessTokenResponse\n\njavax.ws.rs.client.ResponseProcessingException: javax.ws.rs.ProcessingException: RESTEASY003145: Unable to find a MessageBodyReader of content-type application/json and type class org.keycloak.representations.AccessTokenResponse\nat org.jboss.resteasy.client.jaxrs.internal.ClientInvocation.extractResult(ClientInvocation.java:141)\nat org.jboss.resteasy.client.jaxrs.internal.proxy.extractors.BodyEntityExtractor.extractEntity(BodyEntityExtractor.java:60)\nat org.jboss.resteasy.client.jaxrs.internal.proxy.ClientInvoker.invoke(ClientInvoker.java:104)\nat org.jboss.resteasy.client.jaxrs.internal.proxy.ClientProxy.invoke(ClientProxy.java:76)\nat com.sun.proxy.$Proxy20.grantToken(Unknown Source)\nat org.keycloak.admin.client.token.TokenManager.grantToken(TokenManager.java:85)\nat org.keycloak.admin.client.token.TokenManager.getAccessToken(TokenManager.java:65)\nat org.keycloak.admin.client.token.TokenManager.getAccessTokenString(TokenManager.java:60)\nat org.keycloak.admin.client.resource.BearerAuthFilter.filter(BearerAuthFilter.java:52)\nat org.jboss.resteasy.client.jaxrs.internal.ClientInvocation.invoke(ClientInvocation.java:413)\nat org.jboss.resteasy.client.jaxrs.internal.proxy.ClientInvoker.invoke(ClientInvoker.java:102)\nat org.jboss.resteasy.client.jaxrs.internal.proxy.ClientProxy.invoke(ClientProxy.java:76)\nat com.sun.proxy.$Proxy22.findAll(Unknown Source)\nat br.com.rps.itsm.sd.SgpKeycloakClient.doGet(SgpKeycloakClient.java:71)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\nat io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:85)\nat io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)\nat io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)\nat io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:131)\nat io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)\nat io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)\nat io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)\nat io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)\nat io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)\nat io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)\nat io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)\nat io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)\nat io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)\nat io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:284)\nat io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:263)\nat io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)\nat io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:174)\nat io.undertow.server.Connectors.executeRootHandler(Connectors.java:202)\nat io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:793)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: javax.ws.rs.ProcessingException: RESTEASY003145: Unable to find a MessageBodyReader of content-type application/json and type class org.keycloak.representations.AccessTokenResponse\nat org.jboss.resteasy.core.interception.ClientReaderInterceptorContext.throwReaderNotFound(ClientReaderInterceptorContext.java:42)\nat org.jboss.resteasy.core.interception.AbstractReaderInterceptorContext.getReader(AbstractReaderInterceptorContext.java:75)\nat org.jboss.resteasy.core.interception.AbstractReaderInterceptorContext.proceed(AbstractReaderInterceptorContext.java:52)\nat org.jboss.resteasy.plugins.interceptors.encoding.GZIPDecodingInterceptor.aroundReadFrom(GZIPDecodingInterceptor.java:59)\nat org.jboss.resteasy.core.interception.AbstractReaderInterceptorContext.proceed(AbstractReaderInterceptorContext.java:55)\nat org.jboss.resteasy.client.jaxrs.internal.ClientResponse.readFrom(ClientResponse.java:251)\nat org.jboss.resteasy.client.jaxrs.internal.ClientResponse.readEntity(ClientResponse.java:181)\nat org.jboss.resteasy.specimpl.BuiltResponse.readEntity(BuiltResponse.java:213)\nat org.jboss.resteasy.client.jaxrs.internal.ClientInvocation.extractResult(ClientInvocation.java:105)\n\nI added the dependencies above, but do not solve my problem:\n    <dependency>\n            <groupId>org.jboss.resteasy</groupId>\n            <artifactId>resteasy-client</artifactId>\n            <version>3.0.19.Final</version>\n    </dependency>\n    <dependency>\n            <groupId>org.jboss.resteasy</groupId>\n            <artifactId>resteasy-jackson-provider</artifactId>\n            <version>3.0.19.Final</version>\n    </dependency>\n\nAny clues?",
    "answer": "I used the dependency to fix this issue\n   <dependency>\n        <groupId>org.jboss.resteasy</groupId>\n        <artifactId>resteasy-jackson2-provider</artifactId>\n        <version>3.1.0.Final</version>\n    </dependency>"
  },
  {
    "question": "I'm trying to set up Terraform for use with GCP and I'm having trouble creating a new project from the gcloud cli: Terraform Lab\nThe command I'm using is\ngcloud projects create testproject\n\nThe error I get over and over is:\nERROR: (gcloud.projects.create) Project creation failed. The project ID you specified is already in use by another project. Please try an alternative ID.\n\nHere's what I did so far:\n\nI created an \"organization\" and a user in Cloud Identity\nLogged into GCP console in the browser with the user I created\nThe user has \"Organization Administrator\" role\nUsing the Cloud Shell or gcloud configured on my home computer, I am not able to create a new project. I am able to do things like \"gcloud projects list\" and \"gcloud organizations list\" successfully in both cases (cloud shell & local gcloud install)\nI have tried this with different project ID names that are within the format requirements (eg 6-30 chars, lowercase, etc). I can also confirm that the project IDs do not exist.\nHowever, I am able to successfully create projects via GCP web console (https://console.cloud.google.com) (using the same IAM account configured in gcloud cli)\nI have tried \"gcloud init\" several times making sure I am using the right IAM account, just in case. \n\nHere's the error I get when I try to create a new project from the \"gcloud init\" command:\nEnter a Project ID. Note that a Project ID CANNOT be changed later.\nProject IDs must be 6-30 characters (lowercase ASCII, digits, or\nhyphens) in length and start with a lowercase letter. vincetest\nWARNING: Project creation failed: HttpError accessing \n<https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: \nresponse: <{'status': '409', 'content-length': '268', 'x-xss\n-protection': '1; mode=block', 'x-content-type-options': 'nosniff', \n'transfer-encoding': 'chunked', 'vary': 'Origin, X-Origin, Referer', \n'server': 'ESF', '-content-encoding': 'gzip',\n 'cache-control': 'private', 'date': 'Fri, 28 Sep 2018 18:38:11 GMT', \n 'x-frame-options': 'SAMEORIGIN', 'content-type': 'application/json; \n charset=UTF-8'}>, content <{\n  \"error\": {\n    \"code\": 409,\n    \"message\": \"Requested entity already exists\",\n    \"status\": \"ALREADY_EXISTS\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ResourceInfo\",\n        \"resourceName\": \"projects/vincetest\"\n      }\n    ]\n  }\n}\n>\n\nCreating the project from the web page console worked fine.",
    "answer": "Project IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.\nTry a more unique ID. One common technique is to user your organization's name as a prefix."
  },
  {
    "question": "I wanted to try out Caddy in a docker environment but it does not seem to be able to connect to other containers. I created a network \"caddy\" and want to run a portainer alongside it. If I go into the volume of caddy, I can see, that there are certs generated, so that seems to work. Also portainer is running and accessible via the Server IP (http://65.21.139.246:1000/). But when I access via the url: https://smallhetzi.fading-flame.com/ I get a 502 and in the log of caddy I can see this message:\n{\n    \"level\": \"error\",\n    \"ts\": 1629873106.715402,\n    \"logger\": \"http.log.error\",\n    \"msg\": \"dial tcp 172.20.0.2:1000: connect: connection refused\",\n    \"request\": {\n        \"remote_addr\": \"89.247.255.231:15146\",\n        \"proto\": \"HTTP/2.0\",\n        \"method\": \"GET\",\n        \"host\": \"smallhetzi.fading-flame.com\",\n        \"uri\": \"/\",\n        \"headers\": {\n            \"Accept-Encoding\": [\n                \"gzip, deflate, br\"\n            ],\n            \"Accept-Language\": [\n                \"de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7\"\n            ],\n            \"Cache-Control\": [\n                \"max-age=0\"\n            ],\n            \"User-Agent\": [\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"\n            ],\n            \"Sec-Fetch-Site\": [\n                \"none\"\n            ],\n            \"Accept\": [\n                \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n            ],\n            \"Sec-Fetch-Mode\": [\n                \"navigate\"\n            ],\n            \"Sec-Fetch-User\": [\n                \"?1\"\n            ],\n            \"Sec-Fetch-Dest\": [\n                \"document\"\n            ],\n            \"Sec-Ch-Ua\": [\n                \"\\\"Chromium\\\";v=\\\"92\\\", \\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"92\\\"\"\n            ],\n            \"Sec-Ch-Ua-Mobile\": [\n                \"?0\"\n            ],\n            \"Upgrade-Insecure-Requests\": [\n                \"1\"\n            ]\n        },\n        \"tls\": {\n            \"resumed\": false,\n            \"version\": 772,\n            \"cipher_suite\": 4865,\n            \"proto\": \"h2\",\n            \"proto_mutual\": true,\n            \"server_name\": \"smallhetzi.fading-flame.com\"\n        }\n    },\n    \"duration\": 0.000580828,\n    \"status\": 502,\n    \"err_id\": \"pq78d9hen\",\n    \"err_trace\": \"reverseproxy.statusError (reverseproxy.go:857)\"\n}\n\nBut two compose files:\nCaddy:\nversion: '3.9'\n\nservices:\n  caddy:\n    image: caddy:2-alpine\n    container_name: caddy\n    restart: unless-stopped\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile\n      - certs-volume:/data\n      - caddy_config:/config\n\nvolumes:\n  certs-volume:\n  caddy_config:\n\nnetworks:\n  default:\n    external:\n      name: caddy\n\nCaddyfile:\n{\n    email simonheiss87@gmail.com\n    # acme_ca https://acme-staging-v02.api.letsencrypt.org/directory\n}\n\nsmallhetzi.fading-flame.com {\n    reverse_proxy portainer:1000\n}\n\nand my portainer file:\nversion: '3.9'\n\nservices:\n  portainer:\n    image: portainer/portainer-ce\n    container_name: portainer\n    restart: always\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - portainer_data:/data portainer/portainer\n    entrypoint: /portainer -p :80\n    ports:\n      - \"1000:80\"\n\nvolumes:\n  portainer_data:\n\nnetworks:\n  default:\n    external:\n      name: caddy\n\nWhat I think happens is, that those two containers are somehow not in the same network, but I dont get why.\nWhat works as a workaround right now is, when i make this change to my Caddyfile:\nsmallhetzi.fading-flame.com {\n    reverse_proxy 65.21.139.246:1000\n}\n\nThen I get a valid certificate and the portainer ui. But i would rather not spread the IPs over my Caddyfile. Do I have to configure something else for caddy to run in docker?",
    "answer": "I just got help from the forum and it turns out, that caddy redirects to the port INSIDE the container, not the public one. In my case, portainer runs on 80 internally, so changing the Caddyfile to this:\nsmallhetzi.fading-flame.com {\n    reverse_proxy portainer:80\n}\n\nor this\nsmallhetzi.fading-flame.com {\n    reverse_proxy http://portainer\n}\n\ndoes the job. This also means, that I could get rid of exposing portainer directly over the port 1000. Now I can only access it via the proxy.\nHope someone gets some help from that :)"
  },
  {
    "question": "I am using Java and Protoc 3.0 compiler and my proto file is mention below. \n    https://github.com/openconfig/public/blob/master/release/models/rpc/openconfig-rpc-api.yang\nsyntax = \"proto3\";\n\npackage Telemetry;\n\n// Interface exported by Agent\nservice OpenConfigTelemetry {\n    // Request an inline subscription for data at the specified path.\n    // The device should send telemetry data back on the same\n    // connection as the subscription request.\n    rpc telemetrySubscribe(SubscriptionRequest)                     returns (stream OpenConfigData) {}\n\n    // Terminates and removes an exisiting telemetry subscription\n    rpc cancelTelemetrySubscription(CancelSubscriptionRequest)      returns (CancelSubscriptionReply) {}\n\n    // Get the list of current telemetry subscriptions from the\n    // target. This command returns a list of existing subscriptions\n    // not including those that are established via configuration.\n    rpc getTelemetrySubscriptions(GetSubscriptionsRequest)          returns (GetSubscriptionsReply) {}\n\n    // Get Telemetry Agent Operational States\n    rpc getTelemetryOperationalState(GetOperationalStateRequest)    returns (GetOperationalStateReply) {}\n\n    // Return the set of data encodings supported by the device for\n    // telemetry data\n    rpc getDataEncodings(DataEncodingRequest)                       returns (DataEncodingReply) {}\n}\n\n// Message sent for a telemetry subscription request\nmessage SubscriptionRequest {\n    // Data associated with a telemetry subscription\n    SubscriptionInput input                                 = 1;\n\n    // List of data models paths and filters\n    // which are used in a telemetry operation.\n    repeated Path path_list                                 = 2;\n\n    // The below configuration is not defined in Openconfig RPC.\n    // It is a proposed extension to configure additional\n    // subscription request features.\n    SubscriptionAdditionalConfig additional_config          = 3;\n}\n\n// Data associated with a telemetry subscription\nmessage SubscriptionInput {\n    // List of optional collector endpoints to send data for\n    // this subscription.\n    // If no collector destinations are specified, the collector\n    // destination is assumed to be the requester on the rpc channel.\n    repeated Collector  collector_list                      = 1;\n}\n\n// Collector endpoints to send data specified as an ip+port combination.\nmessage Collector {\n    // IP address of collector endpoint\n    string address                                          = 1;\n\n    // Transport protocol port number for the collector destination.\n    uint32 port                                             = 2;\n}\n\n// Data model path\nmessage Path {\n    // Data model path of interest\n    // Path specification for elements of OpenConfig data models\n    string path                                             = 1;\n\n    // Regular expression to be used in filtering state leaves\n    string filter                                           = 2;\n\n    // If this is set to true, the target device will only send\n    // updates to the collector upon a change in data value\n    bool suppress_unchanged                                 = 3;\n\n    // Maximum time in ms the target device may go without sending\n    // a message to the collector. If this time expires with\n    // suppress-unchanged set, the target device must send an update\n    // message regardless if the data values have changed.\n    uint32 max_silent_interval                              = 4;\n\n    // Time in ms between collection and transmission of the\n    // specified data to the collector platform. The target device\n    // will sample the corresponding data (e.g,. a counter) and\n    // immediately send to the collector destination.\n    //\n    // If sample-frequency is set to 0, then the network device\n    // must emit an update upon every datum change.\n    uint32 sample_frequency                                 = 5;\n}\n\n// Configure subscription request additional features.\nmessage SubscriptionAdditionalConfig {\n    // limit the number of records sent in the stream\n    int32 limit_records                                     = 1;\n\n    // limit the time the stream remains open\n    int32 limit_time_seconds                                = 2;\n}\n\n// Reply to inline subscription for data at the specified path is done in\n// two-folds.\n// 1. Reply data message sent out using out-of-band channel.\n// 2. Telemetry data send back on the same connection as the\n//    subscription request.\n\n// 1. Reply data message sent out using out-of-band channel.\nmessage SubscriptionReply {\n    // Response message to a telemetry subscription creation or\n    // get request.\n    SubscriptionResponse response                           = 1;\n\n    // List of data models paths and filters\n    // which are used in a telemetry operation.\n    repeated Path path_list                                 = 2;\n}\n\n// Response message to a telemetry subscription creation or get request.\nmessage SubscriptionResponse {\n    // Unique id for the subscription on the device. This is\n    // generated by the device and returned in a subscription\n    // request or when listing existing subscriptions\n    uint32 subscription_id = 1;\n}\n\n// 2. Telemetry data send back on the same connection as the\n//    subscription request.\nmessage OpenConfigData {\n    // router name:export IP address\n    string system_id                                        = 1;\n\n    // line card / RE (slot number)\n    uint32 component_id                                     = 2;\n\n    // PFE (if applicable)\n    uint32 sub_component_id                                 = 3;\n\n    // Path specification for elements of OpenConfig data models\n    string path                                             = 4;\n\n    // Sequence number, monotonically increasing for each\n    // system_id, component_id, sub_component_id + path.\n    uint64 sequence_number                                  = 5;\n\n    // timestamp (milliseconds since epoch)\n    uint64 timestamp                                        = 6;\n\n    // List of key-value pairs\n    repeated KeyValue kv                                    = 7;\n}\n\n// Simple Key-value, where value could be one of scalar types\nmessage KeyValue {\n    // Key\n    string key                                              =  1;\n\n    // One of possible values\n    oneof value {\n        double double_value                                 =  5;\n        int64  int_value                                    =  6;\n        uint64 uint_value                                   =  7;\n        sint64 sint_value                                   =  8;\n        bool   bool_value                                   =  9;\n        string str_value                                    = 10;\n        bytes  bytes_value                                  = 11;\n    }\n}\n\n// Message sent for a telemetry subscription cancellation request\nmessage CancelSubscriptionRequest {\n    // Subscription identifier as returned by the device when\n    // subscription was requested\n    uint32 subscription_id                                  = 1;\n}\n\n// Reply to telemetry subscription cancellation request\nmessage CancelSubscriptionReply {\n    // Return code\n    ReturnCode code                                         = 1;\n\n    // Return code string\n    string     code_str                                     = 2;\n};\n\n// Result of the operation\nenum ReturnCode {\n    SUCCESS                                                 = 0;\n    NO_SUBSCRIPTION_ENTRY                                   = 1;\n    UNKNOWN_ERROR                                           = 2;\n}\n\n// Message sent for a telemetry get request\nmessage GetSubscriptionsRequest {\n    // Subscription identifier as returned by the device when\n    // subscription was requested\n    // --- or ---\n    // 0xFFFFFFFF for all subscription identifiers\n    uint32 subscription_id                                  = 1;\n}\n\n// Reply to telemetry subscription get request\nmessage GetSubscriptionsReply {\n    // List of current telemetry subscriptions\n    repeated SubscriptionReply subscription_list            = 1;\n}\n\n// Message sent for telemetry agent operational states request\nmessage GetOperationalStateRequest {\n    // Per-subscription_id level operational state can be requested.\n    //\n    // Subscription identifier as returned by the device when\n    // subscription was requested\n    // --- or ---\n    // 0xFFFFFFFF for all subscription identifiers including agent-level\n    // operational stats\n    // --- or ---\n    // If subscription_id is not present then sent only agent-level\n    // operational stats\n    uint32 subscription_id                                  = 1;\n\n    // Control verbosity of the output\n    VerbosityLevel verbosity                                = 2;\n}\n\n// Verbosity Level\nenum VerbosityLevel {\n    DETAIL                                                  = 0;\n    TERSE                                                   = 1;\n    BRIEF                                                   = 2;\n}\n\n// Reply to telemetry agent operational states request\nmessage GetOperationalStateReply {\n    // List of key-value pairs where\n    //     key      = operational state definition\n    //     value    = operational state value\n    repeated KeyValue kv                                    = 1;\n}\n\n// Message sent for a data encoding request\nmessage DataEncodingRequest {\n}\n\n// Reply to data encodings supported request\nmessage DataEncodingReply {\n    repeated EncodingType  encoding_list                    = 1;\n}\n\n// Encoding Type Supported\nenum EncodingType {\n    UNDEFINED                                               = 0;\n    XML                                                     = 1;\n    JSON_IETF                                               = 2;\n    PROTO3                                                  = 3;\n}\n\nIn order to do the service call (rpc TelemetrySubscribe) first i need to read header which have subscription id and then start reading messages. Now, using Java i am able to connect with the service, i did introduce the interceptor but when i print/retrieve header it is null. My code of calling interceptor is below,\n ClientInterceptor interceptor = new HeaderClientInterceptor();\n      originChannel = OkHttpChannelBuilder.forAddress(host, port)\n        .usePlaintext(true)\n        .build();\n     Channel channel =  ClientInterceptors.intercept(originChannel, interceptor);\n      telemetryStub = OpenConfigTelemetryGrpc.newStub(channel);\n\nThis is interceptor code to read meta Data. \n  @Override\n  public <ReqT, RespT> ClientCall<ReqT, RespT> interceptCall(MethodDescriptor<ReqT, RespT> method,\n      CallOptions callOptions, Channel next) {\n    return new SimpleForwardingClientCall<ReqT, RespT>(next.newCall(method, callOptions)) {\n\n      @Override\n      public void start(Listener<RespT> responseListener, Metadata headers) {\n\n        super.start(new SimpleForwardingClientCallListener<RespT>(responseListener) {\n          @Override\n          public void onHeaders(Metadata headers) {\n\n             Key<String> CUSTOM_HEADER_KEY = Metadata.Key.of(\"responseKEY\", Metadata.ASCII_STRING_MARSHALLER);\n\n            System.out.println(\"Contains Key?? \"+headers.containsKey(CUSTOM_HEADER_KEY));\n\nWondering is there any other way to read meta data or first message which have subscription ID in it? All i need to read first message which have subscription Id, and return the same subscription id to server so that streaming can start I have equivalent Python code using same proto file and it is communicating with server by code mention below for reference only:\n     sub_req = SubscribeRequestMsg(\"host\",port)\n     data_itr = stub.telemetrySubscribe(sub_req, _TIMEOUT_SECONDS)\n     metadata = data_itr.initial_metadata()\n\n                   if metadata[0][0] == \"responseKey\":\n                    metainfo = metadata[0][1]\n                    print metainfo\n\n                    subreply = agent_pb2.SubscriptionReply()\n                    subreply.SetInParent()\n                    google.protobuf.text_format.Merge(metainfo, subreply)\n\n                    if subreply.response.subscription_id:\n                    SUB_ID = subreply.response.subscription_id\n\nFrom the python code above i can easily retrieve meta data object, not sure how to retrieve same using Java?\nAfter reading metaData all i am getting is: Metadata({content-type=[application/grpc], grpc-encoding=[identity], grpc-accept-encoding=[identity,deflate,gzip]})\nBut i know there is one more line from meta data to it, which is \nresponse {\n  subscription_id: 2\n}\n\nHow can i extract last response from Header which have subscription id in it. I did try many options and i am lost here.",
    "answer": "The method you used is for request metadata, not response metadata:\npublic void start(Listener<RespT> responseListener, Metadata headers) {\n\nFor response metadata, you will need a ClientCall.Listener and wait for the onHeaders callback:\npublic void onHeaders(Metadata headers)\n\nI do feel like the usage of metadata you mention seems strange. Metadata is generally for additional error details or cross-cutting features that aren't specific to the RPC method (like auth, tracing, etc.)."
  },
  {
    "question": "I am transitioning my react app from webpack-dev-server to nginx.\nWhen I go to the root url \"localhost:8080/login\" I simply get a 404 and in my nginx log I see that it is trying to get:\nmy-nginx-container | 2017/05/12 21:07:01 [error] 6#6: *11 open() \"/wwwroot/login\" failed (2: No such file or directory), client: 172.20.0.1, server: , request: \"GET /login HTTP/1.1\", host: \"localhost:8080\"\nmy-nginx-container | 172.20.0.1 - - [12/May/2017:21:07:01 +0000] \"GET /login HTTP/1.1\" 404 169 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0\" \"-\"\n\nWhere should I look for a fix ?\nMy router bit in react looks like this:\nrender(\n\n  <Provider store={store}>\n    <MuiThemeProvider>\n      <BrowserRouter history={history}>\n        <div>\n          Hello there p\n          <Route path=\"/login\" component={Login} />\n          <App>\n\n            <Route path=\"/albums\" component={Albums}/>\n\n            <Photos>\n              <Route path=\"/photos\" component={SearchPhotos}/>\n            </Photos>\n            <div></div>\n            <Catalogs>\n              <Route path=\"/catalogs/list\" component={CatalogList}/>\n              <Route path=\"/catalogs/new\" component={NewCatalog}/>\n              <Route path=\"/catalogs/:id/photos/\" component={CatalogPhotos}/>\n              <Route path=\"/catalogs/:id/photos/:photoId/card\" component={PhotoCard}/>\n            </Catalogs>\n          </App>\n        </div>\n      </BrowserRouter>\n    </MuiThemeProvider>\n  </Provider>, app);\n\nAnd my nginx file like this:\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n\n    server {\n        listen 8080;\n        root /wwwroot;\n\n        location / {\n            root /wwwroot;\n            index index.html;\n\n            try_files $uri $uri/ /wwwroot/index.html;\n        }\n\n\n    }\n}\n\nEDIT:\nI know that most of the setup works because when I go to localhost:8080 without being logged in I get the login page as well. this is not through a redirect to localhost:8080/login - it is some react code.",
    "answer": "The location block in your nginx config should be:\nlocation / {\n  try_files $uri /index.html;\n}\n\nThe problem is that requests to the index.html file work, but you're not currently telling nginx to forward other requests to the index.html file too."
  },
  {
    "question": "I have a webapp on a NGinx server. I set gzip on in the conf file and now I'm trying to see if it works. YSlow says it's not, but 5 out of 6 websites that do the test say it is. How can I get a definite answer on this and why is there a difference in the results?",
    "answer": "It looks like one possible answer is, unsurprisingly, curl:\n$ curl http://example.com/ --silent --write-out \"%{size_download}\\n\" --output /dev/null\n31032\n$ curl http://example.com/ --silent -H \"Accept-Encoding: gzip,deflate\" --write-out \"%{size_download}\\n\" --output /dev/null\n2553\n\nIn the second case the client tells the server that it supports content encoding and you can see that the response was indeed shorter, compressed."
  },
  {
    "question": "I have nginx up and running with a Ruby/Sinatra app and all is well. However, I'm now trying to have a second application running from the same server and I noticed something weird. First, here's my nginx.conf:\npid /tmp/nginx.pid;\nerror_log /tmp/nginx.error.log;\n\nevents {\n  worker_connections 1024;\n  accept_mutex off;\n}\n\nhttp {\n  default_type application/octet-stream;\n  access_log /tmp/nginx.access.log combined;\n\n  sendfile on;\n  tcp_nopush on;\n  tcp_nodelay off;\n\n  gzip on;\n  gzip_http_version 1.0;\n  gzip_proxied any;\n  gzip_min_length 500;\n  gzip_disable \"MSIE [1-6]\\.\";\n  gzip_types text/plain text/xml text/css\n             text/comma-separated-values\n             text/javascript application/x-javascript\n             application/atom+xml;\n\n  upstream app {\n    server unix:/var/www/app/tmp/sockets/unicorn.sock fail_timeout=0;\n  }\n\n  server {\n    listen 80;\n    client_max_body_size 4G;\n    server_name FAKE.COM;\n\n    keepalive_timeout 5;\n\n    root /var/www/app/public;\n\n    location / {\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n\n      if (!-f $request_filename) {\n        proxy_pass http://app;\n        break;\n      }\n    }\n\n    error_page 500 502 503 504 /500.html;\n    location = /500.html {\n      root /var/www/app/public;\n    }\n  }\n}\n                                                          68,0-1        B\n\nNotice how server_name is set to FAKE.COM yet the server is responding to all hosts that hit that server via other domain names. How can I make that particular server respond only to requests for FAKE.COM?",
    "answer": "The first server block in the nginx config is the default for all requests that hit the server for which there is no specific server block.\nSo in your config, assuming your real domain is REAL.COM, when a user types that in, it will resolve to your server, and since there is no server block for this setup, the server block for FAKE.COM, being the first server block (only server block in your case), will process that request.\nThis is why proper Nginx configs have a specific server block for defaults before following with others for specific domains.\n# Default server\nserver {\n    return 404;\n}\n\nserver {\n    server_name domain_1;\n    [...]\n}\n\nserver {\n    server_name domain_2;\n    [...]\n}\n\netc\n** EDIT **\nIt seems some users are a bit confused by this example and think it is limited to a single conf file etc. \nPlease note that the above is a simple example for the OP to develop as required.\nI personally use separate vhost conf files with this as so (CentOS/RHEL):\nhttp {\n    [...]\n    # Default server\n    server {\n        return 404;\n    }\n    # Other servers\n    include /etc/nginx/conf.d/*.conf;\n}\n\n/etc/nginx/conf.d/ will contain domain_1.conf, domain_2.conf... domain_n.conf which will be included after the server block in the main nginx.conf file which will always be the first and will always be the default unless it is overridden it with the default_server directive elsewhere.\nThe alphabetical order of the file names of the conf files for the other servers becomes irrelevant in this case.\nIn addition, this arrangement gives a lot of flexibility in that it is possible to define multiple defaults.\nIn my specific case, I have Apache listening on Port 8080 on the internal interface only and I proxy PHP and Perl scripts to Apache. \nHowever, I run two separate applications that both return links with \":8080\" in the output html attached as they detect that Apache is not running on the standard Port 80 and try to \"help\" me out. \nThis causes an issue in that the links become invalid as Apache cannot be reached from the external interface and the links should point at Port 80.\nI resolve this by creating a default server for Port 8080 to redirect such requests.\nhttp {\n    [...]\n    # Default server block for undefined domains\n    server {\n        listen 80;\n        return 404;\n    }\n    # Default server block to redirect Port 8080 for all domains\n    server {\n        listen my.external.ip.addr:8080;\n        return 301 http://$host$request_uri;\n    }\n    # Other servers\n    include /etc/nginx/conf.d/*.conf;\n}\n\nAs nothing in the regular server blocks listens on Port 8080, the redirect default server block transparently handles such requests by virtue of its position in nginx.conf. \nI actually have four of such server blocks and this is a simplified use case."
  },
  {
    "question": "I have this in Nginx configuration files\ngzip_types text/plain text/html text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;\n\nbut Nginx give error when starting up\n[warn]: duplicate MIME type \"text/html\" in /etc/nginx/nginx.conf:25\n\nWhat is actually duplicate to text/html? Is it text/plain?",
    "answer": "For the option gzip_types, the mime-type text/html is always included by default, so you don't need to specify it explicitly."
  },
  {
    "question": "I've recently decided to switch from Apache2 to Nginx. I installed Nginx on my CentOS server and setup a basic configuration.\nWhen I tried to load my site in browser (FF/Chrome) I noticed that css file is not loaded. I checked the error console and saw this message:\nError: The stylesheet http://example.com/style.css was not loaded because its MIME type, \"text/html\", is not \"text/css\".\nI checked Nginx configuration and everything seems to be fine:\nhttp {\n    include /etc/nginx/mime.types;\n    ..........\n}\n\nThe mime type for css files is correctly set in /etc/nginx/mime.types.\ntext/css css;\nEverything seems to be well configured but my css files are still not loaded. I have no explanation.\nAnother thing worth mentioning. Initially I installed Nginx using epel repositories and i got an old version: 0.8... It appeared to me that my problem was a bug in that version so I uninstalled 0.8 version, added nginx repository to yum and then installed latest version: 1.0.14. I thought the new version will solve my problem, but unfortunately it didn't so I am running out of ideas.\nI appreciate any help.\nConfiguration files:\n/etc/nginx/nginx.conf\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n\n/etc/nginx/conf.d/default.conf\nserver {\n    listen       80;\n    server_name  localhost;\n\n    #charset koi8-r;\n    #access_log  /var/log/nginx/log/host.access.log  main;\n\n    location / {\n         root    /usr/share/nginx/html;\n         index  index.html index.htm index.php;\n         fastcgi_pass   127.0.0.1:9000;\n         fastcgi_index  index.php;\n         fastcgi_param  SCRIPT_FILENAME  /usr/share/nginx/html$fastcgi_script_name;\n         include        fastcgi_params;\n    }\n\n    #error_page  404              /404.html;\n\n    # redirect server error pages to the static page /50x.html\n    #\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n\n    # proxy the PHP scripts to Apache listening on 127.0.0.1:80\n    #\n    #location ~ \\.php$ {\n    #    proxy_pass   http://127.0.0.1;\n    #}\n\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    #\n    #location ~ \\.php$ {\n    #    root           html;\n    #    fastcgi_pass   127.0.0.1:9000;\n    #    fastcgi_index  index.php;\n    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;\n    #    include        fastcgi_params;\n    #}\n\n    # deny access to .htaccess files, if Apache's document root\n    # concurs with nginx's one\n    #\n    #location ~ /\\.ht {\n    #    deny  all;\n    #}\n}\n\n/etc/nginx/mime.types\ntypes {\n    text/html                             html htm shtml;\n    text/css                              css;\n    text/xml                              xml;\n    image/gif                             gif;\n    image/jpeg                            jpeg jpg;\n    application/x-javascript              js;\n    application/atom+xml                  atom;\n    application/rss+xml                   rss;\n    ..........................................\n    other types here\n    ..........................................\n}",
    "answer": "Putting the  include  /etc/nginx/mime.types; under location / { instead of under http { solved the issue for me."
  },
  {
    "question": "All JavaScript files are not compressed by nginx gzip.\nCSS files are working.\nIn my nginx.conf I have the following lines:\ngzip on;\ngzip_disable \"MSIE [1-6]\\.(?!.*SV1)\";\ngzip_proxied any;\ngzip_buffers 16 8k;\ngzip_types    text/plain application/x-javascript text/xml text/css;\ngzip_vary on;",
    "answer": "Change this line:\ngzip_types    text/plain application/x-javascript text/xml text/css;\n\nTo be this:\ngzip_types    text/plain application/javascript application/x-javascript text/javascript text/xml text/css;\n\nNote the addition of application/javascript and text/javascript to your list of gzip types.\nThere are also more details—and a more expansive list of gzip types—in the answer posted here."
  },
  {
    "question": "I am having an intriguing problem where whenever I use add_header in my virtual host configuration on an ubuntu server running nginx with PHP and php-fpm it simply doesn't work and I have no idea what I am doing wrong. Here is my config file:\nserver {\n    listen   80; ## listen for ipv4; this line is default and implied\n    #listen   [::]:80 default ipv6only=on; ## listen for ipv6\n\n    root /var/www/example.com/webroot/;\n    index index.html index.htm index.php;\n\n    # Make site accessible from http://www.example.com/\n    server_name www.example.com;\n\n    # max request size\n    client_max_body_size 20m;\n\n    # enable gzip compression\n    gzip             on;\n    gzip_static      on;\n    gzip_min_length  1000;\n    gzip_proxied     expired no-cache no-store private auth;\n    gzip_types       text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript;\n\n    add_header 'Access-Control-Allow-Origin' '*';\n    add_header 'Access-Control-Allow-Credentials' 'true';\n    add_header 'Access-Control-Allow-Headers' 'Authorization,Content-Type,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken';\n    add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';\n    add_header PS 1\n\n    location / {\n            # First attempt to serve request as file, then\n            # as directory, then fall back to index.html\n            try_files $uri $uri/ /index.php?$query_string;\n            # Uncomment to enable naxsi on this location\n            # include /etc/nginx/naxsi.rules\n    }\n\n\n    location ~* \\.(css|js|asf|asx|wax|wmv|wmx|avi|bmp|class|divx|doc|docx|eot|exe|gif|gz|gzip|ico|jpg|jpeg|jpe|mdb|mid|midi|mov|qt|mp3|m4a|mp4|m4v|mpeg|mpg|mpe|mpp|odb|odc|odf|odg|odp|ods|odt|ogg|ogv|$\n            # 1 year -> 31536000\n            expires 500s;\n            access_log off;\n            log_not_found off;\n            add_header Pragma public;\n            add_header Cache-Control \"max-age=31536000, public\";\n    }\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    location ~ \\.php$ {\n            fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n            # NOTE: You should have \"cgi.fix_pathinfo = 0;\" in php.ini\n\n            # With php5-cgi alone:\n            #fastcgi_pass 127.0.0.1:9000;\n            # With php5-fpm:\n            fastcgi_pass unix:/var/run/example.sock;\n            fastcgi_index index.php?$query_string;\n            include fastcgi_params;\n\n            # instead I want to get the value from Origin request header\n    }\n\n    # Deny access to hidden files\n    location ~ /\\. {\n            deny all;\n            access_log off;\n            log_not_found off;\n    }\n\n    error_page 403 /403/;\n}\n\nserver {\n    listen 80;\n    server_name example.com;\n    rewrite     ^ http://www.example.com$request_uri? permanent;\n}\n\nI've tried adding the headers to the other location sections but the result is the same.\nAny help appreciated!!",
    "answer": "There were two issues for me.\nOne is that nginx only processes the last add_header it spots down a tree. So if you have an add_header in the server context, then another in the location nested context, it will only process the add_header directive inside the location context. Only the deepest context.\nFrom the NGINX docs on add_header:\n\nThere could be several add_header directives. These directives are inherited from the previous level if and only if there are no add_header directives defined on the current level. \n\nSecond issue was that the location / {} block I had in place was actually sending nginx to the other location ~* (\\.php)$ block (because it would repath all requests through index.php, and that actually makes nginx process this php block). So, my add_header directives inside the first location directive were useless, and it started working after I put all the directives I needed inside the php location directive.\nFinally, here's my working configuration to allow CORS in the context of an MVC framework called Laravel (you could change this easily to fit any PHP framework that has index.php as a single entry point for all requests).\n\nserver {\n    root /path/to/app/public;\n    index index.php;\n\n    server_name test.dev;\n\n    # redirection to index.php\n    location / {\n        try_files $uri $uri/ /index.php?$query_string;\n    }\n\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    location ~ \\.php$ {\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        # NOTE: You should have \"cgi.fix_pathinfo = 0;\" in php.ini\n\n        # With php5-fpm:\n        fastcgi_pass unix:/var/run/php5-fpm.sock;\n        fastcgi_index index.php;\n        include fastcgi_params;\n\n        # cors configuration\n        # whitelist of allowed domains, via a regular expression\n        # if ($http_origin ~* (http://localhost(:[0-9]+)?)) {\n        if ($http_origin ~* .*) { # yeah, for local development. tailor your regex as needed\n             set $cors \"true\";\n        }\n\n        # apparently, the following three if statements create a flag for \"compound conditions\"\n        if ($request_method = OPTIONS) {\n            set $cors \"${cors}options\";\n        }\n\n        if ($request_method = GET) {\n            set $cors \"${cors}get\";\n        }\n\n        if ($request_method = POST) {\n            set $cors \"${cors}post\";\n        }\n\n        # now process the flag\n        if ($cors = 'trueget') {\n            add_header 'Access-Control-Allow-Origin' \"$http_origin\";\n            add_header 'Access-Control-Allow-Credentials' 'true';\n        }\n\n        if ($cors = 'truepost') {\n            add_header 'Access-Control-Allow-Origin' \"$http_origin\";\n            add_header 'Access-Control-Allow-Credentials' 'true';\n        }\n\n        if ($cors = 'trueoptions') {\n            add_header 'Access-Control-Allow-Origin' \"$http_origin\";\n            add_header 'Access-Control-Allow-Credentials' 'true';\n\n            add_header 'Access-Control-Max-Age' 1728000; # cache preflight value for 20 days\n            add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';\n            add_header 'Access-Control-Allow-Headers' 'Authorization,Content-Type,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken,Keep-Alive,X-Requested-With,If-Modified-Since';\n\n            add_header 'Content-Length' 0;\n            add_header 'Content-Type' 'text/plain charset=UTF-8';\n            return 204;\n        }\n    }\n\n    error_log /var/log/nginx/test.dev.error.log;\n    access_log /var/log/nginx/test.dev.access.log;\n}\n\nThe gist for the above is at: https://gist.github.com/adityamenon/6753574"
  },
  {
    "question": "Nginx, Passenger, and Rails are running beautifully on my Linode. Before I launch, I'd like to restrict access so only my IP can view the site. \nI've tried to deny access to all, and allow access to only my IP in Nginx. It does deny access to all, but I can't get the allow to work. I have checked to ensure the IP address I'm specifying in nginx.conf is my correct public ip.\nHere's my nginx.conf. I've restarted nginx after editing the file, and tested some other changes which worked as expected (for instance, I removed deny all and was able to access the site, as expected).\nWhat am I doing wrong?\n    http {\n      passenger_root /path/to/passenger-3.0.11;\n      passenger_ruby /path/to/ruby;\n      include       mime.types;\n      default_type  application/octet-stream;\n      sendfile        on;\n      keepalive_timeout  65;\n      gzip  on;\n      server {\n        listen 80;\n        server_name www.foo.bar;\n        root /path/to/rails/public/;\n        passenger_enabled on;\n        location / {\n          allow   my.public.ip.here;\n          deny    all;\n        }\n      }\n    }",
    "answer": "modify your nginx.conf\n  server {\n    listen 80;\n    server_name www.foo.bar;\n\n    location / {\n      root /path/to/rails/public/;\n      passenger_enabled on;\n\n      allow   my.public.ip.here;\n      deny    all;\n    }\n  }"
  },
  {
    "question": "Configuration\n\nUbuntu Server 11.10 64 bit\nAmazon AWS, Ec2, hosted on the cloud\nt1.micro instance\n\nBefore I write anything else, I'd like to state that I've checked both nginx 502 bad gateway and Nginx + PHP-FPM 502 Bad Gateway threads, which unfortunately haven't helped me in this regard.\nThe issue appears to be rather common: a misconfiguration of nginx or php-fpm can lead to a 502 Bad Gateway error, which is something that I haven't been able to get rid of. Note that this appears even when I go to my domain root, without specifying any particular directory. \nI'm running an Amazon EC2 webserver, with port 9000 enabled, port 80 open, etc.\nThe question in particular is, how can I get rid of this nasty error? Or, better yet, how can I get php5-fpm to actually work.\nWhat I Have Attempted so Far\nMostly consistent editing of configuration files, notably php-fpm.conf and nginx.conf.\ni. php-fpm.conf\nI've added the following, which hasn't quite helped much:\n;;;;;;;;;;;;;\n; Fpm Start ;\n;;;;;;;;;;;;;\n\n;pm.start_servers = 20\n;pm.min_spare_servers = 5\n;pm.max_spare_servers = 35\n\nNow, afterward I tried including my configuration files:\ninclude=/etc/php5/fpm/*.conf\nWhich only screwed me even further.\nFull Configuration\n;;;;;;;;;;;;;;;;;;;;;\n; FPM Configuration ;\n;;;;;;;;;;;;;;;;;;;;;\n\n; All relative paths in this configuration file are relative to PHP's install\n; prefix (/usr). This prefix can be dynamicaly changed by using the\n; '-p' argument from the command line.\n\n; Include one or more files. If glob(3) exists, it is used to include a bunch of\n; files from a glob(3) pattern. This directive can be used everywhere in the\n; file.\n; Relative path can also be used. They will be prefixed by:\n;  - the global prefix if it's been set (-p arguement)\n;  - /usr otherwise\n;include=/etc/php5/fpm/*.conf\n\n;;;;;;;;;;;;;;;;;;\n; Global Options ;\n;;;;;;;;;;;;;;;;;;\n\n[global]\n; Pid file\n; Note: the default prefix is /var\n; Default Value: none\npid = /var/run/php5-fpm.pid\n\n; Error log file\n; Note: the default prefix is /var\n; Default Value: log/php-fpm.log\nerror_log = /var/log/php5-fpm.log\n\n; Log level\n; Possible Values: alert, error, warning, notice, debug\n; Default Value: notice\nlog_level = notice\n\n; If this number of child processes exit with SIGSEGV or SIGBUS within the time\n; interval set by emergency_restart_interval then FPM will restart. A value\n; of '0' means 'Off'.\n; Default Value: 0\n;emergency_restart_threshold = 0\n\n; Interval of time used by emergency_restart_interval to determine when \n; a graceful restart will be initiated.  This can be useful to work around\n; accidental corruptions in an accelerator's shared memory.\n; Available Units: s(econds), m(inutes), h(ours), or d(ays)\n; Default Unit: seconds\n; Default Value: 0\nemergency_restart_interval = 0\n\n; Time limit for child processes to wait for a reaction on signals from master.\n; Available units: s(econds), m(inutes), h(ours), or d(ays)\n; Default Unit: seconds\n; Default Value: 0\n;process_control_timeout = 0\n\n; Send FPM to background. Set to 'no' to keep FPM in foreground for debugging.\n; Default Value: yes\ndaemonize = no\n\n;;;;;;;;;;;;;\n; Fpm Start ;\n;;;;;;;;;;;;;\n\n;pm.start_servers = 20\n;pm.min_spare_servers = 5\n;pm.max_spare_servers = 35\n\n;;;;;;;;;;;;;;;;;;;;\n; Pool Definitions ; \n;;;;;;;;;;;;;;;;;;;;\n\n; Multiple pools of child processes may be started with different listening\n; ports and different management options.  The name of the pool will be\n; used in logs and stats. There is no limitation on the number of pools which\n; FPM can handle. Your system will tell you anyway :)\n\n; To configure the pools it is recommended to have one .conf file per\n; pool in the following directory:\ninclude=/etc/php5/fpm/pool.d/*.conf\n\nii. nginx.conf\nIn all honesty this configuration is a smattering of a few websites I've visited, but I can tell you that before this 502 Bad Gateway business, the server was running fine (without PHP working. Period.).\nThe issue primarily lies in the fact that something is terribly, terribly wrong. And now, when I try to do a service php5-fpm restart, it hangs in what I'm guessing is an infinite loop or something, which I can't even CTRL-C out of.\nFull Configuration\nuser www-data;\nworker_processes 1;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 64;\n    # multi_accept on;\n}\n\nhttp {\n\n    ##\n    # Basic Settings\n    ##\n\n    sendfile on;\n    tcp_nopush off;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    # server_tokens off;\n\n    # server_names_hash_bucket_size 64;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    ##\n    # Logging Settings\n    ##\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    ##\n    # Gzip Settings\n    ##\n\n    gzip on;\n    gzip_disable \"msie6\";\n\n    # gzip_vary on;\n    # gzip_proxied any;\n    # gzip_comp_level 6;\n    # gzip_buffers 16 8k;\n    # gzip_http_version 1.1;\n    # gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;\n\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n\n    server {\n        listen 80;\n        server_name ec2-xx-xx-xx-xx.compute-x.amazonaws.com;\n\n        location ~ ^(.+\\.php)(.*)$ {\n            root   /home/wayvac/public;\n            fastcgi_pass   unix:/var/run/php5-fpm.pid;  \n            #fastcgi_pass   127.0.0.1:9000; #Un-comment this and comment \"fastcgi_pass   unix:/var/run/php-fpm/php-fpm.sock;\" if you are not using php-fpm.\n            fastcgi_index  index.php;\n            set $document_root2 $document_root;\n            if ($document_root2 ~ \"^(.*\\\\\\\\).*?[\\\\\\\\|\\/]\\.\\.\\/(.*)$\") { set $document_root2 $1$2; }\n            if ($document_root2 ~ \"^(.*\\\\\\\\).*?[\\\\\\\\|\\/]\\.\\.\\/(.*)$\") { set $document_root2 $1$2; }\n            if ($document_root2 ~ \"^(.*\\\\\\\\).*?[\\\\\\\\|\\/]\\.\\.\\/(.*)$\") { set $document_root2 $1$2; }\n            if ($document_root2 ~ \"^(.*\\\\\\\\).*?[\\\\\\\\|\\/]\\.\\.\\/(.*)$\") { set $document_root2 $1$2; }\n            if ($document_root2 ~ \"^(.*\\\\\\\\).*?[\\\\\\\\|\\/]\\.\\.\\/(.*)$\") { set $document_root2 $1$2; }\n            fastcgi_split_path_info ^(.+\\.php)(.*)$;\n            fastcgi_param   SCRIPT_FILENAME $document_root2$fastcgi_script_name;\n            fastcgi_param   PATH_INFO   $fastcgi_path_info;\n            fastcgi_param   PATH_TRANSLATED $document_root2$fastcgi_path_info;\n            include fastcgi_params;\n            fastcgi_param  DOCUMENT_ROOT      $document_root2;\n        }       \n\n        access_log /var/log/nginx/access.log;\n        error_log /var/log/nginx/error.log;\n\n        location / {\n            root /home/wayvac/public;   \n            index index.html index.htm index.php;\n        }\n\n        location ~* \\.(?:ico|css|js|gif|jpe?g|png)$ {\n            # Some basic cache-control for static files to be sent to the browser\n            expires max;\n            add_header Pragma public;\n            add_header Cache-Control \"public, must-revalidate, proxy-revalidate\";\n        }\n\n        #include drop.conf;\n        #include php.conf;\n    }\n}",
    "answer": "If anyone finds this page by encountering the same problem I had, I found the answer here.\nFor those of you who can't be bothered to click and work it out for themselves... ;)\nThe Condition:\nUbuntu or Debian server with NGINX and PHP 5.3 works fine but upgrading PHP to 5.4 gives 502 Bad Gateway errors. Looking for services running on port 9000 (typically running netstat -lp or similar) returns nothing.\nThe fix:\nOpen /etc/php5/fpm/pool.d/www.conf and make a note of the 'listen' parameter (in my case /var/run/php5-fpm.sock):\n; The address on which to accept FastCGI requests.\n; Valid syntaxes are:\n;   'ip.add.re.ss:port'    - to listen on a TCP socket to a specific address on\n;                            a specific port;\n;   'port'                 - to listen on a TCP socket to all addresses on a\n;                            specific port;\n;   '/path/to/unix/socket' - to listen on a unix socket.\n; Note: This value is mandatory.\nlisten = /var/run/php5-fpm.sock\n\nand replace the fastcgi_pass variable in your vhost with the location you just noted.\nSo this sample symfony2 configuration (taken from here):\n  # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n  location ~ ^/(app|app_dev)\\.php(/|$) {\n    fastcgi_pass   127.0.0.1:9000;\n    fastcgi_split_path_info ^(.+\\.php)(/.*)$;\n    include fastcgi_params;\n    fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;\n    fastcgi_param  HTTPS              off;\n  }\n\nbecomes this:\n  # pass the PHP scripts to FastCGI server at /var/run/php5-fpm.sock\n  location ~ ^/(app|app_dev)\\.php(/|$) {\n    fastcgi_pass unix:/var/run/php5-fpm.sock;\n    fastcgi_split_path_info ^(.+\\.php)(/.*)$;\n    include fastcgi_params;\n    fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;\n    fastcgi_param  HTTPS              off;\n  }\n\nThen restart nginx:\nsudo /etc/init.d/nginx restart\n\nNote: replace ~ ^/(app|app_dev)\\.php(/|$) { with ~ ^/index\\.php(/|$) { if you're not on SF2**\nHope this saves someone a little bit of time :)\nEdit\nOf course, you could change the listen = /var/run/php5-fpm.sock to listen = 127.0.0.1:9000 in /etc/php5/fpm/pool.d/www.conf then restart php5-fpm (which would save you from having to change your vhosts), but you have to assume they changed php5-fpm to run through a socket rather than listening on port 9000 for a reason.\nEdit2\nIf you're still experiencing 502 error see this answer."
  },
  {
    "question": "The default path for teamcity artifacts is \n\nC:\\#User#\\.BuildServer\\system\\artifacts \n\nHow can i change it to\n\nd:\\TeamCity\\Artifacts\n\nThanks",
    "answer": "For me the default is D:\\BuildServer\\system\\artifacts\nYes you can, set the TEAMCITY_DATA_PATH environment variable.\nSee here: http://www.jetbrains.net/confluence/display/TCD4/TeamCity+Data+Directory\n\nBy default, the  is placed in the user's\n  home directory (e.g. it is\n  $HOME/.BuildServer under Linux and\n  C:\\Documents and\n  Settings\\.BuildServer)\n  under Windows. Alternatively, you can\n  define this directory in one of the\n  following ways:\n\nAs a Tomcat system property teamcity.data.path (see System\n  Properties for Running the Server)\nIn the TEAMCITY_DATA_PATH environment variable (this will be\n  used if the teamcity.data.path JVM\n  system property is not found)"
  },
  {
    "question": "I was wondering, if there is a standard, canonical way in Haskell to write not only a parser for a specific file format, but also a writer.\nIn my case, I need to parse a data file for analysis. However, I also simulate data to be analyzed and save it in the same file format. I could now write a parser using Parsec or something equivalent and also write functions that perform the text output in the way that it is needed, but whenever I change my file format, I would have to change two functions in my code. Is there a better way to achieve this goal?\nThank you,\nDominik",
    "answer": "The BNFC-meta package https://hackage.haskell.org/package/BNFC-meta-0.4.0.3\nmight be what you looking for\n\"Specifically, given a quasi-quoted LBNF grammar (as used by the BNF Converter) it generates (using Template Haskell) a LALR parser and pretty pretty printer for the language.\"\nupdate: found this package that also seems to fulfill the objective (not tested yet) http://hackage.haskell.org/package/syntax"
  },
  {
    "question": "I've just started using GitLab, and have created a set of issues, in order to keep an overview of what needs to be done for my application. I was wondering if it was possible to create a branch from these issues, such that the branch and issues are linked, similar as in jira and Stash from atlassian?",
    "answer": "If you create a branch with the name <issue-number>-issue-description and push that branch to gitlab, it will automatically be linked to that issue. For instance, if you have an issue with id 654 and you create a branch with name 654-some-feature and push it to gitlab, it will be linked to issue 654.\nGitlab will even ask you if you want to create a merge request and will automatically add Closes #654 to the merge request description which will close issue 654 when the merge request is accepted.\nAlso if you go to a given issue page on gitlab, you should see a New Branch button which will automatically create a branch with a name of the form <issue-number>-issue-description."
  },
  {
    "question": "My Redis container is defined as a standard image in my docker_compose.yml:\nredis:  \n  image: redis\n  ports:\n    - \"6379\"\n\nI guess it's using standard settings like binding to Redis at localhost.\nI need to bind it to 0.0.0.0, is there any way to add a local redis.conf file to change the binding and let docker-compose use it?",
    "answer": "Yes. Mount the config into the image with a volume and modify the command to call it e.g:\nredis:  \n  image: redis\n  command: redis-server /usr/local/etc/redis/redis.conf\n  volumes:\n    - ./redis.conf:/usr/local/etc/redis/redis.conf\n  ports:\n    - \"6379\"\n\nAlternatively, create a new image based on the redis image with your conf file copied in. Full instructions are at: https://registry.hub.docker.com/_/redis/\nHowever, the redis image does bind to 0.0.0.0 by default. To access it from the host, you need to use the port that Docker has mapped to the host for you which you find by using docker ps or the docker port command, you can then access it at localhost:32678 where 32678 is the mapped port. Alternatively, you can specify a specific port to map to in the docker-compose.yml.\nAs you seem to be new to Docker, this might all make a bit more sense if you start by using raw Docker commands rather than starting with Compose."
  },
  {
    "question": "I'm trying to restore my dump file, but it caused an error:\npsql:psit.sql:27485: invalid command \\N\n\nIs there a solution? I searched, but I didn't get a clear answer.",
    "answer": "Postgres uses \\N as substitute symbol for NULL value. But all psql commands start with a backslash \\ symbol. You can get these messages, when a copy statement fails, but the loading of dump continues. This message is a false alarm. You have to search all lines prior to this error if you want to see the real reason why COPY statement failed.\nIs possible to switch psql to \"stop on first error\" mode and to find error:\npsql -v ON_ERROR_STOP=1"
  },
  {
    "question": "I'm struggling to install Ansible Python package on my Windows 10 machine.\nI don't need Ansible to run on my machine, this is purely for development purpose on my Windows host. All commands will later be issued on a Linux machine.\nAfter running:\npip install ansible\n\nI get the following exception:\n\nCommand \"c:\\users\\evaldas.buinauskas\\appdata\\local\\programs\\python\\python37-32\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\Users\\evaldas.buinauskas\\AppData\\Local\\Temp\\pip-install-hpay_le9\\ansible\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\evaldas.buinauskas\\AppData\\Local\\Temp\\pip-record-dvfgngpp\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\evaldas.buinauskas\\AppData\\Local\\Temp\\pip-install-hpay_le9\\ansible\\\n\nAlso there's a repetetive exception that I think is the root cause:\n\nerror: can't copy 'lib\\ansible\\module_utils\\ansible_release.py': doesn't exist or not a regular file\n\nThis GitHub issue says that installing should be possible, not running it. That's basically all I really need.\nI tried running CMD/PowerShell/Cygwin as Administrator, didn't help. \nAlso, there's an answer that tells how to install it on Windows: How to overcome - pip install ansible on windows failing with filename or extension too long on windows\nBut I don't really understand how to get a wheel file for Ansible package.",
    "answer": "Installing Ansible on Windows is cumbersome. My advice is not a direct solution on how to install Ansible on Windows, but rather a workaround.\nI use a Docker container with Ansible for developing playbooks on my Windows machine. You'd need Docker for Windows on your machine. \nHere's the Dockerfile:\nFROM alpine:3.7\n\nENV ANSIBLE_VERSION=2.5.4\n\nENV BUILD_PACKAGES \\\n        bash \\\n        curl \\\n        tar \\\n        nano \\\n        openssh-client \\\n        sshpass \\\n        git \\\n        python \\\n        py-boto \\\n        py-dateutil \\\n        py-httplib2 \\\n        py-jinja2 \\\n        py-paramiko \\\n        py-pip \\\n        py-setuptools \\\n        py-yaml \\\n        ca-certificates\n\nRUN apk --update add --virtual build-dependencies \\\n        gcc \\\n        musl-dev \\\n        libffi-dev \\\n        openssl-dev \\\n        python-dev && \\\n    set -x && \\\n    apk update && apk upgrade && \\\n    apk add --no-cache ${BUILD_PACKAGES} && \\\n    pip install --upgrade pip && \\\n    pip install python-keyczar docker-py boto3 botocore && \\\n    apk del build-dependencies && \\\n    rm -rf /var/cache/apk/* && \\\n    mkdir -p /etc/ansible/ /ansible && \\\n    echo \"[local]\" >> /etc/ansible/hosts && \\\n    echo \"localhost\" >> /etc/ansible/hosts && \\\n    curl -fsSL https://releases.ansible.com/ansible/ansible-${ANSIBLE_VERSION}.tar.gz -o ansible.tar.gz && \\\n    tar -xzf ansible.tar.gz -C /ansible --strip-components 1 && \\\n    rm -fr ansible.tar.gz /ansible/docs /ansible/examples /ansible/packaging\n\nENV ANSIBLE_GATHERING=smart \\\n    ANSIBLE_HOST_KEY_CHECKING=false \\\n    ANSIBLE_RETRY_FILES_ENABLED=false \\\n    ANSIBLE_ROLES_PATH=/ansible/playbooks/roles \\\n    ANSIBLE_SSH_PIPELINING=True \\\n    PYTHONPATH=/ansible/lib \\\n    PATH=/ansible/bin:$PATH \\\n    ANSIBLE_LIBRARY=/ansible/library \\\n    EDITOR=nano\n\nWORKDIR /ansible/playbooks\n\nENTRYPOINT [\"ansible-playbook\"]\n\nBuild the docker container with the docker build command. Afterwards you can create a small bash script that executes the docker run command and mounts your current directory into the container. You may call it ansible-playbook.sh:\nwinpty docker run --rm -it -v /$(pwd):/ansible/playbooks <name of your container> $@\n\nNow you will be able to launch Ansible playbook with ./ansible-playbook.sh <your playbook> in GIT BASH. If you'd like to run this in PowerShell you would probably need to remove the winpty command, but I did not test this in PS yet.\nIt is not the finest solution but it gets the work done. Hope it helps you, too."
  },
  {
    "question": "I installed rabbitmq service on  the server and on my system.\nI want to use RPC pattern:\nvar factory = new ConnectionFactory() { \n  HostName = \"158.2.14.42\", \n  Port = Protocols.DefaultProtocol.DefaultPort, \n  UserName = \"Administrator\", \n  Password = \"@server@\", \n  VirtualHost = \"/\"\n  ContinuationTimeout = new TimeSpan(10, 0, 0, 0) \n};\n\nconnection = factory.CreateConnection();\n\nI have an error on creating connection with this message:\nNone of the specified endpoints were reachable\nWhen I use it on localhost instance of the server it works, but when I create the connection from local to that server,it returned the error.\nIt not work with local ip and username and password of the my local computer.\nCan anyone help me?",
    "answer": "As this question mentioned.\nAfter I installed RabbitMQ, I enabled management tools on the server and on my local computer with this:\nrabbitmq-plugins enable rabbitmq_management\n\nThen I restarted RabbitMQ service from services.msc\nI could see the Rabbitmq management at http://localhost:15672\nI logged in to rabbit management with user:guest and pass:guest\nI added my favorite user pass with administrator access, so it worked."
  },
  {
    "question": "I have a table which has several ID columns to other tables.\nI want a foreign key to force integrity only if I put data in there. If I do an update at a later time to populate that column, then it should also check the constraint.\n(This is likely database server dependant, I'm using MySQL & InnoDB table type)\nI believe this is a reasonable expectation, but correct me if I am wrong.",
    "answer": "Yes, you can enforce the constraint only when the value is not NULL. This can be easily tested with the following example:\nCREATE DATABASE t;\nUSE t;\n\nCREATE TABLE parent (id INT NOT NULL,\n                     PRIMARY KEY (id)\n) ENGINE=INNODB;\n\nCREATE TABLE child (id INT NULL, \n                    parent_id INT NULL,\n                    FOREIGN KEY (parent_id) REFERENCES parent(id)\n) ENGINE=INNODB;\n\n\nINSERT INTO child (id, parent_id) VALUES (1, NULL);\n-- Query OK, 1 row affected (0.01 sec)\n\n\nINSERT INTO child (id, parent_id) VALUES (2, 1);\n\n-- ERROR 1452 (23000): Cannot add or update a child row: a foreign key \n-- constraint fails (`t/child`, CONSTRAINT `child_ibfk_1` FOREIGN KEY\n-- (`parent_id`) REFERENCES `parent` (`id`))\n\nThe first insert will pass because we insert a NULL in the parent_id. The second insert fails because of the foreign key constraint, since we tried to insert a value that does not exist in the parent table."
  },
  {
    "question": "Imagine the situation:\nvar txn = new DatabaseTransaction();\n\nvar entry = txn.Database.Load<Entry>(id);\nentry.Token = \"123\";\ntxn.Database.Update(entry);\n\nPublishRabbitMqMessage(new EntryUpdatedMessage { ID = entry.ID });\n\n// A bit more of processing\n\ntxn.Commit();\n\nNow a consumer of EntryUpdatedMessage can potentially get this message before the transaction txn is committed and therefore will not be able to see the update.\nNow, I know that RabbitMQ does support transactions by itself, but we cannot really use them because we create a new IModel for each publish and having a per-thread model is really cumbersome in our scenario (ASP.NET web application).\nI thought of having a list of messages due to be published when a DB transaction is committed, but that's a really smelly solution.\nWhat is the correct way of handling this?",
    "answer": "RabbitMQ encourages you to use publisher confirms rather than transactions. Transactions do not perform well.\nIn any case, transactions don't usually work very well with a service oriented architecture. It's better to adopt an 'eventually consistent' approach, where failure can be retried at a later date and duplicate idempotent messages are ignored.\nIn your example I would do the database update and commit it before publishing the message. When the publisher confirm returns, I would update a field in the database record to indicate that the message had been sent. You can then have a sweeper process come along later, check for unsent messages and send them on their way. If the message did get through, but for some reason the confirm, or subsequent database write failed, you will get a duplicate message. But that won't matter, because you've designed your messages to be idempotent."
  },
  {
    "question": "In MS SQL Server, I create my scripts to use customizable variables:\nDECLARE @somevariable int  \nSELECT @somevariable = -1\n\nINSERT INTO foo VALUES ( @somevariable )\n\nI'll then change the value of @somevariable at runtime, depending on the value that I want in the particular situation. Since it's at the top of the script it's easy to see and remember.\nHow do I do the same with the PostgreSQL client psql?",
    "answer": "Postgres variables are created through the \\set command, for example ...\n\\set myvariable value\n\n... and can then be substituted, for example, as ...\nSELECT * FROM :myvariable.table1;\n\n... or ...\nSELECT * FROM table1 WHERE :myvariable IS NULL;\n\nedit: As of psql 9.1, variables can be expanded in quotes as in: \n\\set myvariable value \n\nSELECT * FROM table1 WHERE column1 = :'myvariable';\n\nIn older versions of the psql client: \n... If you want to use the variable as the value in a conditional string query, such as ...\nSELECT * FROM table1 WHERE column1 = ':myvariable';\n\n... then you need to include the quotes in the variable itself as the above will not work.  Instead define your variable as such ...\n\\set myvariable 'value'\n\nHowever, if, like me, you ran into a situation in which you wanted to make a string from an existing variable, I found the trick to be this ...\n\\set quoted_myvariable '\\'' :myvariable '\\''\n\nNow you have both a quoted and unquoted variable of the same string!  And you can do something like this ....\nINSERT INTO :myvariable.table1 SELECT * FROM table2 WHERE column1 = :quoted_myvariable;"
  },
  {
    "question": "We have an application that uses Bouncy Castle to encrypt data using PBEWITHSHA256AND128BITAES-CBC-BC algorithm. It works fine on Ubuntu running OpenJDK 1.7. But when when we move it to RedHat 6.4 also running OpenJDK 1.7, we get the following exception:\n\njava.security.NoSuchAlgorithmException\n\nAny thoughts on what could be causing this. How can we add PBEWITHSHA256AND128BITAES-CBC-BC algorithm to RedHat 6.4?\np.s. the application is running in JBoss.\nprivate String cryptoAlgorithm = \"PBEWITHSHA256AND128BITAES-CBC-BC\";\n\nSecurity.addProvider(new BouncyCastleProvider());\n\n// load passPhrase from configured external file to char array.\nchar[] passPhrase = null;\ntry {\n    passPhrase = loadPassPhrase(passPhraseFile);\n} catch (FileNotFoundException e) {\n    throw BeanHelper.logException(LOG, methodName, new EJBException(\"The file not found: \" + passPhraseFile, e));\n} catch (IOException e) {\n    throw BeanHelper.logException(LOG, methodName, new EJBException(\"Error in reading file: \" + passPhraseFile, e));\n}\n\nPBEKeySpec pbeKeySpec = new PBEKeySpec(passPhrase);\n\ntry {\n    SecretKeyFactory secretKeyFactory = SecretKeyFactory.getInstance(cryptoAlgorithm);\n    SecretKey newSecretKey = secretKeyFactory.generateSecret(pbeKeySpec);\n    return newSecretKey;\n} catch (NoSuchAlgorithmException e) {\n    throw BeanHelper.logException(LOG, methodName, new EJBException(\"The algorithm is not found: \" + cryptoAlgorithm, e));\n} catch (InvalidKeySpecException e) {\n    throw BeanHelper.logException(LOG, methodName, new EJBException(\"The key spec is invalid\", e));\n}\n\n(On RH 6.4)\n#java -version\njava version \"1.7.0_19\"\nOpenJDK Runtime Environment (rhel-2.3.9.1.el6_4-x86_64)\nOpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)\n\n(On Ubuntu 12.04)\n#java version \"1.7.0_15\"\nOpenJDK Runtime Environment (IcedTea7 2.3.7) (7u15-2.3.7-0ubuntu1~12.04)\nOpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)",
    "answer": "Do you have the BouncyCastle provider JAR (e.g. bcprov-jdk15on-149.jar) in your classpath?\nI tested your scenario with a minimal CentOS 6.4 (64-bit) installation, OpenJDK 1.7 and BouncyCastle 1.49, and found no issues with it.\nI placed the JAR in the JRE lib/ext directory:\n/usr/lib/jvm/java-1.7.0-openjdk.x86_64/jre/lib/ext"
  },
  {
    "question": "This is happening in Puppet's bundle.\nThe Gemfile specifies\ngem \"puppet\", :path => File.dirname(__FILE__), :require => false\n\nBut one of the gems I installed in $GEM_HOME appears in $: after all.\n$ bundle exec ruby -e 'puts $:'\n...\n/home/puppy/puppet-git-clone/lib\n...\n/usr/lib/ruby/vendor_ruby\n...\n/home/puppy/gems/gems/puppet-3.7.5/lib\n...\n\nThis is not a problem in and of itself, but apparently Ruby will load Puppet 3.7.5 instead of the 3.7.3 I checked out of the git repo.\n$ bundle exec irb\nirb(main):001:0> require 'puppet'\n=> true\nirb(main):002:0> Facter.value(:puppetversion)\n=> \"3.7.5\"\n\nWhy is Puppet not loaded from the git tree and how can I debug this further?\nUpdate\nPuppets .gemspec might be involved. It's clever about specifying the version. I now worry that Rubygems does in fact load the installed 3.7.5 gem so that Puppet.version would truthfully report a wrong value, throwing off bundler. Could that be what's happening?\nUpdate 2\nAs suggested in the comments, I tried settings the path and version statically in the Gemfile.\ngem \"puppet\", \"3.4.2\", :path => \"/home/puppy/puppet-git-clone\", :require => false\n\nAs for the result, well - at least bundler is consistent in its views ;-)\nCould not find gem 'puppet (= 3.4.2) ruby' in source at /home/ffrank/git/puppet.\nSource contains 'puppet' at: 3.7.3\nRun `bundle install` to install missing gems.",
    "answer": "The quick fix is to add -Ilib to your ruby command:\n$ bundle exec ruby -e \"require 'puppet'; puts Facter.value(:puppetversion)\"\n3.7.5\n\n$ bundle exec ruby -Ilib -e \"require 'puppet'; puts Facter.value(:puppetversion)\"\n3.7.3\n\nIf we compare the load paths, you can see that adding -Ilib results in 3.7.5 not being present in the second load path:\n$ diff <(bundle exec ruby -e 'puts $:') <(bundle exec ruby -Ilib -e 'puts $:') | grep 'puppet-'\n< /Library/Ruby/Gems/2.0.0/gems/puppet-3.7.5/lib\n\nIt seems like this should be the default behavior, so there may be a bug in bundler."
  },
  {
    "question": "What is the equivalent of varchar(max) in MySQL?",
    "answer": "The max length of a varchar is subject to the max row size in MySQL, which is 64KB (not counting BLOBs):\nVARCHAR(65535)\n\nHowever, note that the limit is lower if you use a multi-byte character set:\nVARCHAR(21844) CHARACTER SET utf8\n\n\nHere are some examples:\nThe maximum row size is 65535, but a varchar also includes a byte or two to encode the length of a given string. So you actually can't declare a varchar of the maximum row size, even if it's the only column in the table.\nmysql> CREATE TABLE foo ( v VARCHAR(65534) );\nERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs\n\nBut if we try decreasing lengths, we find the greatest length that works:\nmysql> CREATE TABLE foo ( v VARCHAR(65532) );\nQuery OK, 0 rows affected (0.01 sec)\n\nNow if we try to use a multibyte charset at the table level, we find that it counts each character as multiple bytes. UTF8 strings don't necessarily use multiple bytes per string, but MySQL can't assume you'll restrict all your future inserts to single-byte characters.\nmysql> CREATE TABLE foo ( v VARCHAR(65532) ) CHARSET=utf8;\nERROR 1074 (42000): Column length too big for column 'v' (max = 21845); use BLOB or TEXT instead\n\nIn spite of what the last error told us, InnoDB still doesn't like a length of 21845.\nmysql> CREATE TABLE foo ( v VARCHAR(21845) ) CHARSET=utf8;\nERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs\n\nThis makes perfect sense, if you calculate that 21845*3 = 65535, which wouldn't have worked anyway. Whereas 21844*3 = 65532, which does work.\nmysql> CREATE TABLE foo ( v VARCHAR(21844) ) CHARSET=utf8;\nQuery OK, 0 rows affected (0.32 sec)"
  },
  {
    "question": "Is there any existing Keycloak client for Asp.net Core? I have found a NuGet package for .net but it doesn't work with Core. Do you have any ideas how to easily integrate with this security server (or maybe using any other alternatives)?",
    "answer": "I've played a bit with this today. The most straightforward way is too use OpenId standard.\nIn Startup.cs I used OpenIdConnect Authentication:\n    public void Configure(...)\n    { (...)\n         app.UseCookieAuthentication(new CookieAuthenticationOptions\n        {\n            AuthenticationScheme = CookieAuthenticationDefaults.AuthenticationScheme,\n            AutomaticAuthenticate = true,\n            CookieHttpOnly = true,\n            CookieSecure = CookieSecurePolicy.SameAsRequest\n        });\n        app.UseOpenIdConnectAuthentication(CreateKeycloakOpenIdConnectOptions());`(...)\n }`\n\nOpenIdConnectOptions method:\nprivate OpenIdConnectOptions CreateKeycloakOpenIdConnectOptions()\n    {\n        var options = new OpenIdConnectOptions\n        {\n            AuthenticationScheme = \"oidc\",\n            SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme,\n            Authority = Configuration[\"Authentication:KeycloakAuthentication:ServerAddress\"]+\"/auth/realms/\"+ Configuration[\"Authentication:KeycloakAuthentication:Realm\"],\n            RequireHttpsMetadata = false, //only in development\n            PostLogoutRedirectUri = Configuration[\"Authentication:KeycloakAuthentication:PostLogoutRedirectUri\"],\n            ClientId = Configuration[\"Authentication:KeycloakAuthentication:ClientId\"],\n            ClientSecret = Configuration[\"Authentication:KeycloakAuthentication:ClientSecret\"],\n            ResponseType = OpenIdConnectResponseType.Code,\n            GetClaimsFromUserInfoEndpoint = true,\n            SaveTokens = true\n\n        };\n        options.Scope.Add(\"openid\");\n        return options;\n    }\n\nIn appsettings.json add configuration for Keycloak:\n\n\n{\r\n  (...),\r\n  \"Authentication\": {\r\n    \"KeycloakAuthentication\": {\r\n      \"ServerAddress\": \"http://localhost:8180\",\r\n      \"Realm\": \"demo\",\r\n      \"PostLogoutRedirectUri\": \"http://localhost:57630/\",\r\n      \"ClientId\": \"KeycloakASPNETCore\",\r\n      \"ClientSecret\": \"secret-get-it-in-keycloakConsole-client-credentials\"\r\n    }\r\n  }\r\n}\n\n\n\nKeycloak  client is configuerd as followed:\n\nClient settings,\nI've added 'accounting' role for test,\nI added mapper 'member_of' of type 'User Client Role' for roles so that roles are added in the claims\n\nIf I want to Authorize user by role I do something like this:\nAdd authorization by claims in ConfigureServices method:\npublic void ConfigureServices(IServiceCollection services)\n    {\n        (...)\n\n        services.AddAuthorization(options =>\n        {\n            options.AddPolicy(\"Accounting\", policy =>\n            policy.RequireClaim(\"member_of\", \"[accounting]\")); //this claim value is an array. Any suggestions how to extract just single role? This still works.\n        });\n    }\n\nI've edited get method in ValuesController (Default Web API template):\n[Authorize(Policy = \"Accounting\")]\n[Route(\"api/[controller]\")]\npublic class ValuesController : Controller\n{\n    // GET api/values        \n    [HttpGet]\n    public Dictionary<string,string> Get()\n    {\n        var userPrinciple = User as ClaimsPrincipal;\n        var claims = new Dictionary<string, string>();\n\n        foreach (var claim in userPrinciple.Claims)\n        {\n            var key = claim.Type;\n            var value = claim.Value;\n\n            claims.Add(key, value);\n        }\n\n\n        return claims;\n    }\n\nIf I login with user that has accounting role or is in group that has accounting role, it should display my user claims on address localhost:57630/api/values.\nI hope this works for you.\nEdit: .NET Core 2\nHi everyone! The way my app works changed quite a bit and I have not fully tested .NET Core 2 yet, but you can still try connecting to Keycloak like this in ConfigureServices:\n        services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)\n            .AddJwtBearer(options =>\n            {\n\n                options.Authority = Configuration[\"Authentication:KeycloakAuthentication:ServerAddress\"] + \"/auth/realms/\" + Configuration[\"Authentication:KeycloakAuthentication:Realm\"];\n                options.TokenValidationParameters = new Microsoft.IdentityModel.Tokens.TokenValidationParameters\n                {\n                    ValidAudiences = new string[] { \"curl\", \"financeApplication\", \"accountingApplication\", \"swagger\"}\n                };\n                options.RequireHttpsMetadata = false; //for test only!\n                options.SaveToken = true;\n                options.Validate();\n\n            });\n\nAnd in Configure:\napp.UseAuthentication();\n\nYou can access your token later with IHttpContextAccessor httpContextAccessor, for example:\npublic KeycloakAuthorizationRequirementHandler(IConfiguration config,\n            IHttpContextAccessor httpContextAccessor,\n            IMemoryCache memoryCache)\n        {\n            _config = config;\n            _httpContextAccessor = httpContextAccessor;\n            _memoryCache = memoryCache;\n        }\n\n//get accessToken\nvar accessToken = _httpContextAccessor.HttpContext.GetTokenAsync(\"access_token\");\n\n_httpContextAccessor.HttpContext.Items[\"username\"] = username;\n\nTell me how it goes."
  },
  {
    "question": "I am using two system (both are Nginx load balancer and one act as backup).\nI want to add and use few HTTP custom headers.\nBelow is my code for both:\nupstream upstream0 {\n    #list of upstream servers\n    server backend:80;\n    server backup_load_balancer:777 backup;\n    #healthcheck\n}\n\nserver {\n    listen 80;\n    #Add custom header about the port and protocol  (http or https)\n    server_name _;\n\n    location / {\n        # is included since links are not allowed in the post\n        proxy_pass \"http://upstream0;\"\n    }\n}\n\nBackup system\nserver {\n    listen 777;\n    server_name _;\n    #doing some other extra stuff\n    #use port and protocol to direct\n}\n\nHow can I achieve that?",
    "answer": "To add a header, add the add_header declaration to either the location block or the server block:\nserver {\n   add_header X-server-header \"my server header content!\";\n   location /specific-location {\n       add_header X-location-header \"my specific-location header content!\";\n   }\n}\n\nAn add_header declaration within a location block will override the same add_header declaration in the outer server block.\ne.g. if location contained add_header X-server-header ... that would override the outer declaration for that path location.\nObviously, replace the values with what you want to add. And that's all there is to it."
  },
  {
    "question": "git is asking me to enter my gitlab user credentials when pushing or pulling code. I am using gitlab.com, I'm not self-hosting gitlab.\nI followed the instructions to set up my ssh key. I created a key, I copied the contents from ~/.ssh/id_rsa.pub, added the key to gitlab using gitlab's user interface, and git still asks me for my user and password.\ngit remote -v\norigin  https://gitlab.com/<my_user>/<my_repo> (fetch)\norigin  https://gitlab.com/<my_user>/<my_repo> (push)",
    "answer": "You are using HTTPS authentication.  Switch to an SSH-based URL (in this case, probably ssh://git@gitlab.com/path/to/repo.git or git@gitlab.com:path/to/repo.git).  For instance:\ngit remote set-url origin ssh://git@gitlab.com/<user>/<repo>\n\n(Alternatively, if you're comfortable with using your configured editor, you can run git config --edit and edit the URL directly in the configuration file.  Make sure whatever editor you choose stores files in plain-text format, not any enhanced rich-text or UTF-16 encoding.)\nBackground\nIn order to connect one Git to another, you may need to authenticate yourself.  There are several ways to do that.  (Whether you need to do that depends on the kind of connection you are making, and whether the repository is public.)\nIf you are authenticating over https://, your Git may ask for a user name and password.  It can use various credential helpers, which may also store user names and/or passwords. You can configure which credential helper to use, including the \"cache\" and \"store\" helpers, which use different additional data. Note that the available set of credential helpers varies based on your underlying operating system as well.\nIf you are authenticating over ssh://, you will use an ssh key (SSH authentication).\nIf you use a URL that begins with user@host:, you are using SSH authentication."
  }
]